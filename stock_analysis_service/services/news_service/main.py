"""
ë‰´ìŠ¤ ì„œë¹„ìŠ¤ ë©”ì¸ ëª¨ë“ˆ
- ë‰´ìŠ¤ í¬ë¡¤ë§ ë° ë¶„ì„
- 3ë‹¨ê³„ í•„í„°ë§ ì‹œìŠ¤í…œ (SimHash â†’ ì¢…ëª©ê´€ë ¨ì„± â†’ ë²¡í„°ìœ ì‚¬ë„)
- ê³ ì˜í–¥ë„ ë‰´ìŠ¤ ê°ì§€ ë° ì•Œë¦¼
- SkillStack í†µí•© ì™„ë£Œ
"""

import asyncio
import csv
import hashlib
import json
import logging
import os
import re
import time
import traceback
from datetime import datetime, timedelta
from threading import Lock
from typing import Dict, List, Optional, Tuple, Any, Union
from pathlib import Path

import numpy as np
import requests
import schedule
import simhash
import uvicorn
# yfinance ì œê±° - pykrxë¡œ ëŒ€ì²´
from fastapi import FastAPI, BackgroundTasks, HTTPException, Request
from selenium import webdriver
from selenium.common.exceptions import (
    NoSuchElementException, TimeoutException, WebDriverException
)
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait
from sentence_transformers import SentenceTransformer
from webdriver_manager.chrome import ChromeDriverManager

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ import
import sys
import os

# Windows ìœ ë‹ˆì½”ë“œ ì¸ì½”ë”© ì„¤ì •
if os.name == 'nt':  # Windows
    os.environ['PYTHONIOENCODING'] = 'utf-8'

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë° ì„œë¹„ìŠ¤ ê²½ë¡œ ì¶”ê°€
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
services_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)
sys.path.insert(0, services_path)

from config.env_local import get_config, get_env_var, get_int_env_var
from shared.user_config.user_config_manager import user_config_manager
from shared.database.mysql_client import get_mysql_client
from shared.database.vector_db import VectorDBClient
from shared.llm.llm_manager import llm_manager
from shared.apis.telegram_api import TelegramBotClient
from shared.service_config.user_config_loader import get_config_loader

# ë¡œì»¬ SimHash ëª¨ë“ˆ import
try:
    from .enhanced_simhash import EnhancedSimHashFilter
except ImportError:
    # ì§ì ‘ ì‹¤í–‰ ì‹œ ì ˆëŒ€ import ì‚¬ìš©
    try:
        from enhanced_simhash import EnhancedSimHashFilter
    except ImportError:
        print("ê²½ê³ : EnhancedSimHashFilterë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        EnhancedSimHashFilter = None

# ì£¼ê°€ ì¶”ì´ ë¶„ì„ ì„œë¹„ìŠ¤ import
try:
    from stock_trend_service import StockTrendService
except ImportError:
    try:
        from ..stock_trend_service import StockTrendService  
    except ImportError as e:
        print(f"ê²½ê³ : StockTrendService import ì‹¤íŒ¨: {e}")
        StockTrendService = None

# í†µê³„ ëª¨ë“ˆ import ì œê±°ë¨

# ë¡œê¹… ì„¤ì •
# ğŸ”§ ìˆ˜ì •: ë¡œê·¸ íŒŒì¼ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ìˆ˜ì •í•˜ê³  ë” ì•ˆì •ì ì¸ ì„¤ì •
current_dir = os.path.dirname(os.path.abspath(__file__))
log_dir = os.path.join(current_dir, 'logs')

# ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„± (ê¶Œí•œ ë¬¸ì œ ë°©ì§€)
try:
    os.makedirs(log_dir, exist_ok=True)
    print(f"ğŸ“ ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±/í™•ì¸: {log_dir}")
except Exception as e:
    print(f"âš ï¸ ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„± ì‹¤íŒ¨: {e}")
    # ë°±ì—…: í˜„ì¬ ë””ë ‰í† ë¦¬ì— logs í´ë” ìƒì„±
    log_dir = os.path.join(os.getcwd(), 'logs')
    os.makedirs(log_dir, exist_ok=True)
    print(f"ğŸ“ ë°±ì—… ë¡œê·¸ ë””ë ‰í† ë¦¬ ì‚¬ìš©: {log_dir}")

log_file_path = os.path.join(log_dir, 'news_service.log')

# ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±° (ì¤‘ë³µ ë°©ì§€)
for handler in logging.root.handlers[:]:
    logging.root.removeHandler(handler)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file_path, encoding='utf-8', mode='a'),
        logging.StreamHandler()
    ],
    force=True  # ê¸°ì¡´ ì„¤ì • ë®ì–´ì“°ê¸°
)

logger = logging.getLogger(__name__)

# ë¡œê·¸ íŒŒì¼ ê²½ë¡œ í™•ì¸ ë° ì¶œë ¥
logger.info(f"ğŸ“ ë¡œê·¸ íŒŒì¼ ê²½ë¡œ: {log_file_path}")
logger.info(f"ğŸ“ ë¡œê·¸ ë””ë ‰í† ë¦¬: {log_dir}")
logger.info(f"ğŸ“ ë¡œê·¸ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€: {os.path.exists(log_file_path)}")

# í…ŒìŠ¤íŠ¸ ë¡œê·¸ ë©”ì‹œì§€
logger.info("ğŸš€ ë‰´ìŠ¤ ì„œë¹„ìŠ¤ ë¡œê¹… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")

# ê¸€ë¡œë²Œ ë³€ìˆ˜
app = FastAPI(title="News Crawling Service", version="1.0.0")

# ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ê´€ë¦¬
news_service_instance = None
latest_signal_message = None  # ìµœê·¼ ì•ŒëŒ ë©”ì‹œì§€ ì €ì¥

# ë…ë¦½ì  ìŠ¤ì¼€ì¤„ë§ì„ ìœ„í•œ ë§ˆì§€ë§‰ ì‹¤í–‰ ì‹œê°„ ì¶”ì 
last_execution_time = None


def log_error_with_traceback(message: str, error: Exception, extra_info: Optional[Dict] = None):
    """ì—ëŸ¬ì™€ í•¨ê»˜ ìƒì„¸í•œ íŠ¸ë ˆì´ìŠ¤ë°± ë¡œê¹…"""
    error_details = {
        "error_type": type(error).__name__,
        "error_message": str(error),
        "traceback": traceback.format_exc()
    }
    
    if extra_info:
        error_details.update(extra_info)
    
    logger.error(f"{message}: {error_details}")

    
def log_performance(func_name: str, start_time: float, end_time: float, extra_info: Optional[Dict] = None):
    """ì„±ëŠ¥ ë¡œê¹…"""
    duration = end_time - start_time
    perf_info = {
        "function": func_name,
        "duration_seconds": round(duration, 3),
        "timestamp": datetime.now().isoformat()
    }
    
    if extra_info:
        perf_info.update(extra_info)
    
    logger.debug(f"ğŸš€ ì„±ëŠ¥: {perf_info}")


class NaverStockAPI:
    """pykrx ê¸°ë°˜ ì£¼ì‹ ì •ë³´ API (ì•ˆì „í•˜ê³  ë¹ ë¥¸ ë²„ì „)"""
    
    def __init__(self):
        try:
            from pykrx import stock
            self.stock = stock
            logger.info("âœ… pykrx ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì„±ê³µ")
        except ImportError:
            logger.error("âŒ pykrx ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install pykrxë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
            raise ImportError("pykrx ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
    def get_stock_info(self, stock_code: str) -> Dict:
        """ë‹¨ì¼ ì¢…ëª©ì˜ ì£¼ì‹ ì •ë³´ ì¡°íšŒ (pykrx ê¸°ë°˜)"""
        try:
            logger.info(f"ğŸ“Š pykrxë¡œ ì£¼ì‹ ì •ë³´ ì¡°íšŒ ì‹œì‘: {stock_code}")
            
            # ì˜¤ëŠ˜ ë‚ ì§œ
            from datetime import datetime
            today = datetime.now().strftime("%Y%m%d")
            
            # ì¢…ëª©ëª… ì¡°íšŒ
            stock_name = self.stock.get_market_ticker_name(stock_code)
            if not stock_name:
                logger.warning(f"âš ï¸ ì¢…ëª©ëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {stock_code}")
                stock_name = "ì•Œ ìˆ˜ ì—†ìŒ"
            
            # ì˜¤ëŠ˜ OHLCV ë°ì´í„° ì¡°íšŒ
            df = self.stock.get_market_ohlcv(today, today, stock_code)
            
            if df.empty:
                logger.warning(f"âš ï¸ ì˜¤ëŠ˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì–´ì œ ë°ì´í„°ë¡œ ëŒ€ì²´: {stock_code}")
                # ì–´ì œ ë‚ ì§œë¡œ ì¬ì‹œë„
                from datetime import timedelta
                yesterday = (datetime.now() - timedelta(days=1)).strftime("%Y%m%d")
                df = self.stock.get_market_ohlcv(yesterday, yesterday, stock_code)
                
                if df.empty:
                    logger.warning(f"âš ï¸ ì–´ì œ ë°ì´í„°ë„ ì—†ìŠµë‹ˆë‹¤: {stock_code}")
                    # ê¸°ë³¸ê°’ ë°˜í™˜
                    return self._get_default_stock_info(stock_code, stock_name)
            
            # ë°ì´í„° ì¶”ì¶œ
            row = df.iloc[0]
            
            # í˜„ì¬ê°€ ê³„ì‚° (ì¢…ê°€ ê¸°ì¤€)
            current_price = int(row['ì¢…ê°€'])
            
            # ë“±ë½ë¥  ê³„ì‚°
            change_rate = float(row['ë“±ë½ë¥ '])
            
            # ì „ì¼ ì¢…ê°€ ê³„ì‚°
            prev_close = int(current_price / (1 + change_rate / 100))
            
            # ê±°ë˜ëŸ‰ (ê±°ë˜ëŒ€ê¸ˆì€ ë³„ë„ ê³„ì‚° í•„ìš”)
            volume = int(row['ê±°ë˜ëŸ‰'])
            # ê±°ë˜ëŒ€ê¸ˆ = ê±°ë˜ëŸ‰ * ì¢…ê°€ (ê·¼ì‚¬ê°’)
            trading_value = volume * current_price
            
            # ì‹œê°€ì´ì•¡ ì¡°íšŒ (ì „ì²´ ì‹œì¥ì—ì„œ í•„í„°ë§)
            try:
                # ì „ì²´ ì‹œì¥ ì‹œê°€ì´ì•¡ ì¡°íšŒ í›„ í•´ë‹¹ ì¢…ëª© í•„í„°ë§
                cap_df = self.stock.get_market_cap(today)
                if not cap_df.empty and stock_code in cap_df.index:
                    market_cap = int(cap_df.loc[stock_code, 'ì‹œê°€ì´ì•¡'])
                    market_cap_formatted = self._format_market_cap(market_cap)
                else:
                    # ëŒ€ì•ˆ: ì „ì²´ ì‹œì¥ì—ì„œ ì¢…ëª©ëª…ìœ¼ë¡œ ê²€ìƒ‰
                    stock_name_search = self.stock.get_market_ticker_name(stock_code)
                    if stock_name_search and not cap_df.empty:
                        # ì¢…ëª©ëª…ìœ¼ë¡œ ê²€ìƒ‰
                        matching_rows = cap_df[cap_df.index.str.contains(stock_name_search, na=False)]
                        if not matching_rows.empty:
                            market_cap = int(matching_rows.iloc[0]['ì‹œê°€ì´ì•¡'])
                            market_cap_formatted = self._format_market_cap(market_cap)
                        else:
                            market_cap_formatted = "N/A"
                    else:
                        market_cap_formatted = "N/A"
            except Exception as e:
                logger.warning(f"âš ï¸ ì‹œê°€ì´ì•¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                market_cap_formatted = "N/A"
            
            # PER ì¡°íšŒ (ì „ì²´ ì‹œì¥ì—ì„œ í•„í„°ë§)
            try:
                # ì „ì²´ ì‹œì¥ PER ì¡°íšŒ í›„ í•´ë‹¹ ì¢…ëª© í•„í„°ë§
                fundamental_df = self.stock.get_market_fundamental(today)
                if not fundamental_df.empty and stock_code in fundamental_df.index:
                    per = float(fundamental_df.loc[stock_code, 'PER'])
                    per_formatted = f"{per:.2f}" if per > 0 else "N/A"
                else:
                    # ëŒ€ì•ˆ: ì „ì²´ ì‹œì¥ì—ì„œ ì¢…ëª©ëª…ìœ¼ë¡œ ê²€ìƒ‰
                    stock_name_search = self.stock.get_market_ticker_name(stock_code)
                    if stock_name_search and not fundamental_df.empty:
                        # ì¢…ëª©ëª…ìœ¼ë¡œ ê²€ìƒ‰
                        matching_rows = fundamental_df[fundamental_df.index.str.contains(stock_name_search, na=False)]
                        if not matching_rows.empty:
                            per = float(matching_rows.iloc[0]['PER'])
                            per_formatted = f"{per:.2f}" if per > 0 else "N/A"
                        else:
                            per_formatted = "N/A"
                    else:
                        per_formatted = "N/A"
            except Exception as e:
                logger.warning(f"âš ï¸ PER ì¡°íšŒ ì‹¤íŒ¨: {e}")
                per_formatted = "N/A"
            
            # ê²°ê³¼ êµ¬ì„±
            stock_info = {
                "ì¢…ëª©ëª…": stock_name,
                "í˜„ì¬ê°€": f"{current_price:,}",
                "ë“±ë½ë¥ ": f"{change_rate:+.2f}%",
                "ì „ì¼": f"{prev_close:,}",
                "ì‹œê°€": f"{int(row['ì‹œê°€']):,}",
                "ê³ ê°€": f"{int(row['ê³ ê°€']):,}",
                "ê±°ë˜ëŸ‰": f"{volume:,}",
                "ê±°ë˜ëŒ€ê¸ˆ": f"{trading_value:,}",
                "ì‹œê°€ì´ì•¡": market_cap_formatted,
                "PER": per_formatted
            }
            
            logger.info(f"âœ… ì£¼ì‹ ì •ë³´ ì¡°íšŒ ì™„ë£Œ: {stock_name} ({stock_code})")
            logger.debug(f"ğŸ“Š ì¡°íšŒëœ ì •ë³´: {stock_info}")
            
            return stock_info
            
        except Exception as e:
            logger.error(f"âŒ pykrx ì£¼ì‹ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            log_error_with_traceback("pykrx ì£¼ì‹ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨", e, {"stock_code": stock_code})
            return self._get_default_stock_info(stock_code, "ì•Œ ìˆ˜ ì—†ìŒ")
    
    def _get_default_stock_info(self, stock_code: str, stock_name: str) -> Dict:
        """ê¸°ë³¸ ì£¼ì‹ ì •ë³´ ë°˜í™˜"""
        return {
            "ì¢…ëª©ëª…": stock_name,
            "í˜„ì¬ê°€": "N/A",
            "ë“±ë½ë¥ ": "N/A",
            "ì „ì¼": "N/A",
            "ì‹œê°€": "N/A",
            "ê³ ê°€": "N/A",
            "ê±°ë˜ëŸ‰": "N/A",
            "ê±°ë˜ëŒ€ê¸ˆ": "N/A",
            "ì‹œê°€ì´ì•¡": "N/A",
            "PER": "N/A"
        }

    def _format_market_cap(self, market_cap: int) -> str:
        """ì‹œê°€ì´ì•¡ í¬ë§·íŒ…"""
        if market_cap >= 1_000_000_000_000:  # 1ì¡° ì´ìƒ
            return f"{market_cap / 1_000_000_000_000:.1f}ì¡°ì›"
        elif market_cap >= 1_000_000_000:  # 10ì–µ ì´ìƒ
            return f"{market_cap / 1_000_000_000:.1f}ì–µì›"
        else:
            return f"{market_cap:,}ì›"
    
    def create_chrome_driver(self, headless: bool = True) -> None:
        """pykrxëŠ” Seleniumì´ í•„ìš” ì—†ìœ¼ë¯€ë¡œ ë¹ˆ ë©”ì„œë“œ"""
        logger.debug("ğŸ”§ pykrxëŠ” Seleniumì´ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        pass


class NewsService:
    """ë‰´ìŠ¤ ì„œë¹„ìŠ¤ ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """ë‰´ìŠ¤ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        logger.info("ğŸš€ NewsService ì´ˆê¸°í™” ì‹œì‘")
        
        try:
            # ì„¤ì • ë¡œë“œ (ê°œë°œì ê¸°ë³¸ê°’)
            logger.debug("ğŸ“‹ ì„¤ì • ë¡œë“œ ì¤‘...")
            self.config = get_config()
            
            # ì‚¬ìš©ì ì„¤ì • ê´€ë¦¬ì ì´ˆê¸°í™”
            self.user_config_manager = user_config_manager
            self.current_user_id = os.environ.get('HYPERASSET_USER_ID', "1")  # ğŸ”¥ í™˜ê²½ë³€ìˆ˜ì—ì„œ ì‚¬ìš©ì ID ì½ê¸°
            
            # ì‚¬ìš©ìë³„ ê°œì¸í™” ì„¤ì • ë¡œë” ì´ˆê¸°í™”
            self.user_config_loader = None  # ë¹„ë™ê¸°ë¡œ ì´ˆê¸°í™”ë¨
            self.personalized_configs = {}  # ì‚¬ìš©ìë³„ ê°œì¸í™” ì„¤ì • ìºì‹œ
            logger.debug("âœ… ì„¤ì • ë¡œë“œ ì™„ë£Œ")
            
            # HyperCLOVA API í‚¤ ì½ê¸° ë° LLM í´ë¼ì´ì–¸íŠ¸ì— ì „ë‹¬
            hyperclova_api_key = None
            hyperclova_api_url = None
            try:
                hyperclova_config = self.config.get("hyperclova", {})
                hyperclova_api_key = hyperclova_config.get("api_key")
                hyperclova_api_url = hyperclova_config.get("api_url")
                
                if not hyperclova_api_key:
                    logger.warning("âš ï¸ HyperCLOVA API í‚¤ê°€ ì„¤ì •ì— ì—†ìŠµë‹ˆë‹¤. LLM ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")
                else:
                    logger.info(f"âœ… HyperCLOVA API í‚¤ ë¡œë“œ ì™„ë£Œ: {hyperclova_api_key[:10]}...")
                    logger.info(f"âœ… HyperCLOVA API URL: {hyperclova_api_url}")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ HyperCLOVA API í‚¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
                hyperclova_api_key = None
                hyperclova_api_url = None
            
            # ë°ì´í„°ë² ì´ìŠ¤ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            logger.debug("ğŸ—„ï¸ MySQL í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘...")
            self.mysql_client = get_mysql_client()
            logger.debug("âœ… MySQL í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
            
            # Vector DB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (news_service ì „ìš© ê²½ë¡œ ì‚¬ìš©)
            logger.debug("ğŸ” Vector DB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘...")
            
            # news_service ì „ìš© ChromaDB ê²½ë¡œ ì„¤ì •
            self.news_service_path = os.path.join(os.path.dirname(__file__), "data", "chroma")
            os.makedirs(self.news_service_path, exist_ok=True)
            
            # í™˜ê²½ë³€ìˆ˜ ì„ì‹œ ì„¤ì •
            original_chroma_path = os.environ.get("CHROMADB_PERSIST_DIRECTORY", None)
            os.environ["CHROMADB_PERSIST_DIRECTORY"] = self.news_service_path
            
            try:
                self.vector_db = VectorDBClient()
                logger.debug(f"âœ… Vector DB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ: {self.news_service_path}")
            finally:
                # ì›ë˜ í™˜ê²½ë³€ìˆ˜ ë³µì›
                if original_chroma_path:
                    os.environ["CHROMADB_PERSIST_DIRECTORY"] = original_chroma_path
                elif "CHROMADB_PERSIST_DIRECTORY" in os.environ:
                    del os.environ["CHROMADB_PERSIST_DIRECTORY"]
            
            logger.debug("ğŸ¤– LLM ë§¤ë‹ˆì € ì´ˆê¸°í™” ì¤‘...")
            self.llm_manager = llm_manager
            logger.debug("âœ… LLM ë§¤ë‹ˆì € ì´ˆê¸°í™” ì™„ë£Œ")
            
            logger.debug("ğŸ“± Telegram Bot í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘...")
            self.telegram_bot = TelegramBotClient()
            logger.debug("âœ… Telegram Bot í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")

            # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
            logger.debug("ğŸ§  ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
            self.embedding_model = SentenceTransformer("jhgan/ko-sroberta-multitask")
            logger.debug("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

            # Chrome Driver ê´€ë¦¬
            logger.debug("ğŸŒ Chrome Driver ì„¤ì • ì¤‘...")
            self.driver = None
            self.driver_lock = Lock()
            self.driver_retry_count = 0
            self.max_driver_retries = 3
            self.chrome_options = None
            
            try:
                self.setup_chrome_driver()
                logger.debug("âœ… Chrome Driver ì„¤ì • ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"âš ï¸ Chrome Driver ì„¤ì • ì‹¤íŒ¨: {e}")
                # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì¬ì‹œë„
                try:
                    self.chrome_options = Options()
                    self.chrome_options.add_argument("--headless")
                    self.chrome_options.add_argument("--no-sandbox")
                    self.chrome_options.add_argument("--disable-dev-shm-usage")
                    logger.info("âœ… ê¸°ë³¸ Chrome Driver ì„¤ì •ìœ¼ë¡œ ì¬ì‹œë„ ì™„ë£Œ")
                except Exception as e2:
                    logger.error(f"âŒ ê¸°ë³¸ Chrome Driver ì„¤ì •ë„ ì‹¤íŒ¨: {e2}")
                    self.chrome_options = None

            # í¬ë¡¤ë§ ì„¤ì •
            self.news_sources = [
                "https://finance.naver.com/news/news_list.naver?mode=LSS2&section_id=101&section_id2=258",
                "https://news.naver.com/main/list.naver?mode=LSD&mid=sec&sid1=101",
            ]

            # ì¤‘ë³µ ì œê±° ì„ê³„ê°’ (ê°œë°œì ê¸°ë³¸ê°’)
            self.simhash_threshold = 3
            self.vector_similarity_threshold = self.config.get("NEWS_SIMILARITY_THRESHOLD", 1.1)
            self.impact_threshold = self.config.get("NEWS_IMPACT_THRESHOLD", 0.5)
            self.relevance_threshold = self.config.get("NEWS_RELEVANCE_THRESHOLD", 0.6)
            self.past_events_similarity_threshold = 0.5  # ê³¼ê±° ì‚¬ë¡€ ìœ ì‚¬ë„ ì„ê³„ê°’
            
            # ì‚¬ìš©ìë³„ ì„¤ì • ë¡œë“œ (MySQLì—ì„œ ë®ì–´ì“°ê¸°)
            # asyncio.create_task(self._load_user_settings())  # ì´ë²¤íŠ¸ ë£¨í”„ ì‹¤í–‰ í›„ í˜¸ì¶œ
            
            # í–¥ìƒëœ SimHash í•„í„° ì´ˆê¸°í™”
            logger.debug("ğŸ”§ SimHash í•„í„° ì´ˆê¸°í™” ì¤‘...")
            # SkillStack ì¤‘ë³µ ì œê±° í•„í„° ì´ˆê¸°í™”
            try:
                if EnhancedSimHashFilter is not None:
                    self.simhash_filter = EnhancedSimHashFilter()
                    logger.info("âœ… SkillStack SimHash í•„í„° ì´ˆê¸°í™” ì™„ë£Œ")
                else:
                    logger.warning("âš ï¸ EnhancedSimHashFilter í´ë˜ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    self.simhash_filter = None
            except Exception as e:
                logger.error(f"âŒ SkillStack SimHash í•„í„° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.simhash_filter = None

            # ì¢…ëª© ëª©ë¡ ë¡œë“œ
            logger.debug("ğŸ“Š ì¢…ëª© ëª©ë¡ ë¡œë“œ ì¤‘...")
            self.load_stock_codes()
            logger.debug("âœ… ì¢…ëª© ëª©ë¡ ë¡œë“œ ì™„ë£Œ")
            
            # ì¢…ëª© ì •ë³´ API ì´ˆê¸°í™”
            logger.debug("ğŸ“ˆ ì¢…ëª© ì •ë³´ API ì´ˆê¸°í™” ì¤‘...")
            self.stock_api = NaverStockAPI()
            logger.debug("âœ… ì¢…ëª© ì •ë³´ API ì´ˆê¸°í™” ì™„ë£Œ")
            
            # ì£¼ê°€ ì¶”ì´ ë¶„ì„ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
            logger.debug("ğŸ“Š ì£¼ê°€ ì¶”ì´ ë¶„ì„ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
            if StockTrendService is not None:
                try:
                    self.stock_trend_service = StockTrendService()
                    logger.debug("âœ… ì£¼ê°€ ì¶”ì´ ë¶„ì„ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
                except Exception as e:
                    logger.error(f"âŒ ì£¼ê°€ ì¶”ì´ ë¶„ì„ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    self.stock_trend_service = None
            else:
                logger.warning("âš ï¸ StockTrendService import ì‹¤íŒ¨ë¡œ ì£¼ê°€ ì¶”ì´ ë¶„ì„ ê¸°ëŠ¥ ë¹„í™œì„±í™”")
                self.stock_trend_service = None

            # ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ìºì‹œ ì‹œìŠ¤í…œ
            self.embedding_cache = {}  # ì„ë² ë”© ìºì‹œ
            self.similarity_cache = {}  # ìœ ì‚¬ë„ ìºì‹œ
            self.news_cache = []  # í˜„ì¬ ì„¸ì…˜ ë‰´ìŠ¤ ìºì‹œ
            self.cache_lock = Lock()

            # í†µê³„ ìˆ˜ì§‘ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì œê±°ë¨

            # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
            logger.debug("ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
            self.init_database()
            logger.debug("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
            
            # CSV ì €ì¥ ì„¤ì •
            logger.debug("ğŸ“Š CSV ì¶œë ¥ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
            self.csv_output_dir = Path("output/csv")
            self.csv_output_dir.mkdir(parents=True, exist_ok=True)
            
            # CSV íŒŒì¼ í—¤ë”
            self.csv_headers = [
                "timestamp", "stock_code", "stock_name", "title", "content", 
                "url", "source", "published_at", "impact_score", "reasoning",
                "relevance_score", "similarity_score", "filter_stage", 
                "current_price", "change_rate", "telegram_sent", "processing_time"
            ]
            
            # ì¼ì¼ CSV íŒŒì¼ ì´ˆê¸°í™”
            self.daily_csv_file = self.csv_output_dir / f"news_results_{datetime.now().strftime('%Y%m%d')}.csv"
            self.init_daily_csv()
            logger.debug("âœ… CSV ì¶œë ¥ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            
            logger.info("âœ… NewsService ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            error_type = type(e).__name__
            logger.error(f"âŒ NewsService ì´ˆê¸°í™” ì‹¤íŒ¨ [{error_type}]: {e}")
            log_error_with_traceback("NewsService ì´ˆê¸°í™” ì‹¤íŒ¨", e)
            raise RuntimeError(f"NewsService ì´ˆê¸°í™” ì‹¤íŒ¨: {error_type} - {e}")

    async def _load_user_settings(self):
        """ì‚¬ìš©ìë³„ ì„¤ì • ë¡œë“œ (User Config Managerì—ì„œ ì¤‘ì•™ ì§‘ì¤‘ì‹ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°)"""
        try:
            user_config = await self.user_config_manager.get_user_config(self.current_user_id)
            
            # ì‚¬ìš©ìë³„ ì„ê³„ê°’ìœ¼ë¡œ ë®ì–´ì“°ê¸° (ê¸°ë³¸ê°’ì€ configì—ì„œ ìœ ì§€)
            self.vector_similarity_threshold = user_config.get("news_similarity_threshold", self.vector_similarity_threshold)
            self.impact_threshold = user_config.get("news_impact_threshold", self.impact_threshold)
            
            # ì‚¬ìš©ì ì¢…ëª© ì„¤ì •ìœ¼ë¡œ ë®ì–´ì“°ê¸°
            self.stocks_config = {}
            for stock in user_config.get("stocks", []):
                if stock.get("enabled", True):
                    self.stocks_config[stock["stock_code"]] = {
                        "name": stock["stock_name"],
                        "enabled": True
                    }
            
            logger.info(f"âœ… ì‚¬ìš©ì ì„¤ì • ë¡œë“œ ì™„ë£Œ: {len(self.stocks_config)}ê°œ ì¢…ëª©, "
                       f"ìœ ì‚¬ë„ì„ê³„ê°’={self.vector_similarity_threshold}, "
                       f"ì˜í–¥ë„ì„ê³„ê°’={self.impact_threshold}")
            
        except Exception as e:
            logger.error(f"âŒ ì‚¬ìš©ì ì„¤ì • ë¡œë“œ ì‹¤íŒ¨ (ê¸°ë³¸ê°’ ìœ ì§€): {e}")
            # ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ì¢…ëª© ì„¤ì •
            self.stocks_config = {
                "005930": {"name": "ì‚¼ì„±ì „ì", "enabled": True},
                "000660": {"name": "SKí•˜ì´ë‹‰ìŠ¤", "enabled": True}
            }
    
    async def set_user_id(self, user_id):
        """ì‚¬ìš©ì ID ì„¤ì • ë° ì„¤ì • ì¬ë¡œë“œ"""
        try:
            self.current_user_id = user_id
            await self._load_user_settings()
            logger.info(f"âœ… ì‚¬ìš©ì ID ì„¤ì • ë° ì„¤ì • ì¬ë¡œë“œ ì™„ë£Œ: {user_id}")
        except Exception as e:
            logger.error(f"âŒ ì‚¬ìš©ì ID ì„¤ì • ì‹¤íŒ¨: {e}")
            raise

    def setup_chrome_driver(self):
        """Chrome Driver ì„¤ì • - ê°œì„ ëœ ì•ˆì • ë²„ì „"""
        try:
            self.chrome_options = Options()
            
            # ê¸°ë³¸ ì•ˆì •ì„± ì˜µì…˜
            self.chrome_options.add_argument("--headless")
            self.chrome_options.add_argument("--no-sandbox")
            self.chrome_options.add_argument("--disable-dev-shm-usage")
            self.chrome_options.add_argument("--disable-gpu")
            self.chrome_options.add_argument("--window-size=1920,1080")
            self.chrome_options.add_argument("--disable-web-security")
            self.chrome_options.add_argument("--disable-features=VizDisplayCompositor")
            self.chrome_options.add_argument("--disable-extensions")
            self.chrome_options.add_argument("--disable-plugins")
            self.chrome_options.add_argument("--disable-images")
            
            # ì¶”ê°€ ì•ˆì •ì„± ì˜µì…˜
            self.chrome_options.add_argument("--disable-background-timer-throttling")
            self.chrome_options.add_argument("--disable-backgrounding-occluded-windows")
            self.chrome_options.add_argument("--disable-renderer-backgrounding")
            self.chrome_options.add_argument("--disable-ipc-flooding-protection")
            self.chrome_options.add_argument("--disable-default-apps")
            self.chrome_options.add_argument("--disable-sync")
            self.chrome_options.add_argument("--disable-translate")
            self.chrome_options.add_argument("--disable-logging")
            self.chrome_options.add_argument("--disable-notifications")
            self.chrome_options.add_argument("--disable-popup-blocking")
            
            # ë©”ëª¨ë¦¬ ìµœì í™”
            self.chrome_options.add_argument("--memory-pressure-off")
            self.chrome_options.add_argument("--max_old_space_size=4096")
            self.chrome_options.add_argument("--disable-javascript")
            
            # User Agent ì„¤ì •
            self.chrome_options.add_argument("--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
            
            # ìë™í™” ê°ì§€ ë°©ì§€
            self.chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            self.chrome_options.add_experimental_option('useAutomationExtension', False)
            
            logger.info("âœ… Chrome Driver ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ Chrome Driver ì„¤ì • ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ì˜µì…˜ìœ¼ë¡œ ì¬ì‹œë„
            try:
                self.chrome_options = Options()
                self.chrome_options.add_argument("--headless")
                self.chrome_options.add_argument("--no-sandbox")
                self.chrome_options.add_argument("--disable-dev-shm-usage")
                logger.info("âœ… ê¸°ë³¸ Chrome Driver ì„¤ì •ìœ¼ë¡œ ì¬ì‹œë„ ì™„ë£Œ")
            except Exception as e2:
                logger.error(f"âŒ ê¸°ë³¸ Chrome Driver ì„¤ì •ë„ ì‹¤íŒ¨: {e2}")

    def get_driver(self):
        """Chrome Driver ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸° (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
        with self.driver_lock:
            if self.driver is None or not self._is_driver_alive():
                try:
                    if self.driver:
                        try:
                            self.driver.quit()
                        except:
                            pass
                    
                    # ChromeDriverManager ì„¤ì • ê°œì„ 
                    try:
                        # chrome_optionsê°€ Noneì¸ ê²½ìš° ê¸°ë³¸ ì„¤ì • ìƒì„±
                        if self.chrome_options is None:
                            self.chrome_options = Options()
                            self.chrome_options.add_argument("--headless")
                            self.chrome_options.add_argument("--no-sandbox")
                            self.chrome_options.add_argument("--disable-dev-shm-usage")
                            logger.info("âœ… ê¸°ë³¸ Chrome ì˜µì…˜ ìƒì„±")
                        
                        # ë” ì•ˆì •ì ì¸ ë“œë¼ì´ë²„ ì„¤ì¹˜
                        driver_path = ChromeDriverManager().install()
                        logger.info(f"âœ… Chrome Driver ì„¤ì¹˜ ì™„ë£Œ: {driver_path}")
                        
                        service = Service(driver_path)
                        
                        # Chrome ì˜µì…˜ ì¶”ê°€ ê°œì„  - ë” ì•ˆì •ì ì¸ ì„¤ì •
                        self.chrome_options.add_argument("--disable-blink-features=AutomationControlled")
                        self.chrome_options.add_argument("--disable-web-security")
                        self.chrome_options.add_argument("--allow-running-insecure-content")
                        self.chrome_options.add_argument("--disable-features=TranslateUI")
                        self.chrome_options.add_argument("--disable-ipc-flooding-protection")
                        self.chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
                        self.chrome_options.add_experimental_option('useAutomationExtension', False)
                        
                        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”
                        self.chrome_options.add_argument("--memory-pressure-off")
                        self.chrome_options.add_argument("--max_old_space_size=4096")
                        
                        # ì¶”ê°€ ì•ˆì •ì„± ì˜µì…˜
                        self.chrome_options.add_argument("--disable-background-timer-throttling")
                        self.chrome_options.add_argument("--disable-backgrounding-occluded-windows")
                        self.chrome_options.add_argument("--disable-renderer-backgrounding")
                        
                        # Chrome Driver ìƒì„± ì‹œ ë” ê¸´ íƒ€ì„ì•„ì›ƒ ì„¤ì •
                        self.driver = webdriver.Chrome(service=service, options=self.chrome_options)
                        self.driver.set_page_load_timeout(60)  # 30ì´ˆ â†’ 60ì´ˆë¡œ ì¦ê°€
                        self.driver.implicitly_wait(15)        # 10ì´ˆ â†’ 15ì´ˆë¡œ ì¦ê°€
                        
                        # ë¸Œë¼ìš°ì € ì°½ í¬ê¸° ì„¤ì •
                        self.driver.set_window_size(1920, 1080)
                        
                        # ìë™í™” ê°ì§€ ë°©ì§€
                        try:
                            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
                        except Exception as e:
                            logger.debug(f"ìë™í™” ê°ì§€ ë°©ì§€ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
                        
                        self.driver_retry_count = 0
                        logger.info("âœ… Chrome Driver ìƒì„± ì™„ë£Œ")
                        
                    except Exception as e:
                        logger.error(f"âŒ ChromeDriverManager ì„¤ì¹˜ ì‹¤íŒ¨: {e}")
                        
                        # ëŒ€ì•ˆ: ì‹œìŠ¤í…œì— ì„¤ì¹˜ëœ Chrome ë“œë¼ì´ë²„ ì‚¬ìš©
                        try:
                            logger.info("ğŸ”„ ì‹œìŠ¤í…œ Chrome ë“œë¼ì´ë²„ ì‚¬ìš© ì‹œë„...")
                            self.driver = webdriver.Chrome(options=self.chrome_options)
                            self.driver.set_page_load_timeout(30)
                            self.driver.implicitly_wait(10)
                            self.driver_retry_count = 0
                            logger.info("âœ… ì‹œìŠ¤í…œ Chrome ë“œë¼ì´ë²„ ì‚¬ìš© ì„±ê³µ")
                        except Exception as e2:
                            logger.error(f"âŒ ì‹œìŠ¤í…œ Chrome ë“œë¼ì´ë²„ë„ ì‹¤íŒ¨: {e2}")
                            raise e2
                    
                except Exception as e:
                    self.driver_retry_count += 1
                    logger.error(f"âŒ Chrome Driver ìƒì„± ì‹¤íŒ¨ ({self.driver_retry_count}/{self.max_driver_retries}): {e}")
                    
                    # ìƒì„¸ ì˜¤ë¥˜ ì •ë³´ ë¡œê¹…
                    error_msg = str(e).lower()
                    if "can not connect to the service" in error_msg:
                        logger.error("ğŸ”§ í•´ê²°ë°©ë²•: Chrome ë¸Œë¼ìš°ì €ë¥¼ ì—…ë°ì´íŠ¸í•˜ê±°ë‚˜ ì‹œìŠ¤í…œì„ ì¬ë¶€íŒ…í•´ë³´ì„¸ìš”")
                    elif "permission denied" in error_msg:
                        logger.error("ğŸ”§ í•´ê²°ë°©ë²•: Chrome Driver ì‹¤í–‰ ê¶Œí•œì„ í™•ì¸í•˜ì„¸ìš”")
                    elif "chromedriver" in error_msg:
                        logger.error("ğŸ”§ í•´ê²°ë°©ë²•: ChromeDriverë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì„¤ì¹˜í•˜ê±°ë‚˜ PATHì— ì¶”ê°€í•˜ì„¸ìš”")
                    elif "session not created" in error_msg:
                        logger.error("ğŸ”§ í•´ê²°ë°©ë²•: Chrome ë¸Œë¼ìš°ì € ë²„ì „ê³¼ ë“œë¼ì´ë²„ ë²„ì „ì´ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
                    elif "no such file or directory" in error_msg:
                        logger.error("ğŸ”§ í•´ê²°ë°©ë²•: Chrome ë¸Œë¼ìš°ì €ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”")
                    elif "timeout" in error_msg:
                        logger.error("ğŸ”§ í•´ê²°ë°©ë²•: ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”")
                    else:
                        logger.error(f"ğŸ”§ ì•Œ ìˆ˜ ì—†ëŠ” Chrome Driver ì˜¤ë¥˜: {e}")
                    
                    if self.driver_retry_count >= self.max_driver_retries:
                        logger.error("âŒ Chrome Driver ìƒì„± ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼ - requests ê¸°ë°˜ í¬ë¡¤ë§ìœ¼ë¡œ ëŒ€ì²´")
                        # í¬ë¡¬ë“œë¼ì´ë²„ ê´€ë ¨ ë³€ìˆ˜ ì •ë¦¬
                        self.driver = None
                        self.chrome_options = None
                        return None
                    
                    # ì¬ì‹œë„ ì „ ëŒ€ê¸° (ì§€ìˆ˜ ë°±ì˜¤í”„)
                    wait_time = min(2 ** self.driver_retry_count, 30)  # ìµœëŒ€ 30ì´ˆë¡œ ì œí•œ
                    logger.info(f"â³ {wait_time}ì´ˆ í›„ ì¬ì‹œë„...")
                    time.sleep(wait_time)
                    return self.get_driver()
            
            return self.driver

    def _is_driver_alive(self):
        """Chrome Driver ìƒì¡´ ì—¬ë¶€ í™•ì¸ (ê°œì„ ëœ ë²„ì „)"""
        try:
            if self.driver:
                # ê°„ë‹¨í•œ ëª…ë ¹ìœ¼ë¡œ ë“œë¼ì´ë²„ ìƒíƒœ í™•ì¸
                self.driver.current_url
                return True
        except (WebDriverException, Exception) as e:
            logger.debug(f"Chrome Driver ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
            return False
        return False

    def close_driver(self):
        """Chrome Driver ì¢…ë£Œ (ê°œì„ ëœ ë²„ì „)"""
        with self.driver_lock:
            if self.driver:
                try:
                    # ëª¨ë“  ì°½ ë‹«ê¸°
                    self.driver.quit()
                    logger.info("âœ… Chrome Driver ì¢…ë£Œ ì™„ë£Œ")
                except Exception as e:
                    logger.warning(f"âš ï¸ Chrome Driver ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")
                    try:
                        # ê°•ì œ ì¢…ë£Œ ì‹œë„
                        self.driver.close()
                    except:
                        pass
                finally:
                    self.driver = None
                    self.driver_retry_count = 0

    def init_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        try:
            # ë²¡í„° DB ìƒíƒœ í™•ì¸ (ì»¬ë ‰ì…˜ì€ VectorDBClientì—ì„œ ì´ë¯¸ ìƒì„±ë¨)
            health_status = self.vector_db.health_check()
            
            if health_status["status"] == "healthy":
                logger.info("âœ… ë²¡í„° DB ìƒíƒœ í™•ì¸ ì™„ë£Œ")
                for collection_name, status in health_status["collections"].items():
                    if status["status"] == "healthy":
                        logger.debug(f"âœ… ì»¬ë ‰ì…˜ '{collection_name}' ì •ìƒ (ë¬¸ì„œ {status['count']}ê°œ)")
                    else:
                        logger.warning(f"âš ï¸ ì»¬ë ‰ì…˜ '{collection_name}' ìƒíƒœ ì´ìƒ: {status.get('error', 'Unknown')}")
            else:
                logger.error(f"âŒ ë²¡í„° DB ìƒíƒœ ì´ìƒ: {health_status.get('error', 'Unknown')}")
            
            logger.info("âœ… ë‰´ìŠ¤ ì„œë¹„ìŠ¤ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

        except Exception as e:
            logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    def load_stock_codes(self):
        """ì¢…ëª© ì½”ë“œ ë¡œë“œ"""
        try:
            # ì—¬ëŸ¬ ê²½ë¡œ ì‹œë„
            possible_paths = [
                "config/stocks.json",
                "../../config/stocks.json",
                "../../../config/stocks.json",
                "stock_analysis_service/config/stocks.json"
            ]
            
            stocks_data = None
            used_path = None
            
            for path in possible_paths:
                try:
                    with open(path, "r", encoding="utf-8") as f:
                        stocks_data = json.load(f)
                        used_path = path
                        logger.debug(f"âœ… ì¢…ëª© ì½”ë“œ íŒŒì¼ ë¡œë“œ ì„±ê³µ: {path}")
                        break
                except FileNotFoundError:
                    continue
                except Exception as e:
                    logger.debug(f"âŒ ì¢…ëª© ì½”ë“œ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ [{path}]: {e}")
                    continue
            
            if stocks_data:
                self.stock_codes = [stock["code"] for stock in stocks_data["stocks"]]
                self.stock_names = {
                    stock["code"]: stock["name"] for stock in stocks_data["stocks"]
                }
                logger.info(f"âœ… ì¢…ëª© ì½”ë“œ ë¡œë“œ ì„±ê³µ: {len(self.stock_codes)}ê°œ ì¢…ëª© ({used_path})")
            else:
                raise FileNotFoundError("ëª¨ë“  ê²½ë¡œì—ì„œ stocks.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                
        except Exception as e:
            logger.error(f"âŒ ì¢…ëª© ì½”ë“œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            logger.warning("âš ï¸ ê¸°ë³¸ ì¢…ëª© ì½”ë“œ ì‚¬ìš©: ë¯¸ë˜ì—ì…‹ì¦ê¶Œ")
            self.stock_codes = ["006800"]
            self.stock_names = {"006800": "ë¯¸ë˜ì—ì…‹ì¦ê¶Œ"}

    def is_market_open(self) -> bool:
        """ì¥ì¤‘ ì—¬ë¶€ í™•ì¸"""
        now = datetime.now()
        if now.weekday() >= 5:
            return False

        market_open = now.replace(hour=9, minute=0, second=0, microsecond=0)
        market_close = now.replace(hour=15, minute=30, second=0, microsecond=0)

        return market_open <= now <= market_close

    def get_cached_embedding(self, text: str) -> Optional[np.ndarray]:
        """ìºì‹œëœ ì„ë² ë”© ì¡°íšŒ"""
        text_hash = hashlib.md5(text.encode()).hexdigest()
        with self.cache_lock:
            return self.embedding_cache.get(text_hash)

    def cache_embedding(self, text: str, embedding: np.ndarray):
        """ì„ë² ë”© ìºì‹œ ì €ì¥"""
        text_hash = hashlib.md5(text.encode()).hexdigest()
        with self.cache_lock:
            self.embedding_cache[text_hash] = embedding
            
            # ìºì‹œ í¬ê¸° ì œí•œ (1000ê°œ)
            if len(self.embedding_cache) > 1000:
                # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì‚­ì œ
                oldest_key = next(iter(self.embedding_cache))
                del self.embedding_cache[oldest_key]

    def calculate_simhash(self, text: str) -> int:
        """SimHash ê³„ì‚°"""
        try:
            tokens = text.split()
            token_hashes = []
            for token in tokens:
                hash_value = int(hashlib.md5(token.encode()).hexdigest(), 16)
                token_hashes.append(hash_value)

            if token_hashes:
                simhash_obj = simhash.Simhash(token_hashes)
                return int(simhash_obj.value) if simhash_obj.value is not None else 0

        except Exception as e:
            logger.error(f"SimHash ê³„ì‚° ì‹¤íŒ¨: {e}")

        return 0

    def build_news_url(self, item: Dict) -> str:
        """ë‰´ìŠ¤ URL ìƒì„± (SkillStack ë°©ì‹)"""
        # linkUrl ìš°ì„ , ì—†ìœ¼ë©´ officeId/articleId ì¡°í•©
        if item.get("linkUrl"):
            return item["linkUrl"]
        if item.get("url"):
            return item["url"]
        if item.get("officeId") and item.get("articleId"):
            return f"https://n.news.naver.com/article/{item['officeId']}/{item['articleId']}"
        return ""
    
    def format_news_date(self, dt: str) -> datetime:
        """ë‰´ìŠ¤ ë‚ ì§œ í¬ë§·íŒ… (SkillStack ë°©ì‹)"""
        try:
            return datetime.strptime(dt, "%Y%m%d%H%M")
        except Exception:
            return datetime.now()
    
    def get_stock_info_for_code(self, stock_code: str) -> Dict:
        """ì¢…ëª© ì •ë³´ ìˆ˜ì§‘ (SkillStack ë°©ì‹) - 4ë‹¨ê³„ ì¤€ë¹„"""
        try:
            logger.debug(f"ğŸ“Š ì¢…ëª© ì •ë³´ ìˆ˜ì§‘ ì‹œì‘: {stock_code}")
            
            stock_info = self.stock_api.get_stock_info(stock_code)
            stock_info["ì¢…ëª©ì½”ë“œ"] = stock_code
            
            if stock_info.get("ì¢…ëª©ëª…"):
                # ìƒì„¸ ì •ë³´ ë¡œê¹…
                stock_name = stock_info.get("ì¢…ëª©ëª…", "")
                current_price = stock_info.get("í˜„ì¬ê°€", "")
                change_rate = stock_info.get("ë“±ë½ë¥ ", "")
                trading_volume = stock_info.get("ê±°ë˜ëŸ‰", "")
                trading_value = stock_info.get("ê±°ë˜ëŒ€ê¸ˆ", "")
                market_cap = stock_info.get("ì‹œê°€ì´ì•¡", "")
                per_ratio = stock_info.get("PER", "")
                
                logger.debug(f"ğŸ“ˆ ê¸°ë³¸ì •ë³´: {stock_name} ({stock_code})")
                logger.debug(f"ğŸ’° ê°€ê²©ì •ë³´: í˜„ì¬ê°€={current_price}, ë“±ë½ë¥ ={change_rate}")
                logger.debug(f"ğŸ“Š ê±°ë˜ì •ë³´: ê±°ë˜ëŸ‰={trading_volume}, ê±°ë˜ëŒ€ê¸ˆ={trading_value}")
                logger.debug(f"ğŸ“Š ê¸°ì—…ì •ë³´: ì‹œê°€ì´ì•¡={market_cap}, PER={per_ratio}")
                
                logger.info(f"âœ… ì¢…ëª© ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ: {stock_name} - í˜„ì¬ê°€: {current_price}")
                logger.debug(f"ğŸ”® 4ë‹¨ê³„ ì¤€ë¹„: ì¢…ëª© ì •ë³´ ì™„ì „ ìˆ˜ì§‘ ì™„ë£Œ - DB ì €ì¥ ì¤€ë¹„ë¨")
            else:
                logger.warning(f"âš ï¸ ì¢…ëª© ì •ë³´ ìˆ˜ì§‘ ë¶€ë¶„ ì‹¤íŒ¨: {stock_code}")
                logger.debug(f"ğŸ”® 4ë‹¨ê³„ ì¤€ë¹„: ì¢…ëª© ì •ë³´ ë¶€ë¶„ ì‹¤íŒ¨ - ê¸°ë³¸ê°’ìœ¼ë¡œ ì²˜ë¦¬")
                
            return stock_info
            
        except Exception as e:
            logger.error(f"âŒ ì¢…ëª© ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            log_error_with_traceback("ì¢…ëª© ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨", e, {"stock_code": stock_code})
            logger.debug(f"ğŸ”® 4ë‹¨ê³„ ì¤€ë¹„: ì¢…ëª© ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨ - ì—ëŸ¬ ì²˜ë¦¬ ë° ê¸°ë³¸ê°’ ë°˜í™˜")
            return {
                "ì¢…ëª©ì½”ë“œ": stock_code,
                "ì¢…ëª©ëª…": "", "í˜„ì¬ê°€": "", "ë“±ë½ë¥ ": "", "ì „ì¼": "", "ì‹œê°€": "", "ê³ ê°€": "",
                "ê±°ë˜ëŸ‰": "", "ê±°ë˜ëŒ€ê¸ˆ": "", "ì‹œê°€ì´ì•¡": "", "PER": ""
            }
    
    async def check_three_stage_filtering(self, news_item: Dict) -> Tuple[bool, str, Dict]:
        """3ë‹¨ê³„ í•„í„°ë§ ì‹œìŠ¤í…œ (SimHash â†’ ì¢…ëª©ê´€ë ¨ì„± â†’ ë²¡í„°ìœ ì‚¬ë„)"""
        start_time = time.time()
        filter_result = {
            "stage1_simhash": False,
            "stage2_relevance": False,
            "stage3_vector": False,
            "final_pass": False,
            "details": {}
        }
        
        try:
            # 1ë‹¨ê³„: SimHash ê¸°ë°˜ ì¤‘ë³µ ì œê±° (SkillStack ë°©ì‹)
            if self.simhash_filter:
                # SimHash ê³„ì‚° ë° ì¤‘ë³µ ê²€ì‚¬
                title_content = f"{news_item.get('title', '')} {news_item.get('content', '')}"
                is_duplicate = self.simhash_filter.is_duplicate(title_content)
                
                if is_duplicate:
                    logger.debug(f"ğŸ”„ SimHash ì¤‘ë³µ ë‰´ìŠ¤ í•„í„°ë§: {news_item.get('title', '')[:50]}...")
                    return False, "SimHash ì¤‘ë³µ", {"simhash_duplicate": True}
                
                logger.debug("âœ… SimHash ì¤‘ë³µ ê²€ì‚¬ í†µê³¼")
            else:
                logger.debug("âš ï¸ SimHash í•„í„° ë¹„í™œì„±í™” ìƒíƒœ")
            
            # 2ë‹¨ê³„: ì¢…ëª© ê´€ë ¨ì„± í‰ê°€ (ë¹„í™œì„±í™” - ëª¨ë“  ë‰´ìŠ¤ í†µê³¼)
            logger.debug(f"ğŸ” 2ë‹¨ê³„: ì¢…ëª© ê´€ë ¨ì„± ê²€ì‚¬ (ë¹„í™œì„±í™”ë¨ - ëª¨ë“  ë‰´ìŠ¤ í†µê³¼)")
            
            # ê´€ë ¨ì„± ê³„ì‚° ìŠ¤í‚µ - í•­ìƒ í†µê³¼ ì²˜ë¦¬
            relevance_score = 1.0
            reasoning = "ê´€ë ¨ì„± ê²€ì‚¬ ë¹„í™œì„±í™”ë¨"
            
            filter_result["details"]["relevance_score"] = relevance_score
            filter_result["details"]["relevance_reasoning"] = reasoning
            
            logger.debug(f"âœ… 2ë‹¨ê³„ ìë™ í†µê³¼: ì¢…ëª© ê´€ë ¨ì„± ê²€ì‚¬ ìŠ¤í‚µë¨")
            
            # === 3ë‹¨ê³„: ë²¡í„° ìœ ì‚¬ë„ ì¤‘ë³µ ê²€ì‚¬ ===
            logger.debug(f"ğŸ” 3ë‹¨ê³„: ë²¡í„° ìœ ì‚¬ë„ ì¤‘ë³µ ê²€ì‚¬ ì‹œì‘")
            is_vector_duplicate, similarity_score, similar_news = self.check_vector_similarity(news_item)
            
            filter_result["details"]["vector_similarity"] = similarity_score
            filter_result["details"]["similar_news"] = similar_news
            
            if is_vector_duplicate:
                logger.info(f"âŒ 3ë‹¨ê³„ í•„í„°ë§: ë²¡í„° ìœ ì‚¬ë„ ì¤‘ë³µ ê°ì§€ - '{news_item['title'][:50]}...' (ìœ ì‚¬ë„: {similarity_score:.3f})")
                filter_result["stage3_vector"] = True
                return True, "vector_duplicate", filter_result
            
            logger.debug(f"âœ… 3ë‹¨ê³„ í†µê³¼: ë²¡í„° ìœ ì‚¬ë„ ì¤‘ë³µ ì—†ìŒ")
            
            # === ëª¨ë“  ë‹¨ê³„ í†µê³¼ ===
            filter_result["final_pass"] = True
            self.news_cache.append(news_item)
            
            log_performance("check_three_stage_filtering", start_time, time.time(), {
                "news_title": news_item["title"][:50],
                "relevance_score": relevance_score,
                "vector_similarity": similarity_score,
                "cache_size": len(self.news_cache)
            })
            
            logger.info(f"âœ… ëª¨ë“  ë‹¨ê³„ í†µê³¼: '{news_item['title'][:50]}...' (ê´€ë ¨ì„±: {relevance_score:.3f})")
            return False, "passed", filter_result
            
        except Exception as e:
            log_error_with_traceback("3ë‹¨ê³„ í•„í„°ë§ ì‹œìŠ¤í…œ ì‹¤íŒ¨", e, {
                "news_title": news_item.get("title", "Unknown"),
                "news_url": news_item.get("url", "Unknown"),
                "filter_result": filter_result
            })
            return False, "error", filter_result
    
    async def _call_llm_with_retry(self, prompt: str, max_retries: int = 3, base_delay: float = 2.0) -> Optional[str]:
        """LLM API í˜¸ì¶œ ì¬ì‹œë„ ë¡œì§"""
        for attempt in range(max_retries):
            try:
                logger.debug(f"ğŸ¤– LLM API í˜¸ì¶œ ì‹œë„ {attempt + 1}/{max_retries}")
                
                response = await self.llm_manager.generate_response(self.current_user_id, prompt)
                
                if response and response.strip():
                    logger.debug(f"âœ… LLM API í˜¸ì¶œ ì„±ê³µ (ì‹œë„ {attempt + 1})")
                    return response
                else:
                    logger.warning(f"âš ï¸ LLM API ì‘ë‹µì´ ë¹„ì–´ìˆìŒ (ì‹œë„ {attempt + 1})")
                    
            except Exception as e:
                error_msg = str(e).lower()
                logger.warning(f"âš ï¸ LLM API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}): {e}")
                
                # í˜¸ì¶œ ì œí•œ ê´€ë ¨ ì˜¤ë¥˜ì¸ì§€ í™•ì¸
                is_rate_limit = any(keyword in error_msg for keyword in [
                    'rate limit', 'too many requests', '429', 'quota exceeded', 
                    'limit exceeded', 'throttle', 'throttling'
                ])
                
                if is_rate_limit:
                    # ì§€ìˆ˜ ë°±ì˜¤í”„ë¡œ ëŒ€ê¸° ì‹œê°„ ê³„ì‚°
                    wait_time = base_delay * (2 ** attempt)
                    logger.info(f"â³ í˜¸ì¶œ ì œí•œ ê°ì§€. {wait_time}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...")
                    await asyncio.sleep(wait_time)
                else:
                    # ì¼ë°˜ ì˜¤ë¥˜ëŠ” ì§§ì€ ëŒ€ê¸° í›„ ì¬ì‹œë„
                    wait_time = base_delay * (1.5 ** attempt)
                    logger.info(f"â³ ì¼ë°˜ ì˜¤ë¥˜. {wait_time}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...")
                    await asyncio.sleep(wait_time)
                
                if attempt == max_retries - 1:
                    logger.error(f"âŒ LLM API ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼")
                    return None
        
        return None

    async def evaluate_stock_relevance(self, news_item: Dict) -> Tuple[bool, float, str]:
        """ì¢…ëª© ê´€ë ¨ì„± í‰ê°€ (LLM ê¸°ë°˜)"""
        try:
            title = news_item.get('title', '')
            content = news_item.get('content', '')[:1000]  # 1000ìë¡œ ì œí•œ
            stock_code = news_item.get('stock_code', '')
            stock_name = self.stock_names.get(stock_code, stock_code)
            
            if not stock_name or not title.strip():
                return True, 1.0, "ì¢…ëª©ëª… ë˜ëŠ” ì œëª© ì •ë³´ ì—†ìŒ"
            
            prompt = f"""
ë‹¹ì‹ ì€ í•œêµ­ ì¦ê¶Œì‹œì¥ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ë‹¤ìŒ ë‰´ìŠ¤ê°€ í•´ë‹¹ ì¢…ëª©ê³¼ ê´€ë ¨ì´ ìˆëŠ”ì§€ í‰ê°€í•´ì£¼ì„¸ìš”.
ë°˜ë“œì‹œ ë¶„ì„ ëŒ€ìƒ ì¢…ëª©ê³¼ ê´€ë ¨ì„±ì„ ê³ ë ¤í•˜ì„¸ìš”.

ë¶„ì„ ëŒ€ìƒ ì¢…ëª©: {stock_name} (ì¢…ëª©ì½”ë“œ: {stock_code})

ë‰´ìŠ¤ ì œëª©: {title}
ë‰´ìŠ¤ ë³¸ë¬¸: {content}

í‰ê°€ ê¸°ì¤€:
1. ê¸°ì—…ëª… ì§ì ‘ ì–¸ê¸‰: í•´ë‹¹ ì¢…ëª©ì´ ì§ì ‘ ì–¸ê¸‰ë˜ëŠ”ê°€?
2. ì—…ì¢… ê´€ë ¨ì„±: í•´ë‹¹ ì¢…ëª©ì˜ ì£¼ìš” ì‚¬ì—… ë¶„ì•¼ì™€ ê´€ë ¨ì´ ìˆëŠ”ê°€?
3. ê³„ì—´ì‚¬/ìíšŒì‚¬: í•´ë‹¹ ì¢…ëª©ì˜ ê³„ì—´ì‚¬ë‚˜ ìíšŒì‚¬ ê´€ë ¨ ë‰´ìŠ¤ì¸ê°€?
4. ì‹œì¥ ì˜í–¥: í•´ë‹¹ ì¢…ëª©ì— ì§ê°„ì ‘ì  ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆëŠ”ê°€?

ê´€ë ¨ì„± ì ìˆ˜: 0.0~1.0 (0.0=ì™„ì „ ë¬´ê´€, 0.5=ê°„ì ‘ ê´€ë ¨, 1.0=ì§ì ‘ ê´€ë ¨)
íŒë‹¨ ê¸°ì¤€: 0.5 ì´ìƒì´ë©´ ê´€ë ¨, ë¯¸ë§Œì´ë©´ ë¬´ê´€

ì‘ë‹µ í˜•ì‹:
ê´€ë ¨ì„±: 0.XX
íŒë‹¨ê·¼ê±°: í•œ ì¤„ ì„¤ëª…
"""
            
            response = await self._call_llm_with_retry(prompt)
            
            # ê¸°ë³¸ê°’ ì„¤ì •
            relevance_score = 0.5
            reasoning = "í‰ê°€ ì‹¤íŒ¨"
            
            # ì‘ë‹µ íŒŒì‹±
            if response:
                lines = response.split('\n')
                for line in lines:
                    line = line.strip()
                    if line.startswith('ê´€ë ¨ì„±:'):
                        try:
                            score_match = re.search(r'[\d.]+', line)
                            if score_match:
                                relevance_score = float(score_match.group())
                                relevance_score = max(0.0, min(1.0, relevance_score))
                        except (ValueError, AttributeError):
                            pass
                    elif line.startswith('íŒë‹¨ê·¼ê±°:'):
                        reasoning = line.replace('íŒë‹¨ê·¼ê±°:', '').strip() or "í‰ê°€ ì‹¤íŒ¨"
            
            # ê´€ë ¨ì„± ì ìˆ˜ ê¸°ë°˜ ìµœì¢… íŒë‹¨
            is_relevant = relevance_score >= self.relevance_threshold
            
            return is_relevant, relevance_score, reasoning
            
        except Exception as e:
            logger.error(f"ì¢…ëª© ê´€ë ¨ì„± í‰ê°€ ì‹¤íŒ¨: {e}")
            log_error_with_traceback("ì¢…ëª© ê´€ë ¨ì„± í‰ê°€ ì‹¤íŒ¨", e, {
                "stock_code": stock_code,
                "title": title[:50] if title else "No title"
            })
            return True, 0.5, f"í‰ê°€ ì‹¤íŒ¨: {str(e)}"
    
    def check_vector_similarity(self, news_item: Dict) -> Tuple[bool, float, Optional[str]]:
        """ë²¡í„° ìœ ì‚¬ë„ ì¤‘ë³µ ê²€ì‚¬"""
        try:
            current_text = news_item.get("title", "") + " " + news_item.get("content", "")
            
            if not current_text.strip():
                logger.warning("ë‰´ìŠ¤ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŒ")
                return False, 0.0, None
            
            # 1. ì„¸ì…˜ ìºì‹œì™€ ë¹„êµ
            current_embedding = self.get_cached_embedding(current_text)
            if current_embedding is None:
                current_embedding = self.embedding_model.encode([current_text])[0]
                self.cache_embedding(current_text, current_embedding)
            
            max_similarity = 0.0
            similar_news_title: Optional[str] = None
            
            # ìºì‹œëœ ë‰´ìŠ¤ì™€ ë¹„êµ
            for cached_news in self.news_cache:
                cached_title = cached_news.get("title", "")
                cached_content = cached_news.get("content", "")
                cached_text = cached_title + " " + cached_content
                
                if not cached_text.strip():
                    continue
                    
                cached_embedding = self.get_cached_embedding(cached_text)
                
                if cached_embedding is None:
                    cached_embedding = self.embedding_model.encode([cached_text])[0]
                    self.cache_embedding(cached_text, cached_embedding)
                
                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                similarity = np.dot(current_embedding, cached_embedding) / (
                    np.linalg.norm(current_embedding) * np.linalg.norm(cached_embedding)
                )
                
                if similarity > max_similarity:
                    max_similarity = similarity
                    similar_news_title = cached_title
                
                if similarity >= self.vector_similarity_threshold:
                    return True, similarity, cached_title
            
            # 2. ë°ì´í„°ë² ì´ìŠ¤ì™€ ë¹„êµ
            db_duplicate, db_similarity, db_title = False, 0.0, None  # âŒ MySQL ë³µì¡í•œ ì¤‘ë³µ ì²´í¬ ì œê±°
            if db_duplicate:
                return True, db_similarity, db_title
            
            return False, max_similarity, similar_news_title
            
        except Exception as e:
            logger.error(f"ë²¡í„° ìœ ì‚¬ë„ ê²€ì‚¬ ì‹¤íŒ¨: {e}")
            log_error_with_traceback("ë²¡í„° ìœ ì‚¬ë„ ê²€ì‚¬ ì‹¤íŒ¨", e, {
                "news_title": news_item.get("title", "")[:50]
            })
            return False, 0.0, None
    
    def check_database_duplicate(self, news_item: Dict) -> Tuple[bool, float, Optional[str]]:
        """âŒ ë‹¨ìˆœí™”: ë³µì¡í•œ MySQL ì¤‘ë³µ ê²€ì‚¬ ì œê±° - ChromaDB ë²¡í„° ìœ ì‚¬ë„ë¡œ ëŒ€ì²´"""
        return False, 0.0, None

    def crawl_naver_finance_news(self, stock_code: str, pages: int = 1, size: int = 3) -> List[Dict]:
        """ë„¤ì´ë²„ ëª¨ë°”ì¼ APIë¥¼ í†µí•œ ë‰´ìŠ¤ í¬ë¡¤ë§ (SkillStack ì™„ì „ í†µí•© ë°©ì‹)"""
        news_list = []
        start_time = time.time()
        
        try:
            logger.info(f"ğŸš€ ë„¤ì´ë²„ ëª¨ë°”ì¼ API ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘: {stock_code} (ìµœëŒ€ {pages}í˜ì´ì§€, í˜ì´ì§€ë‹¹ {size}ê°œ)")
            
            # ëª¨ë°”ì¼ API URL êµ¬ì„±
            api_url = f"https://m.stock.naver.com/api/news/stock/{stock_code}"
            
            # í—¤ë” ì„¤ì • (SkillStack ë°©ì‹)
            headers = {
                'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Mobile/15E148 Safari/604.1',
                'Accept': 'application/json, text/plain, */*',
                'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Referer': f'https://m.stock.naver.com/domestic/stock/{stock_code}/news',
                'Sec-Fetch-Dest': 'empty',
                'Sec-Fetch-Mode': 'cors',
                'Sec-Fetch-Site': 'same-origin'
            }
            
            total_available = None
            
            # í˜ì´ì§€ë³„ í¬ë¡¤ë§
            for page in range(1, pages + 1):
                max_retries = 3  # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
                retry_delay = 2   # ì¬ì‹œë„ ê°„ê²© (ì´ˆ)
                
                for retry in range(max_retries):
                    try:
                        logger.info(f"ğŸ“„ í˜ì´ì§€ {page}/{pages} í¬ë¡¤ë§ ì¤‘... (ì‹œë„ {retry + 1}/{max_retries})")
                        
                        # API ìš”ì²­ íŒŒë¼ë¯¸í„°
                        params = {
                            'page': page,
                            'size': size
                        }
                        
                        # API í˜¸ì¶œ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
                        response = requests.get(api_url, headers=headers, params=params, timeout=10)
                        
                        # HTTP ìƒíƒœ ì½”ë“œ í™•ì¸
                        if response.status_code == 429:  # Too Many Requests
                            wait_time = retry_delay * (2 ** retry)  # ì§€ìˆ˜ ë°±ì˜¤í”„
                            logger.warning(f"âš ï¸ API í˜¸ì¶œ ì œí•œ ê°ì§€ (429). {wait_time}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...")
                            time.sleep(wait_time)
                            continue
                        elif response.status_code == 403:  # Forbidden
                            wait_time = retry_delay * (2 ** retry) + 5  # ì¶”ê°€ ëŒ€ê¸°
                            logger.warning(f"âš ï¸ API ì ‘ê·¼ ê±°ë¶€ (403). {wait_time}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...")
                            time.sleep(wait_time)
                            continue
                        elif response.status_code != 200:
                            logger.error(f"âŒ API ì‘ë‹µ ì˜¤ë¥˜: {response.status_code}")
                            if retry < max_retries - 1:
                                time.sleep(retry_delay)
                                continue
                            else:
                                break
                        
                        response.raise_for_status()
                        data = response.json()
                        page_news_count = 0
                        
                        # API ì‘ë‹µ êµ¬ì¡° ë¶„ì„ ë° ì²˜ë¦¬
                        logger.debug(f"ğŸ” API ì‘ë‹µ êµ¬ì¡° ë¶„ì„: {type(data)}")
                        
                        # ë‹¤ì–‘í•œ ì‘ë‹µ êµ¬ì¡° ì²˜ë¦¬
                        items = []
                        total_count = "ì•Œ ìˆ˜ ì—†ìŒ"
                        
                        if isinstance(data, list):
                            # ë¦¬ìŠ¤íŠ¸ í˜•íƒœ ì‘ë‹µ ì²˜ë¦¬
                            logger.debug(f"ğŸ“‹ ë¦¬ìŠ¤íŠ¸ í˜•íƒœ ì‘ë‹µ: ê¸¸ì´ {len(data)}")
                            for entry in data:
                                if isinstance(entry, dict):
                                    # ì²« ë²ˆì§¸ í˜ì´ì§€ì—ì„œ ì´ ê°œìˆ˜ ì •ë³´ ì¶”ì¶œ
                                    if page == 1 and total_available is None:
                                        total_count = entry.get("totalCount", entry.get("total", "ì•Œ ìˆ˜ ì—†ìŒ"))
                                        total_available = total_count
                                        current_items = entry.get("items", [])
                                        logger.info(f"ğŸ“ˆ API ì •ë³´: ì´ {total_count}ê°œ ë‰´ìŠ¤ | í˜„ì¬ í˜ì´ì§€: {len(current_items)}ê°œ")
                                    
                                    # items ì¶”ì¶œ
                                    entry_items = entry.get("items", [])
                                    if entry_items:
                                        items.extend(entry_items)
                        elif isinstance(data, dict):
                            # ë”•ì…”ë„ˆë¦¬ í˜•íƒœ ì‘ë‹µ ì²˜ë¦¬
                            logger.debug(f"ğŸ“‹ ë”•ì…”ë„ˆë¦¬ í˜•íƒœ ì‘ë‹µ: í‚¤ {list(data.keys())}")
                            
                            # ì²« ë²ˆì§¸ í˜ì´ì§€ì—ì„œ ì´ ê°œìˆ˜ ì •ë³´ ì¶”ì¶œ
                            if page == 1 and total_available is None:
                                total_count = data.get("totalCount", data.get("total", "ì•Œ ìˆ˜ ì—†ìŒ"))
                                total_available = total_count
                                current_items = data.get("items", [])
                                logger.info(f"ğŸ“ˆ API ì •ë³´: ì´ {total_count}ê°œ ë‰´ìŠ¤ | í˜„ì¬ í˜ì´ì§€: {len(current_items)}ê°œ")
                            
                            # items ì¶”ì¶œ
                            items = data.get("items", [])
                        else:
                            logger.error(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ API ì‘ë‹µ êµ¬ì¡°: {type(data)}")
                            continue
                        
                        # size íŒŒë¼ë¯¸í„°ì— ë”°ë¼ ì•„ì´í…œ ìˆ˜ ì œí•œ
                        if size and len(items) > size:
                            items = items[:size]
                            logger.info(f"ğŸ“Š ìš”ì²­ëœ í¬ê¸°({size}ê°œ)ì— ë§ì¶° {len(items)}ê°œë¡œ ì œí•œ")
                        
                        # ë‰´ìŠ¤ ì•„ì´í…œ ì²˜ë¦¬
                        for item in items:
                            if not isinstance(item, dict):
                                logger.debug(f"â­ï¸ ìœ íš¨í•˜ì§€ ì•Šì€ ì•„ì´í…œ íƒ€ì…: {type(item)}")
                                continue
                                
                            # type í•„ë“œ í™•ì¸ (1: ì¼ë°˜ ê¸°ì‚¬, 2: ê³µì‹œ ë“±)
                            item_type = item.get("type", 1)
                            if item_type != 1:  # ì¼ë°˜ ê¸°ì‚¬ë§Œ
                                logger.debug(f"â­ï¸ ì¼ë°˜ ê¸°ì‚¬ê°€ ì•„ë‹˜ (type: {item_type}), ê±´ë„ˆëœ€")
                                continue
                            
                            page_news_count += 1
                            
                            # URL ìƒì„± (SkillStack ë°©ì‹)
                            url = self.build_news_url(item)
                            if not url:
                                logger.debug(f"â­ï¸ URL ìƒì„± ì‹¤íŒ¨, ê±´ë„ˆëœ€")
                                continue
                            
                            # ë‚ ì§œ í¬ë§·íŒ… (SkillStack ë°©ì‹)
                            published_at = self.format_news_date(item.get("datetime", ""))
                            
                            title = item.get("title", "").strip()
                            if not title:
                                logger.debug(f"â­ï¸ ì œëª© ì—†ìŒ, ê±´ë„ˆëœ€")
                                continue
                            
                            logger.debug(f"ğŸ“° ë‰´ìŠ¤ ë°œê²¬: '{title[:50]}...' -> {url}")
                            
                            # ë‰´ìŠ¤ ì•„ì´í…œ ìƒì„± (SkillStack ë°©ì‹)
                            news_item = {
                                "title": title,
                                "url": url,
                                "published_at": published_at,
                                "stock_code": stock_code,
                                "source": item.get("source", ""),
                                "summary": item.get("summary", ""),
                                "content": "",  # ë‚˜ì¤‘ì— ë³„ë„ë¡œ ì¶”ì¶œ
                                "simhash": 0,
                                "created_at": datetime.now()
                            }
                            
                            news_list.append(news_item)
                            
                            # ë³¸ë¬¸ ì¶”ì¶œ (ì„ íƒì )
                            content = self.get_news_content(url)
                            if content:
                                news_item["content"] = content
                            
                            time.sleep(0.5)  # ë³¸ë¬¸ ì¶”ì¶œ ê°„ê²©
                            
                        logger.info(f"âœ… í˜ì´ì§€ {page} ì™„ë£Œ: {page_news_count}ê°œ ìˆ˜ì§‘ (ëˆ„ì : {len(news_list)}ê°œ)")
                        time.sleep(0.2)  # API í˜¸ì¶œ ê°„ê²©
                        break  # ì„±ê³µ ì‹œ ì¬ì‹œë„ ë£¨í”„ ì¢…ë£Œ
                        
                    except requests.exceptions.RequestException as e:
                        logger.error(f"âŒ API ìš”ì²­ ì‹¤íŒ¨ (í˜ì´ì§€ {page}, ì‹œë„ {retry + 1}): {e}")
                        if retry < max_retries - 1:
                            wait_time = retry_delay * (2 ** retry)
                            logger.info(f"â³ {wait_time}ì´ˆ í›„ ì¬ì‹œë„...")
                            time.sleep(wait_time)
                        else:
                            logger.error(f"âŒ í˜ì´ì§€ {page} ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼")
                    except Exception as e:
                        logger.error(f"âŒ í˜ì´ì§€ {page} ì²˜ë¦¬ ì‹¤íŒ¨ (ì‹œë„ {retry + 1}): {e}")
                        if retry < max_retries - 1:
                            wait_time = retry_delay * (2 ** retry)
                            logger.info(f"â³ {wait_time}ì´ˆ í›„ ì¬ì‹œë„...")
                            time.sleep(wait_time)
                        else:
                            logger.error(f"âŒ í˜ì´ì§€ {page} ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼")
                            break

        except Exception as e:
            logger.error(f"âŒ ë„¤ì´ë²„ ëª¨ë°”ì¼ API ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            log_error_with_traceback("ë„¤ì´ë²„ ëª¨ë°”ì¼ API ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨", e, {
                "stock_code": stock_code,
                "api_url": api_url
            })
        
        processing_time = time.time() - start_time
        logger.info(f"ğŸ ì¢…ëª© {stock_code} API í¬ë¡¤ë§ ì™„ë£Œ: {len(news_list)}ê°œ ìˆ˜ì§‘ (ì²˜ë¦¬ì‹œê°„: {processing_time:.2f}ì´ˆ)")
        return news_list

    def get_news_content(self, url: str) -> Optional[str]:
        """ë‰´ìŠ¤ ìƒì„¸ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° (Chrome Driver + Requests ì´ì¤‘í™”)"""
        if not url:
            return None
        
        try:
            start_time = time.time()
            
            # 1ì°¨ ì‹œë„: Chrome Driver ë°©ì‹
            try:
                driver = self.get_driver()
                if driver:
                    content = self._get_content_with_chrome(driver, url, start_time)
                    if content:
                        return content
                    logger.warning("Chrome Driverë¡œ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨, requestsë¡œ ì‹œë„...")
                else:
                    logger.warning("Chrome Driver ì´ˆê¸°í™” ì‹¤íŒ¨, requestsë¡œ ì§ì ‘ ì‹œë„...")
            except Exception as e:
                logger.warning(f"Chrome Driver ì‚¬ìš© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}, requestsë¡œ ì „í™˜...")
            
            # 2ì°¨ ì‹œë„: Requests ê¸°ë°˜ ë°©ì‹ (Chrome Driver ì‹¤íŒ¨ ì‹œ)
            try:
                return self._get_content_with_requests(url, start_time)
            except Exception as e:
                logger.error(f"âŒ Requests ê¸°ë°˜ ë³¸ë¬¸ ì¶”ì¶œë„ ì‹¤íŒ¨: {e}")
                return None
            
        except Exception as e:
            logger.error(f"âŒ ë‰´ìŠ¤ ë³¸ë¬¸ ì¶”ì¶œ ì™„ì „ ì‹¤íŒ¨: {e}")
            return None

    def _get_content_with_chrome(self, driver, url: str, start_time: float) -> Optional[str]:
        """Chrome Driverë¥¼ ì‚¬ìš©í•œ ë³¸ë¬¸ ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)"""
        try:
            logger.debug(f"ğŸ“„ Chrome Driver ë³¸ë¬¸ ì¶”ì¶œ ì‹œì‘: {url[:80]}...")
            
            # í˜ì´ì§€ ì ‘ê·¼ ì‹œë„
            try:
                driver.get(url)
            except Exception as e:
                logger.warning(f"âŒ Chrome Driver í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨: {e}")
                return None
            
            # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° (ë” ì•ˆì •ì ì¸ ë°©ì‹)
            try:
                # ë¨¼ì € body ìš”ì†Œê°€ ìˆëŠ”ì§€ í™•ì¸
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )
                
                # ì¶”ê°€ë¡œ í˜ì´ì§€ê°€ ì™„ì „íˆ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
                WebDriverWait(driver, 5).until(
                    lambda driver: driver.execute_script("return document.readyState") == "complete"
                )
                
                logger.debug("âœ… Chrome Driver í˜ì´ì§€ ë¡œë”© ì™„ë£Œ")
            except TimeoutException:
                logger.warning(f"â° Chrome Driver í˜ì´ì§€ ë¡œë”© ì‹œê°„ ì´ˆê³¼: {url}")
                # ë¡œë”©ì´ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰ (ì¼ë¶€ ë‚´ìš©ì´ë¼ë„ ì¶”ì¶œ)
                pass

            # ì¶”ê°€ ë Œë”ë§ ëŒ€ê¸°
            time.sleep(2)
            
            # ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸ ì¶”ì¶œì„ ìœ„í•œ ì„ íƒìë“¤ (í™•ì¥)
            selectors = [
                "div#newsct_article .newsct_article._article_body",
                "article#dic_area.go_trans._article_content", 
                "div#articleBodyContents",
                "div#newsct_article",
                ".article_body",
                "#content",
                ".article-content",
                ".news-content",
                ".content",
                "div.article_body",
                "div.article-content",
                "div.content",
                "article",
                "div[class*='article']",
                "div[class*='content']",
                "div[class*='news']"
            ]
            
            # ê° ì„ íƒìë¡œ ë³¸ë¬¸ ì¶”ì¶œ ì‹œë„
            for i, selector in enumerate(selectors):
                try:
                    elements = driver.find_elements(By.CSS_SELECTOR, selector)
                    
                    for element in elements:
                        if element and element.text.strip():
                            text = self._clean_news_text(element.text.strip())
                            
                            if len(text) > 30:  # ë” ì§§ì€ í…ìŠ¤íŠ¸ë„ í—ˆìš©
                                processing_time = time.time() - start_time
                                logger.debug(f"âœ… Chrome Driver ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ: ê¸¸ì´ {len(text)}ì, ì²˜ë¦¬ì‹œê°„ {processing_time:.2f}ì´ˆ")
                                return text[:10000]
                            
                except (NoSuchElementException, TimeoutException):
                    continue
                except Exception as e:
                    logger.debug(f"Chrome Driver ì„ íƒì {i+1}ë²ˆ ì˜¤ë¥˜: {e}")
                    continue
            
            # ë§ˆì§€ë§‰ ì‹œë„: ì „ì²´ body í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ
            try:
                body_element = driver.find_element(By.TAG_NAME, "body")
                if body_element and body_element.text.strip():
                    text = self._clean_news_text(body_element.text.strip())
                    if len(text) > 100:  # body í…ìŠ¤íŠ¸ëŠ” ë” ê¸´ ê²½ìš°ë§Œ
                        processing_time = time.time() - start_time
                        logger.debug(f"âœ… Chrome Driver body í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ: ê¸¸ì´ {len(text)}ì, ì²˜ë¦¬ì‹œê°„ {processing_time:.2f}ì´ˆ")
                        return text[:10000]
            except Exception as e:
                logger.debug(f"Chrome Driver body í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            
            return None
            
        except Exception as e:
            logger.warning(f"Chrome Driver ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

    def _get_content_with_requests(self, url: str, start_time: float) -> Optional[str]:
        """Requestsë¥¼ ì‚¬ìš©í•œ ë³¸ë¬¸ ì¶”ì¶œ (Chrome Driver ëŒ€ì²´)"""
        try:
            logger.debug(f"ğŸ“„ Requests ë³¸ë¬¸ ì¶”ì¶œ ì‹œì‘: {url[:80]}...")
            
            # Requests ê¸°ë°˜ í¬ë¡¤ë§ í—¤ë”
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
            }
            
            # HTTP ìš”ì²­
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            # ì¸ì½”ë”© ì„¤ì •
            if response.encoding.lower() in ['iso-8859-1', 'windows-1252']:
                response.encoding = 'utf-8'
            
            html_content = response.text
            
            # BeautifulSoup ì‚¬ìš©í•˜ì—¬ ë³¸ë¬¸ ì¶”ì¶œ
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ë„¤ì´ë²„ ë‰´ìŠ¤ ë° ì¼ë°˜ ë‰´ìŠ¤ ì„ íƒìë“¤
            selectors = [
                {'selector': 'div#newsct_article', 'attr': 'text'},
                {'selector': 'article#dic_area', 'attr': 'text'},
                {'selector': 'div#articleBodyContents', 'attr': 'text'},
                {'selector': '.article_body', 'attr': 'text'},
                {'selector': '.article-content', 'attr': 'text'},
                {'selector': '.news-content', 'attr': 'text'},
                {'selector': '#content', 'attr': 'text'}
            ]
            
            # ê° ì„ íƒìë¡œ ë³¸ë¬¸ ì¶”ì¶œ ì‹œë„
            for i, sel_info in enumerate(selectors):
                try:
                    element = soup.select_one(sel_info['selector'])
                    if element:
                        # ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼ íƒœê·¸ ì œê±°
                        for script in element(["script", "style", "nav", "header", "footer", "aside"]):
                            script.decompose()
                        
                        text = element.get_text(strip=True)
                        if text:
                            text = self._clean_news_text(text)
                            
                            if len(text) > 50:
                                processing_time = time.time() - start_time
                                logger.debug(f"âœ… Requests ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ: ê¸¸ì´ {len(text)}ì, ì²˜ë¦¬ì‹œê°„ {processing_time:.2f}ì´ˆ")
                                return text[:10000]
                                
                except Exception as e:
                    logger.debug(f"Requests ì„ íƒì {i+1}ë²ˆ ì˜¤ë¥˜: {e}")
                    continue
            
            # ë§ˆì§€ë§‰ ì‹œë„: ëª¨ë“  p íƒœê·¸ ìˆ˜ì§‘
            paragraphs = soup.find_all('p')
            if paragraphs:
                text = ' '.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])
                if text:
                    text = self._clean_news_text(text)
                    if len(text) > 50:
                        processing_time = time.time() - start_time
                        logger.debug(f"âœ… Requests píƒœê·¸ ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ: ê¸¸ì´ {len(text)}ì, ì²˜ë¦¬ì‹œê°„ {processing_time:.2f}ì´ˆ")
                        return text[:10000]
            
            logger.warning(f"Requestsë¡œ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {url}")
            return None
            
        except Exception as e:
            logger.warning(f"Requests ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

    def _clean_news_text(self, text: str) -> str:
        """ë‰´ìŠ¤ í…ìŠ¤íŠ¸ ì •ë¦¬"""
        if not text:
            return ""
        
        # ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±°
        text = re.sub(r'^\s*\[.*?\]\s*', '', text)  # [ê¸°ìëª…] ì œê±°
        text = re.sub(r'(ê¸°ì|í¸ì§‘ì|íŠ¹íŒŒì›).*?=\s*', '', text)  # ê¸°ì ì •ë³´ ì œê±°
        text = re.sub(r'\s+', ' ', text)  # ì—°ì† ê³µë°± ì •ë¦¬
        text = re.sub(r'[\r\n\t]+', ' ', text)  # ê°œí–‰ë¬¸ì ì •ë¦¬
        
        return text.strip()
    
    async def evaluate_impact_with_rag(self, news_item: Dict) -> Tuple[float, str]:
        """ìˆœìˆ˜ RAG ë°©ì‹ ì˜í–¥ë„ í‰ê°€"""
        try:
            stock_code = news_item["stock_code"]
            stock_name = self.stock_names.get(stock_code, stock_code)
            title = news_item["title"]
            content = news_item["content"]
            
            logger.info(f"ğŸ” RAG ë°©ì‹ ì˜í–¥ë„ í‰ê°€ ì‹œì‘: '{title[:50]}...'")
            
            # API í˜¸ì¶œ ê°„ê²© ì¡°ì ˆ (í…”ë ˆê·¸ë¨ API 429 ì—ëŸ¬ ë°©ì§€)
            api_interval = self.config.get("NEWS_API_CALL_INTERVAL", 2)
            time.sleep(api_interval)
            
            # RAG ë°©ì‹ ì˜í–¥ë„ í‰ê°€ (ëª¨ë“  ë‰´ìŠ¤ë¥¼ LLMì— ì „ë‹¬)
            api_score, direction_score, market_reaction, push_message = await self.evaluate_news_impact_api(
                news_item, stock_name or stock_code
            )
            
            # 0-10 ìŠ¤ì¼€ì¼ì„ 0-1 ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜ (ê¸°ì¡´ ì‹œìŠ¤í…œ í˜¸í™˜ì„±)
            normalized_score = api_score / 10.0
            
            # í™•ì¥ëœ ì •ë³´ë¥¼ news_itemì— ì¶”ê°€ (DB ì €ì¥ìš©)
            news_item["push_content"] = push_message
            
            logger.info(f"âœ… RAG ì˜í–¥ë„ í‰ê°€ ì™„ë£Œ: {api_score}/10 (ì •ê·œí™”: {normalized_score:.2f})")
            return normalized_score, push_message

        except Exception as e:
            logger.error(f"âŒ RAG ì˜í–¥ë„ í‰ê°€ ì‹¤íŒ¨: {e}")
            log_error_with_traceback("RAG ì˜í–¥ë„ í‰ê°€ ì‹¤íŒ¨", e, {
                "stock_code": stock_code,
                "title": title[:50] if title else "No title"
            })
            return 0.0, "í‰ê°€ ì‹¤íŒ¨"
    
    def calculate_basic_impact_score(self, news_item: Dict) -> int:
        """ê¸°ë³¸ ì˜í–¥ë„ ì ìˆ˜ ê³„ì‚° (0-10 ìŠ¤ì¼€ì¼)"""
        try:
            title = news_item["title"].lower()
            content = news_item["content"].lower()
            text = title + " " + content
            
            # ê³ ì˜í–¥ë„ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜
            high_impact_keywords = {
                "ì‹¤ì ": 3, "ë§¤ì¶œ": 3, "ì˜ì—…ì´ìµ": 3, "ìˆœì´ìµ": 3,
                "ì¸ìˆ˜í•©ë³‘": 4, "M&A": 4, "íˆ¬ì": 2, "íˆ¬ììœ ì¹˜": 3,
                "ì‹ ì œí’ˆ": 2, "ì¶œì‹œ": 2, "ê³„ì•½": 2, "ìˆ˜ì£¼": 3,
                "ìƒì¥": 3, "IPO": 3, "ì¦ì": 2, "ê°ì": 2,
                "ë°°ë‹¹": 2, "ì£¼ì‹ë¶„í• ": 2, "í•©ë³‘": 4, "ë¶„í• ": 3,
                "ê·œì œ": 2, "ìŠ¹ì¸": 2, "í—ˆê°€": 2, "ë¼ì´ì„¼ìŠ¤": 2,
                "íŠ¹í—ˆ": 2, "ê¸°ìˆ ": 1, "ê°œë°œ": 1, "ì—°êµ¬": 1,
                "ìœ„ê¸°": 3, "ì†ì‹¤": 2, "ì ì": 2, "ë¶€ì‹¤": 3,
                "ì‚¬ì—…": 1, "í™•ì¥": 2, "ì§„ì¶œ": 2, "ì² ìˆ˜": 2
            }
            
            # í‚¤ì›Œë“œ ì ìˆ˜ ê³„ì‚°
            score = 0
            for keyword, weight in high_impact_keywords.items():
                if keyword in text:
                    score += weight
            
            # ì œëª©ì— ìˆëŠ” í‚¤ì›Œë“œëŠ” ê°€ì¤‘ì¹˜ ì¶”ê°€
            for keyword, weight in high_impact_keywords.items():
                if keyword in title:
                    score += weight * 0.5
            
            # ì •ê·œí™” (0-10 ë²”ìœ„)
            score = int(min(score, 10))
            
            return score
            
        except Exception as e:
            logger.error(f"ê¸°ë³¸ ì˜í–¥ë„ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 5
    
    def get_latest_impact_keywords(self, stock_code: str, week_start: Optional[str] = None) -> Dict[str, int]:
        """âŒ ë‹¨ìˆœí™”: ë³µì¡í•œ MySQL í‚¤ì›Œë“œ ë¶„ì„ ì œê±° - ChromaDB keywords ì»¬ë ‰ì…˜ ì§ì ‘ ì‚¬ìš©ìœ¼ë¡œ ëŒ€ì²´"""
        return {}
    
    def calculate_basic_impact_score_with_db(self, news_item: Dict) -> int:
        """í¬ë¡œë§ˆ DB í•µì‹¬ í‚¤ì›Œë“œ ê¸°ë°˜ ê¸°ë³¸ ì˜í–¥ë„ ì ìˆ˜ ê³„ì‚° (ê°œì„  ë²„ì „)"""
        try:
            title = news_item["title"].lower()
            content = news_item["content"].lower()
            text = title + " " + content
            stock_code = news_item.get("stock_code", "")
            
            logger.info(f"ğŸ¯ ê¸°ë³¸ ì˜í–¥ë„ ì ìˆ˜ ê³„ì‚° ì‹œì‘: {stock_code}")
            
            # ğŸ”§ ê°œì„ : í‚¤ì›Œë“œ ì ìˆ˜ì™€ ë³„ê°œë¡œ ë‰´ìŠ¤ ë‚´ìš© ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°
            content_score = self._calculate_content_based_score(news_item)
            logger.info(f"ğŸ“ ë‰´ìŠ¤ ë‚´ìš© ê¸°ë°˜ ì ìˆ˜: {content_score}/10")
            
            # í¬ë¡œë§ˆ DBì—ì„œ ìµœì‹  í•µì‹¬ í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°
            high_impact_keywords = {}  # âŒ MySQL ë³µì¡í•œ í‚¤ì›Œë“œ ë¶„ì„ ì œê±°
            
            # í‚¤ì›Œë“œ ì ìˆ˜ ê³„ì‚°
            keyword_score = 0
            matched_keywords = []
            
            if high_impact_keywords:
                for keyword, weight in high_impact_keywords.items():
                    if keyword in text:
                        keyword_score += weight
                        matched_keywords.append(f"{keyword}({weight})")
                
                # ì œëª©ì— ìˆëŠ” í‚¤ì›Œë“œëŠ” ê°€ì¤‘ì¹˜ ì¶”ê°€
                for keyword, weight in high_impact_keywords.items():
                    if keyword in title:
                        keyword_score += weight * 0.5
                
                # í‚¤ì›Œë“œ ì ìˆ˜ ì •ê·œí™” (0-10 ë²”ìœ„)
                keyword_score = int(min(keyword_score, 10))
                
                if matched_keywords:
                    logger.info(f"ğŸ¯ ë§¤ì¹­ëœ í‚¤ì›Œë“œ: {', '.join(matched_keywords)} â†’ í‚¤ì›Œë“œ ì ìˆ˜: {keyword_score}/10")
                else:
                    logger.info(f"ğŸ¯ ë§¤ì¹­ëœ í‚¤ì›Œë“œ ì—†ìŒ â†’ í‚¤ì›Œë“œ ì ìˆ˜: {keyword_score}/10")
            else:
                logger.warning(f"âš ï¸ í‚¤ì›Œë“œ ë°ì´í„° ì—†ìŒ â†’ í‚¤ì›Œë“œ ì ìˆ˜: 0/10")
            
            # ğŸ”§ ê°œì„ : í‚¤ì›Œë“œ ì ìˆ˜ì™€ ë‚´ìš© ì ìˆ˜ë¥¼ ê²°í•© (í‚¤ì›Œë“œê°€ ì—†ì–´ë„ ë‚´ìš© ê¸°ë°˜ìœ¼ë¡œ í‰ê°€ ê°€ëŠ¥)
            final_score = max(content_score, keyword_score)
            
            logger.info(f"âœ… ìµœì¢… ì˜í–¥ë„ ì ìˆ˜: {final_score}/10 (ë‚´ìš©: {content_score}, í‚¤ì›Œë“œ: {keyword_score})")
            return final_score
            
        except Exception as e:
            logger.error(f"âŒ DB ê¸°ë°˜ ê¸°ë³¸ ì˜í–¥ë„ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            log_error_with_traceback("DB ê¸°ë°˜ ê¸°ë³¸ ì˜í–¥ë„ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨", e, {
                "stock_code": news_item.get("stock_code", ""),
                "title": news_item.get("title", "")[:50]
            })
            return 5  # ê¸°ë³¸ê°’ìœ¼ë¡œ 5ì  ë°˜í™˜
    
    def _calculate_content_based_score(self, news_item: Dict) -> int:
        """ë‰´ìŠ¤ ë‚´ìš© ê¸°ë°˜ ì˜í–¥ë„ ì ìˆ˜ ê³„ì‚° (í‚¤ì›Œë“œì™€ ë…ë¦½ì )"""
        try:
            title = news_item["title"].lower()
            content = news_item["content"].lower()
            text = title + " " + content
            
            score = 0
            
            # 1. ë‰´ìŠ¤ ê¸¸ì´ ê¸°ë°˜ ì ìˆ˜ (ê¸´ ë‰´ìŠ¤ëŠ” ë” ì¤‘ìš”í•  ê°€ëŠ¥ì„±)
            content_length = len(content)
            if content_length > 1000:
                score += 2
            elif content_length > 500:
                score += 1
            
            # 2. ì œëª© ê¸¸ì´ ê¸°ë°˜ ì ìˆ˜
            title_length = len(title)
            if title_length > 50:
                score += 1
            
            # 3. íŠ¹ì • íŒ¨í„´ ê¸°ë°˜ ì ìˆ˜
            important_patterns = {
                "ì‹¤ì ": 3, "ë§¤ì¶œ": 3, "ì˜ì—…ì´ìµ": 3, "ìˆœì´ìµ": 3,
                "ì¸ìˆ˜í•©ë³‘": 4, "m&a": 4, "íˆ¬ì": 2, "íˆ¬ììœ ì¹˜": 3,
                "ì‹ ì œí’ˆ": 2, "ì¶œì‹œ": 2, "ê³„ì•½": 2, "ìˆ˜ì£¼": 3,
                "ìƒì¥": 3, "ipo": 3, "ì¦ì": 2, "ê°ì": 2,
                "ë°°ë‹¹": 2, "ì£¼ì‹ë¶„í• ": 2, "í•©ë³‘": 4, "ë¶„í• ": 3,
                "ê·œì œ": 2, "ìŠ¹ì¸": 2, "í—ˆê°€": 2, "ë¼ì´ì„¼ìŠ¤": 2,
                "íŠ¹í—ˆ": 2, "ê¸°ìˆ ": 1, "ê°œë°œ": 1, "ì—°êµ¬": 1,
                "ìœ„ê¸°": 3, "ì†ì‹¤": 2, "ì ì": 2, "ë¶€ì‹¤": 3,
                "ì‚¬ì—…": 1, "í™•ì¥": 2, "ì§„ì¶œ": 2, "ì² ìˆ˜": 2,
                "ì¦ê¶Œ": 1, "ì£¼ì‹": 1, "ì£¼ê°€": 1, "ì‹œì¥": 1,
                "ê¸ˆìœµ": 1, "ì€í–‰": 1, "ë³´í—˜": 1, "íˆ¬ì": 1
            }
            
            for pattern, weight in important_patterns.items():
                if pattern in text:
                    score += weight
            
            # 4. ì œëª©ì— ìˆëŠ” íŒ¨í„´ì€ ê°€ì¤‘ì¹˜ ì¶”ê°€
            for pattern, weight in important_patterns.items():
                if pattern in title:
                    score += weight * 0.5
            
            # 5. íŠ¹ë³„í•œ í‘œí˜„ ê¸°ë°˜ ì ìˆ˜
            special_expressions = {
                "ë°œí‘œ": 2, "ê³µì‹œ": 2, "ë°œí–‰": 2, "ìƒì¥": 3,
                "ì¸ìˆ˜": 3, "í•©ë³‘": 4, "ë¶„í• ": 3, "ë§¤ê°": 3,
                "íˆ¬ì": 2, "ìœ ì¹˜": 3, "ê³„ì•½": 2, "ìˆ˜ì£¼": 3,
                "ì‹ ê·œ": 2, "ì²«": 2, "ìµœì´ˆ": 2, "ìµœê³ ": 2,
                "ê¸‰ë“±": 3, "ê¸‰ë½": 3, "ìƒìŠ¹": 2, "í•˜ë½": 2,
                "í˜¸ì¬": 3, "ì•…ì¬": 3, "ê¸ì •": 2, "ë¶€ì •": 2
            }
            
            for expr, weight in special_expressions.items():
                if expr in text:
                    score += weight
            
            # ì •ê·œí™” (0-10 ë²”ìœ„)
            score = int(min(score, 10))
            
            return score
            
        except Exception as e:
            logger.error(f"âŒ ë‚´ìš© ê¸°ë°˜ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 3  # ê¸°ë³¸ê°’
    
    def search_similar_historical_cases(self, news_item: Dict) -> List[Dict]:
        """âŒ ë‹¨ìˆœí™”: ë³µì¡í•œ MySQL ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰ ì œê±° - ChromaDB past_events ì»¬ë ‰ì…˜ ì§ì ‘ ì‚¬ìš©ìœ¼ë¡œ ëŒ€ì²´"""
        return []
        try:
            logger.info(f"ğŸ” ê³¼ê±° ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰ ì‹œì‘: '{news_item['title'][:50]}...'")
            logger.info(f"ğŸ“Š í¬ë¡œë§ˆDB ì—°ê²° ìƒíƒœ í™•ì¸ ì¤‘...")
            logger.info(f"ğŸ”— í¬ë¡œë§ˆDB ê°ì²´: {self.vector_db is not None}")
            
            # ë‰´ìŠ¤ í…ìŠ¤íŠ¸ ì¤€ë¹„
            news_text = news_item["title"] + " " + news_item["content"]
            stock_code = news_item.get("stock_code", "")
            logger.info(f"ğŸ“ ê²€ìƒ‰ ì¿¼ë¦¬ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(news_text)}ì")
            logger.info(f"ğŸ“ ê²€ìƒ‰ ì¿¼ë¦¬ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {news_text[:100]}...")
            
            # 1ì°¨: past_events ì»¬ë ‰ì…˜ì—ì„œ ê²€ìƒ‰ (ê³¼ê±° ì¤‘ìš” ì‚¬ê±´ë“¤)
            try:
                logger.info("ğŸ“š 1ì°¨: past_events ì»¬ë ‰ì…˜ì—ì„œ ê²€ìƒ‰ ì‹œì‘ (ê³¼ê±° ì¤‘ìš” ì‚¬ê±´ë“¤)")
                logger.info(f"ğŸ”— í¬ë¡œë§ˆDB ì¿¼ë¦¬: collection=past_events, top_k=3")
                
                # í¬ë¡œë§ˆDB ê²€ìƒ‰ ì „ ìƒíƒœ í™•ì¸
                if hasattr(self.vector_db, 'collections') and 'past_events' in self.vector_db.collections:
                    try:
                        collection_count = self.vector_db.collections['past_events'].count()
                        logger.info(f"ğŸ“Š past_events ì»¬ë ‰ì…˜ ë¬¸ì„œ ìˆ˜: {collection_count}ê°œ")
                    except Exception as count_error:
                        logger.warning(f"âš ï¸ past_events ì»¬ë ‰ì…˜ ì¹´ìš´íŠ¸ ì‹¤íŒ¨: {count_error}")
                        collection_count = 0
                else:
                    logger.warning("âš ï¸ past_events ì»¬ë ‰ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    collection_count = 0
                
                # ì•ˆì „í•œ ê²€ìƒ‰ ì‹œë„
                past_events_results = []
                try:
                    past_events_results = self.vector_db.search_similar_documents(
                        query=news_text,
                        collection_name="past_events",
                        top_k=3
                    )
                    logger.info(f"âœ… past_events í¬ë¡œë§ˆDB ì¿¼ë¦¬ ì™„ë£Œ")
                    logger.info(f"ğŸ“Š past_events ê²€ìƒ‰ ê²°ê³¼: {len(past_events_results) if past_events_results else 0}ê°œ")
                except Exception as search_error:
                    logger.error(f"âŒ past_events ê²€ìƒ‰ ì‹¤íŒ¨: {search_error}")
                    # ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ë¹ˆ ê²°ê³¼ë¡œ ì²˜ë¦¬
                    past_events_results = []
                
                # ê²€ìƒ‰ ê²°ê³¼ ìƒì„¸ ë¡œê¹…
                if past_events_results:
                    logger.info(f"ğŸ“‹ ê²€ìƒ‰ ê²°ê³¼ ìƒì„¸ ì •ë³´:")
                    for i, result in enumerate(past_events_results):
                        logger.info(f"  ê²°ê³¼ {i+1}: distance={result.get('distance', 'N/A')}, metadata_keys={list(result.get('metadata', {}).keys())}")
                
                enhanced_cases = []
                
                if past_events_results and len(past_events_results) > 0:
                    logger.info(f"ğŸ“š past_eventsì—ì„œ {len(past_events_results)}ê°œ ìœ ì‚¬ ì‚¬ë¡€ ë°œê²¬")
                    
                    for i, case in enumerate(past_events_results):
                        try:
                            metadata = case.get("metadata", {})
                            # ğŸ”§ ìˆ˜ì •: distance ê°’ì˜ ì•ˆì „í•œ ë³€í™˜
                            distance = case.get("distance", 0.0)
                            try:
                                distance = float(distance) if distance is not None else 0.0
                            except (ValueError, TypeError):
                                distance = 0.0
                            
                            # ì•ˆì „í•œ ìœ ì‚¬ë„ ê³„ì‚° (0 â‰¤ similarity â‰¤ 1)
                            similarity_score = max(0.0, min(1.0, 1.0 - distance))
                            # ìœ ì‚¬ë„ê°€ ë„ˆë¬´ ë‚®ìœ¼ë©´ 0ìœ¼ë¡œ ì²˜ë¦¬
                            if similarity_score < 0.1:
                                similarity_score = 0.0
                            
                            logger.info(f"ğŸ” ì‚¬ë¡€ {i+1} ìœ ì‚¬ë„ ê³„ì‚°: distance={distance:.4f} â†’ similarity={similarity_score:.4f}")
                            
                            # ê³¼ê±° ì‚¬ë¡€ ì •ë³´ ì¶”ì¶œ
                            case_info = {
                                "title": metadata.get("title", metadata.get("event_type", "ì œëª© ì—†ìŒ")),
                                "published_date": metadata.get("event_date", "ë‚ ì§œ ì—†ìŒ"),
                                "stock_code": metadata.get("stock_code", "ì¢…ëª© ì—†ìŒ"),
                                "impact_score": 0.8,  # past_eventsëŠ” ì¤‘ìš” ì‚¬ê±´ì´ë¯€ë¡œ ë†’ì€ ì ìˆ˜
                                "market_reaction": f"ì£¼ê°€ {metadata.get('price_change', 'N/A')}",
                                "price_change": metadata.get("price_change", "ê°€ê²© ë³€ë™ ì—†ìŒ"),
                                "similarity_score": similarity_score,
                                "summary": metadata.get("description", case.get("document", "")[:100] + "...")
                            }
                            
                            enhanced_cases.append(case_info)
                            
                            logger.info(f"ğŸ“‹ ê³¼ê±° ì‚¬ë¡€ {i+1}: {case_info['title'][:30]}... (ìœ ì‚¬ë„: {similarity_score:.3f})")
                            logger.info(f"ğŸ“Š í¬ë¡œë§ˆDB ë©”íƒ€ë°ì´í„°: {list(metadata.keys())}")
                            logger.info(f"ğŸ“… ì‚¬ë¡€ ë‚ ì§œ: {case_info['published_date']}")
                            logger.info(f"ğŸ’° ê°€ê²© ë³€ë™: {case_info['price_change']}")
                            
                        except Exception as e:
                            logger.warning(f"âš ï¸ ê³¼ê±° ì‚¬ë¡€ {i+1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                            continue
                    
                    if enhanced_cases:
                        logger.info(f"âœ… past_events ê²€ìƒ‰ ì™„ë£Œ: {len(enhanced_cases)}ê°œ ì‚¬ë¡€")
                        # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
                        enhanced_cases.sort(key=lambda x: x.get('similarity_score', 0.0), reverse=True)
                        similarity_scores = [f"{case.get('similarity_score', 0.0):.3f}" for case in enhanced_cases]
                        logger.info(f"ğŸ“Š ìœ ì‚¬ë„ ìˆœ ì •ë ¬ ì™„ë£Œ: {similarity_scores}")
                        return enhanced_cases
                        
                else:
                    logger.warning("âš ï¸ past_eventsì—ì„œ ìœ ì‚¬ ì‚¬ë¡€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    
            except Exception as e:
                logger.error(f"âŒ past_events í¬ë¡œë§ˆDB ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                logger.error(f"ğŸ” í¬ë¡œë§ˆDB ì—°ê²° ìƒíƒœ: {self.vector_db is not None}")
                
                # HNSW ì¸ë±ìŠ¤ ì˜¤ë¥˜ì¸ ê²½ìš° ì»¬ë ‰ì…˜ ì¬ìƒì„± ì‹œë„
                if "hnsw" in str(e).lower() or "index" in str(e).lower():
                    logger.warning("âš ï¸ HNSW ì¸ë±ìŠ¤ ì˜¤ë¥˜ ê°ì§€ - ì»¬ë ‰ì…˜ ì¬ìƒì„± ì‹œë„")
                    try:
                        # past_events ì»¬ë ‰ì…˜ ì¬ìƒì„±
                        if 'past_events' in self.vector_db.collections:
                            del self.vector_db.collections['past_events']
                        
                        # ì»¬ë ‰ì…˜ ì¬ìƒì„±
                        if hasattr(self.vector_db, 'client') and self.vector_db.client is not None:
                            self.vector_db.collections['past_events'] = self.vector_db.client.create_collection(
                                name="past_events",
                                metadata={"hnsw:space": "cosine"}
                            )
                        else:
                            logger.error("âŒ ChromaDB í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ì–´ ì»¬ë ‰ì…˜ ì¬ìƒì„± ë¶ˆê°€")
                            past_events_results = []
                            return []
                        logger.info("âœ… past_events ì»¬ë ‰ì…˜ ì¬ìƒì„± ì™„ë£Œ")
                        
                        # ì¬ìƒì„± í›„ ë‹¤ì‹œ ê²€ìƒ‰ ì‹œë„
                        try:
                            past_events_results = self.vector_db.search_similar_documents(
                                query=news_text,
                                collection_name="past_events",
                                top_k=3
                            )
                            logger.info("âœ… ì»¬ë ‰ì…˜ ì¬ìƒì„± í›„ ê²€ìƒ‰ ì„±ê³µ")
                        except Exception as retry_error:
                            logger.error(f"âŒ ì¬ìƒì„± í›„ ê²€ìƒ‰ë„ ì‹¤íŒ¨: {retry_error}")
                            past_events_results = []
                    except Exception as recreate_error:
                        logger.error(f"âŒ ì»¬ë ‰ì…˜ ì¬ìƒì„± ì‹¤íŒ¨: {recreate_error}")
                        past_events_results = []
                else:
                    past_events_results = []
                
                log_error_with_traceback("past_events ê²€ìƒ‰ ì‹¤íŒ¨", e, {
                    "stock_code": stock_code,
                    "title": news_item.get("title", "")[:50]
                })
            
            # ìµœì¢… ë°±ì—…: ë¹ˆ ë¦¬ìŠ¤íŠ¸ (past_events ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ)
            logger.debug("ğŸ“‹ ìœ ì‚¬ ì‚¬ë¡€ ì—†ìŒ - ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜")
            return []
            
        except Exception as e:
            logger.error(f"âŒ ê³¼ê±° ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            log_error_with_traceback("ê³¼ê±° ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰ ì‹¤íŒ¨", e, {
                "stock_code": news_item.get("stock_code", ""),
                "title": news_item.get("title", "")[:50]
            })
            return []
    
    def format_similar_cases_for_llm(self, similar_cases: List[Dict]) -> str:
        """ìœ ì‚¬ ì‚¬ë¡€ë¥¼ LLM í”„ë¡¬í”„íŠ¸ìš©ìœ¼ë¡œ í¬ë§·íŒ… (RAG ë°©ì‹)"""
        try:
            if not similar_cases:
                return "ê³¼ê±° ìœ ì‚¬ ì‚¬ë¡€: ì—†ìŒ"
            
            formatted_cases = []
            for i, case in enumerate(similar_cases, 1):
                case_text = f"""
ì‚¬ë¡€ {i}:
- ì œëª©: {case.get('title', 'N/A')}
- ë‚ ì§œ: {case.get('published_date', 'N/A')}
- ì˜í–¥ë„: {case.get('impact_score', 0.0):.2f}/1.0
- ì‹œì¥ ë°˜ì‘: {case.get('market_reaction', 'N/A')}
- ê°€ê²© ë³€ë™: {case.get('price_change', 'N/A')}
- ìœ ì‚¬ë„: {case.get('similarity_score', 0.0):.3f}
- ìš”ì•½: {case.get('summary', 'N/A')[:100]}...
"""
                formatted_cases.append(case_text.strip())
            
            result = "ê³¼ê±° ìœ ì‚¬ ì‚¬ë¡€:\n" + "\n\n".join(formatted_cases)
            logger.debug(f"ğŸ“ LLMìš© ìœ ì‚¬ ì‚¬ë¡€ í¬ë§·íŒ… ì™„ë£Œ: {len(similar_cases)}ê°œ ì‚¬ë¡€")
            return result
            
        except Exception as e:
            logger.error(f"âŒ ìœ ì‚¬ ì‚¬ë¡€ í¬ë§·íŒ… ì‹¤íŒ¨: {e}")
            return "ê³¼ê±° ìœ ì‚¬ ì‚¬ë¡€: í¬ë§·íŒ… ì‹¤íŒ¨"
    
    async def evaluate_news_impact_api(self, news_item: Dict, stock_name: str) -> Tuple[int, int, str, str]:
        """API ê¸°ë°˜ ë‰´ìŠ¤ ì˜í–¥ë„ í‰ê°€ (SkillStack ë°©ì‹)"""
        try:
            logger.debug(f"ğŸ” ë‰´ìŠ¤ ì˜í–¥ë„ í‰ê°€ ì‹œì‘: '{news_item['title'][:50]}...'")
            
            # 1. ê³¼ê±° ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰: past_events DBì—ì„œ ìœ ì‚¬í•œ ì‚¬ë¡€ ê°€ì ¸ì˜¤ê¸°
            similar_cases = []  # âŒ MySQL ë³µì¡í•œ ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰ ì œê±°
            formatted_cases = self.format_similar_cases_for_llm(similar_cases)
            
            # 2. í•µì‹¬ í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°: keywords DBì—ì„œ ê°€ì¥ ìµœê·¼ ì£¼ì°¨ í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°
            stock_code = news_item.get("stock_code", "")
            impact_keywords = {}  # âŒ MySQL ë³µì¡í•œ í‚¤ì›Œë“œ ë¶„ì„ ì œê±°
            
            # 3. í‚¤ì›Œë“œ ì •ë³´ë¥¼ LLMìš©ìœ¼ë¡œ í¬ë§·íŒ…
            keywords_text = "í•µì‹¬ ì˜í–¥ í‚¤ì›Œë“œ (ìµœê·¼ ì£¼ì°¨):\n"
            for keyword, weight in impact_keywords.items():
                keywords_text += f"- {keyword}: ê°€ì¤‘ì¹˜ {weight}\n"
            
            # 4. ê°•í™”ëœ RAG í”„ë¡¬í”„íŠ¸ ìƒì„± (ìˆœìˆ˜ RAG ë°©ì‹)
            enhanced_prompt = f"""
ë‹¹ì‹ ì€ ì£¼ì‹ ì‹œì¥ ì „ë¬¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ì°¸ê³  ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•´ë‹¹ ì¢…ëª©ì— ë‰´ìŠ¤ê°€ ì–´ëŠì •ë„ì˜  ì˜í–¥ë„ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.
ì¤‘ìš”í•œ ê²ƒì€ í•´ë‹¹ ì¢…ëª©ì— ì–´ëŠ ì •ë„ì˜ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ê°€ë¥¼ íŒë‹¨í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤!!!! ì˜í–¥ë„ ì ìˆ˜ëŠ” 0.0-1.0 ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ íŒë‹¨í•´ì£¼ì„¸ìš”.

[ë¶„ì„ ëŒ€ìƒ]
ì¢…ëª©ëª…: {stock_name}
ì¢…ëª©ì½”ë“œ: {stock_code}
ë‰´ìŠ¤ ì œëª©: {news_item['title']}
ë‰´ìŠ¤ ë‚´ìš©: {news_item['content'][:800]}...

[ì°¸ê³  ì •ë³´ - RAG ê²€ìƒ‰ ê²°ê³¼]
{keywords_text}

{formatted_cases}

[ë¶„ì„ ì§€ì¹¨]
ìœ„ì˜ ì°¸ê³  ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒì„ ë¶„ì„í•´ì£¼ì„¸ìš”:
1. ê³¼ê±° ìœ ì‚¬ ì‚¬ë¡€(past_events DB)ì™€ì˜ ë¹„êµ ë¶„ì„
2. ìµœê·¼ ì£¼ì°¨ í•µì‹¬ í‚¤ì›Œë“œ(keywords DB)ì™€ì˜ ì—°ê´€ì„± í‰ê°€
3. ì‹œì¥ í™˜ê²½ê³¼ì˜ ì—°ê´€ì„± ê³ ë ¤

[ë¶„ì„ ìš”ì²­]
1. ì˜í–¥ë„ ì ìˆ˜ (0.0-1.0): ì´ ë‰´ìŠ¤ê°€ í•´ë‹¹ ì¢…ëª©ì˜ ì£¼ê°€ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì˜ ê°•ë„
   - 0.0-0.3: ë‚®ì€ ì˜í–¥ (ì¼ë°˜ì ì¸ ì—…ê³„ ë‰´ìŠ¤)
   - 0.4-0.6: ë³´í†µ ì˜í–¥ (ê¸°ì—…ì— ì§ì ‘ì  ì˜í–¥)
   - 0.7-1.0: ë†’ì€ ì˜í–¥ (ì‹¤ì , ì¸ìˆ˜í•©ë³‘, ê·œì œ ë“±)

2. ë°©í–¥ì„± ì ìˆ˜ (1-5): í•´ë‹¹ ì¢…ëª©ì˜ ì£¼ê°€ì— ë¯¸ì¹˜ëŠ” ë°©í–¥ì„±
   - 1: ë§¤ìš° ë¶€ì •ì  (ì‹¤ì  ì•…í™”, ê·œì œ ê°•í™” ë“±)
   - 2: ë¶€ì •ì  (ì¼ì‹œì  ì•…ì¬, ê²½ìŸ ì‹¬í™” ë“±)
   - 3: ì¤‘ë¦½ (ì •ë³´ ì œê³µ, ì—…ê³„ ë™í–¥ ë“±)
   - 4: ê¸ì •ì  (ì‹¤ì  ê°œì„ , ì‹ ê·œ ì‚¬ì—… ë“±)
   - 5: ë§¤ìš° ê¸ì •ì  (ì‹¤ì  ëŒ€í­ ê°œì„ , ì¸ìˆ˜í•©ë³‘ ë“±)

3. ì‹œì¥ ì˜í–¥ ë¶„ì„: êµ¬ì²´ì ì¸ ì‹œì¥ ë°˜ì‘ ì˜ˆì¸¡ (100ì ì´ë‚´)

4. íˆ¬ìì ì•Œë¦¼ ë©”ì‹œì§€: í…”ë ˆê·¸ë¨ ë°œì†¡ìš© ê°„ê²°í•œ ë©”ì‹œì§€ (50ì ì´ë‚´)

[ì¶œë ¥ í˜•ì‹]
ì˜í–¥ë„ì ìˆ˜: 0.X
ë°©í–¥ì„±ì ìˆ˜: X
ì‹œì¥ì˜í–¥: (ë¶„ì„ ë‚´ìš©)
ì•Œë¦¼ë©”ì‹œì§€: (ë©”ì‹œì§€ ë‚´ìš©)

ìœ„ í˜•ì‹ì„ ì •í™•íˆ ì§€ì¼œì„œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
"""
            
            # 5. LLM API í˜¸ì¶œ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
            logger.debug("ğŸ¤– LLM API í˜¸ì¶œ ì¤‘...")
            
            response = await self._call_llm_with_retry(enhanced_prompt, max_retries=3, base_delay=3.0)
            
            if not response:
                logger.warning("âš ï¸ LLM API ì‘ë‹µì´ ë¹„ì–´ìˆìŒ")
                return 5, 3, "API ì‘ë‹µ ì—†ìŒ", "ë‰´ìŠ¤ ë¶„ì„ ì‹¤íŒ¨"
            
            # 6. ì‘ë‹µ íŒŒì‹±
            logger.debug(f"ğŸ“ LLM ì‘ë‹µ: {response[:200]}...")
            
            # ì˜í–¥ë„ ì ìˆ˜ ì¶”ì¶œ (0.0-1.0 â†’ 0-10 ë³€í™˜)
            impact_score = 5
            impact_match = re.search(r'ì˜í–¥ë„ì ìˆ˜:\s*([0-9.]+)', response)
            if impact_match:
                try:
                    impact_float = float(impact_match.group(1))
                    impact_score = int(impact_float * 10)  # 0.0-1.0 â†’ 0-10
                    impact_score = max(0, min(10, impact_score))  # ë²”ìœ„ ì œí•œ
                except ValueError:
                    logger.warning(f"âš ï¸ ì˜í–¥ë„ ì ìˆ˜ íŒŒì‹± ì‹¤íŒ¨: {impact_match.group(1)}")
            
            # ë°©í–¥ì„± ì ìˆ˜ ì¶”ì¶œ (1-5)
            direction_score = 3
            direction_match = re.search(r'ë°©í–¥ì„±ì ìˆ˜:\s*([1-5])', response)
            if direction_match:
                try:
                    direction_score = int(direction_match.group(1))
                except ValueError:
                    logger.warning(f"âš ï¸ ë°©í–¥ì„± ì ìˆ˜ íŒŒì‹± ì‹¤íŒ¨: {direction_match.group(1)}")
            
            # ì‹œì¥ ì˜í–¥ ë¶„ì„ ì¶”ì¶œ
            market_reaction = "ë¶„ì„ ì •ë³´ ì—†ìŒ"
            market_match = re.search(r'ì‹œì¥ì˜í–¥:\s*(.+?)(?=\n|ì•Œë¦¼ë©”ì‹œì§€:|$)', response, re.DOTALL)
            if market_match:
                market_reaction = market_match.group(1).strip()[:100]
            
            # ì•Œë¦¼ ë©”ì‹œì§€ ì¶”ì¶œ
            push_message = "ë‰´ìŠ¤ ì•Œë¦¼"
            push_match = re.search(r'ì•Œë¦¼ë©”ì‹œì§€:\s*(.+?)(?=\n|$)', response)
            if push_match:
                push_message = push_match.group(1).strip()[:50]
            
            # 7. ê²°ê³¼ ë¡œê¹…
            logger.info(f"âœ… ì˜í–¥ë„ í‰ê°€ ì™„ë£Œ: ì˜í–¥ë„={impact_score}/10, ë°©í–¥ì„±={direction_score}/5")
            logger.debug(f"ğŸ“Š ì‹œì¥ì˜í–¥: {market_reaction}")
            logger.debug(f"ğŸ“± ì•Œë¦¼ë©”ì‹œì§€: {push_message}")
            
            # 8. í¬ë¡œë§ˆ DB ì—…ë°ì´íŠ¸ (ê³ ì˜í–¥ ë‰´ìŠ¤ë§Œ)
            if impact_score >= 0.7:  # 0.7 ì´ìƒ
                try:
                    # ê³¼ê±° ì¤‘ìš” ì´ë²¤íŠ¸ DBì— ì¶”ê°€ (ì˜¬ë°”ë¥¸ ë°©ì‹ìœ¼ë¡œ ìˆ˜ì •)
                    high_impact_data = {
                        "stock_code": stock_code,
                        "stock_name": self.stock_names.get(stock_code, stock_code),
                        "title": news_item["title"],
                        "summary": news_item["content"][:200] + "...",
                        "impact_score": impact_score / 10.0,  # 0.0-1.0 ë³€í™˜
                        "publication_date": news_item.get("published_at", datetime.now()),
                    }
                    
                    # ê³ ì˜í–¥ ë‰´ìŠ¤ë¡œ ì €ì¥
                    self.vector_db.add_high_impact_news(high_impact_data)
                    
                    logger.debug("ğŸ“š ê³ ì˜í–¥ ë‰´ìŠ¤ë¥¼ í¬ë¡œë§ˆ DBì— ì €ì¥ ì™„ë£Œ")
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ í¬ë¡œë§ˆ DB ì €ì¥ ì‹¤íŒ¨: {e}")
            
            return impact_score, direction_score, market_reaction, push_message
            
        except Exception as e:
            logger.error(f"âŒ ë‰´ìŠ¤ ì˜í–¥ë„ í‰ê°€ ì‹¤íŒ¨: {e}")
            log_error_with_traceback("ë‰´ìŠ¤ ì˜í–¥ë„ í‰ê°€ ì‹¤íŒ¨", e, {
                "stock_name": stock_name,
                "title": news_item.get("title", "")[:50]
            })
            return 5, 3, "í‰ê°€ ì‹¤íŒ¨", "ë‰´ìŠ¤ ë¶„ì„ ì˜¤ë¥˜"
    
    async def generate_news_summary(self, news_item: Dict) -> Tuple[str, str]:
        """ë‰´ìŠ¤ í•µì‹¬ ìš”ì•½ ìƒì„± (HyperCLOVA API ì‚¬ìš©) - ì™„ì „ ì¬êµ¬ì„±"""
        try:
            title = news_item.get('title', '').strip()
            content = news_item.get('content', '').strip()
            stock_code = news_item.get('stock_code', '')
            stock_name = self.stock_names.get(stock_code, stock_code)
            
            logger.info(f"ğŸ“ ë‰´ìŠ¤ ìš”ì•½ ìƒì„± ì‹œì‘")
            logger.info(f"   â€¢ ì¢…ëª©: {stock_name} ({stock_code})")
            logger.info(f"   â€¢ ì œëª©: {title[:50]}...")
            logger.info(f"   â€¢ ë‚´ìš© ê¸¸ì´: {len(content)}ì")
            
            # 1ë‹¨ê³„: ê¸°ë³¸ ë°ì´í„° ê²€ì¦
            if not title:
                logger.warning("âŒ ì œëª©ì´ ë¹„ì–´ìˆìŒ - ìš”ì•½ ë¶ˆê°€")
                return "ì œëª© ì—†ìŒ", "NO_TITLE"
            
            # 2ë‹¨ê³„: ë‚´ìš©ì´ ë„ˆë¬´ ì§§ì€ ê²½ìš° ì œëª© ê°€ê³µ
            if len(content) < 50:
                logger.info(f"ğŸ“ ë‚´ìš©ì´ ì§§ìŒ ({len(content)}ì) - ì œëª© ê¸°ë°˜ ìš”ì•½ ìƒì„±")
                processed_title = self._process_title_for_summary(title)
                return processed_title, "TITLE_BASED"
            
            # 3ë‹¨ê³„: HyperCLOVA API ìš”ì•½ ì‹œë„ (ì™„ì „ ì¬êµ¬ì„±)
            logger.info("ğŸ¤– HyperCLOVA API ìš”ì•½ ì‹œë„ ì‹œì‘")
            
            # ì…ë ¥ ë°ì´í„° ì •ì œ
            clean_title = title.replace('"', '').replace("'", "").strip()
            clean_content = content[:1000].replace('"', '').replace("'", "").strip()  # 1000ìë¡œ ì œí•œ
            
            logger.debug(f"ğŸ“ ì •ì œëœ ì œëª©: {clean_title[:30]}...")
            logger.debug(f"ğŸ“ ì •ì œëœ ë‚´ìš©: {clean_content[:100]}...")
            
            # ê°•í™”ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±
            summary_prompt = f"""
ë‹¤ìŒ ë‰´ìŠ¤ë¥¼ 1000ì ì´ë‚´ë¡œ í•µì‹¬ ìš”ì•½í•´ì£¼ì„¸ìš”.

ì¢…ëª©: {stock_name}
ì œëª©: {clean_title}
ë‚´ìš©: {clean_content}

ìš”ì•½ ê·œì¹™:
1. íˆ¬ìì ê´€ì ì—ì„œ í•µì‹¬ ë‚´ìš©ë§Œ
2. 1000ì ì´ë‚´ ê°„ê²°í•˜ê²Œ
3. ê°ì •ì  í‘œí˜„ ì œê±°

ì‘ë‹µ: (1000ì ì´ë‚´ ìš”ì•½)
"""
            
            logger.debug(f"ğŸ“ í”„ë¡¬í”„íŠ¸ ì¤€ë¹„ ì™„ë£Œ (ì´ {len(summary_prompt)}ì)")
            
            # API í˜¸ì¶œ ì‹œë„ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
            max_retries = 2
            for attempt in range(max_retries):
                try:
                    logger.info(f"ğŸ”„ HyperCLOVA API í˜¸ì¶œ ì‹œë„ {attempt + 1}/{max_retries}")
                    
                    api_start_time = time.time()
                    
                    # ì‹¤ì œ API í˜¸ì¶œ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
                    raw_response = await self._call_llm_with_retry(summary_prompt, max_retries=2, base_delay=2.0)
                    
                    api_end_time = time.time()
                    api_duration = api_end_time - api_start_time
                    
                    logger.info(f"ğŸ“¡ API í˜¸ì¶œ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {api_duration:.2f}ì´ˆ)")
                    logger.debug(f"ğŸ“¡ ì›ë³¸ ì‘ë‹µ: {raw_response}")
                    
                    # ì‘ë‹µ ê²€ì¦
                    if not raw_response:
                        logger.warning(f"âš ï¸ ì‹œë„ {attempt + 1}: API ì‘ë‹µ None")
                        if attempt == max_retries - 1:
                            break
                        continue
                    
                    if len(raw_response.strip()) < 5:
                        logger.warning(f"âš ï¸ ì‹œë„ {attempt + 1}: API ì‘ë‹µ ë„ˆë¬´ ì§§ìŒ ({len(raw_response)}ì)")
                        if attempt == max_retries - 1:
                            break
                        continue
                    
                    # ì‘ë‹µ íŒŒì‹± (ë‹¤ë‹¨ê³„)
                    logger.debug("ğŸ” ì‘ë‹µ íŒŒì‹± ì‹œì‘...")
                    
                    parsed_summary = ""
                    
                    # íŒŒì‹± ë°©ë²• 1: 'ì‘ë‹µ:' íŒ¨í„´
                    response_match = re.search(r'ì‘ë‹µ:\s*(.+?)(?=\n|$)', raw_response, re.DOTALL)
                    if response_match:
                        parsed_summary = response_match.group(1).strip()
                        logger.debug(f"âœ… íŒŒì‹± ë°©ë²• 1 ì„±ê³µ: {parsed_summary[:30]}...")
                    
                    # íŒŒì‹± ë°©ë²• 2: ì²« ë²ˆì§¸ ì˜ë¯¸ìˆëŠ” ì¤„
                    if not parsed_summary:
                        lines = raw_response.strip().split('\n')
                        for line in lines:
                            line = line.strip()
                            if line and len(line) > 10 and not any(keyword in line for keyword in ['ì‘ë‹µ:', 'ìš”ì•½:', 'ê·œì¹™:', 'ì¢…ëª©:']):
                                parsed_summary = line
                                logger.debug(f"âœ… íŒŒì‹± ë°©ë²• 2 ì„±ê³µ: {parsed_summary[:30]}...")
                                break
                    
                    # íŒŒì‹± ë°©ë²• 3: ì „ì²´ ì‘ë‹µ ì •ì œ
                    if not parsed_summary:
                        cleaned_response = raw_response.strip()
                        # ë¶ˆí•„ìš”í•œ íŒ¨í„´ ì œê±°
                        cleaned_response = re.sub(r'(ì‘ë‹µ|ìš”ì•½|ì¢…ëª©|ì œëª©|ë‚´ìš©):\s*', '', cleaned_response)
                        cleaned_response = re.sub(r'\n+', ' ', cleaned_response)
                        if len(cleaned_response) > 10:
                            parsed_summary = cleaned_response
                            logger.debug(f"âœ… íŒŒì‹± ë°©ë²• 3 ì„±ê³µ: {parsed_summary[:30]}...")
                    
                    # ìµœì¢… ê²€ì¦ ë° ì •ë¦¬
                    if parsed_summary:
                        # ê¸¸ì´ ì œí•œ ë° ì •ë¦¬
                        final_summary = parsed_summary[:1000].strip()
                        
                        # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
                        final_summary = re.sub(r'["\'\[\]{}()]', '', final_summary)
                        final_summary = re.sub(r'\s+', ' ', final_summary)
                        
                        if len(final_summary) >= 10:
                            logger.info(f"âœ… HyperCLOVA ìš”ì•½ ì„±ê³µ: '{final_summary}'")
                            return final_summary, "HYPERCLOVA_SUCCESS"
                        else:
                            logger.warning(f"âš ï¸ íŒŒì‹±ëœ ìš”ì•½ì´ ë„ˆë¬´ ì§§ìŒ: '{final_summary}'")
                    
                    logger.warning(f"âš ï¸ ì‹œë„ {attempt + 1}: íŒŒì‹± ì‹¤íŒ¨")
                    
                except Exception as api_error:
                    api_end_time = time.time()
                    api_duration = api_end_time - api_start_time if 'api_start_time' in locals() else 0
                    logger.warning(f"âš ï¸ ì‹œë„ {attempt + 1} API ì˜¤ë¥˜ (ì†Œìš”ì‹œê°„: {api_duration:.2f}ì´ˆ): {api_error}")
                    
                    if attempt == max_retries - 1:
                        log_error_with_traceback("HyperCLOVA API ìµœì¢… ì‹¤íŒ¨", api_error, {
                            "stock_code": stock_code,
                            "title": title[:50],
                            "content_length": len(content),
                            "attempt": attempt + 1
                        })
            
            # 4ë‹¨ê³„: ëª¨ë“  API ì‹œë„ ì‹¤íŒ¨ ì‹œ ì œëª© ê¸°ë°˜ ìš”ì•½
            logger.warning("âŒ ëª¨ë“  HyperCLOVA API ì‹œë„ ì‹¤íŒ¨ - ì œëª© ê¸°ë°˜ ìš”ì•½ìœ¼ë¡œ ëŒ€ì²´")
            fallback_summary = self._process_title_for_summary(title)
            return fallback_summary, "API_FAILED_FALLBACK"
                
        except Exception as e:
            logger.error(f"âŒ ë‰´ìŠ¤ ìš”ì•½ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {e}")
            log_error_with_traceback("ë‰´ìŠ¤ ìš”ì•½ ì „ì²´ ì‹¤íŒ¨", e, {
                "stock_code": news_item.get('stock_code', ''),
                "title": news_item.get('title', '')[:50]
            })
            # ìµœì¢… fallback
            fallback_title = news_item.get('title', 'ë‰´ìŠ¤ ìš”ì•½ ì˜¤ë¥˜')
            final_summary = fallback_title[:50] + "..." if len(fallback_title) > 50 else fallback_title
            return final_summary, "TOTAL_ERROR"

    def _process_title_for_summary(self, title: str) -> str:
        """ì œëª©ì„ ìš”ì•½ìš©ìœ¼ë¡œ ê°€ê³µ"""
        try:
            # ê°„ë‹¨í•œ ì œëª© ì •ë¦¬
            import re
            cleaned = title.strip()
            
            # HTML ì—”í‹°í‹° ì œê±°
            html_entities = {
                '&quot;': '"',
                '&amp;': '&',
                '&lt;': '<',
                '&gt;': '>',
                '&nbsp;': ' '
            }
            
            for entity, replacement in html_entities.items():
                cleaned = cleaned.replace(entity, replacement)
            
            # ì–¸ë¡ ì‚¬ íƒœê·¸ ì œê±° ([ì„œìš¸ê²½ì œ], [ì—°í•©ë‰´ìŠ¤] ë“±)
            cleaned = re.sub(r'\[.*?\]', '', cleaned)
            
            # ê¸°ìëª… ì œê±° (ê¹€â—‹â—‹ ê¸°ì, â—‹â—‹â—‹ê¸°ì ë“±)
            cleaned = re.sub(r'[ê°€-í£]{2,3}\s*ê¸°ì', '', cleaned)
            
            # ì—°ì† ê³µë°± ì •ë¦¬
            cleaned = re.sub(r'\s+', ' ', cleaned)
            
            return cleaned.strip()
            
        except Exception as e:
            logger.error(f"ì œëª© ì •ë¦¬ ì˜¤ë¥˜: {e}")
            return title.strip()
    
    def get_stock_price_history(self, stock_code: str, news_date: str, days: int = 5) -> List[Dict]:
        """ì£¼ê°€ ë°ì´í„° ì¡°íšŒ (pykrx ê¸°ë°˜) - ë‰´ìŠ¤ ë°œìƒì¼ Të¶€í„° T+4ê¹Œì§€"""
        try:
            logger.info(f"ğŸ“ˆ ì£¼ê°€ ë°ì´í„° ì¡°íšŒ ì‹œì‘: {stock_code}, ë‰´ìŠ¤ ë°œìƒì¼: {news_date}")
            logger.info(f"ğŸ”— pykrx API í˜¸ì¶œ ì¤€ë¹„ ì¤‘...")
            
            # pykrx import
            try:
                from pykrx import stock
            except ImportError:
                logger.error("âŒ pykrx ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install pykrxë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
                return []
            
            # ë‰´ìŠ¤ ë°œìƒì¼ íŒŒì‹± (Tì¼)
            try:
                news_dt = datetime.strptime(news_date, '%Y-%m-%d')
            except ValueError:
                # ë‹¤ë¥¸ í˜•ì‹ìœ¼ë¡œ ì‹œë„
                news_dt = datetime.strptime(news_date[:10], '%Y-%m-%d')
            
            # ì¡°íšŒ ê¸°ê°„: Tì¼ë¶€í„° T+4ì¼ê¹Œì§€ (ì´ 5ì¼)
            start_dt = news_dt
            end_dt = news_dt + timedelta(days=days)
            logger.info(f"ğŸ“… ì¡°íšŒ ê¸°ê°„: T({start_dt.strftime('%Y-%m-%d')}) ~ T+{days-1}({end_dt.strftime('%Y-%m-%d')})")
            
            # pykrxë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ê°€ ë°ì´í„° ì¡°íšŒ
            logger.info(f"ğŸŒ pykrx API í˜¸ì¶œ ì¤‘...")
            
            # ë‚ ì§œ í˜•ì‹ì„ YYYYMMDDë¡œ ë³€í™˜
            start_date = start_dt.strftime('%Y%m%d')
            end_date = end_dt.strftime('%Y%m%d')
            
            # pykrxë¡œ OHLCV ë°ì´í„° ì¡°íšŒ
            df = stock.get_market_ohlcv(start_date, end_date, stock_code)
            
            logger.info(f"âœ… pykrx API í˜¸ì¶œ ì™„ë£Œ")
            logger.info(f"ğŸ“Š ìˆ˜ì‹ ëœ ë°ì´í„°: {len(df)}ì¼")
            
            if df.empty:
                logger.warning(f"âš ï¸ ì£¼ê°€ ë°ì´í„° ì—†ìŒ: {stock_code}")
                return []
            
            logger.info(f"ğŸ“Š ì£¼ê°€ ë°ì´í„° ì²˜ë¦¬ ì‹œì‘: {len(df)}ì¼ ë°ì´í„°")
            price_history = []
            
            for idx, row in df.iterrows():
                try:
                    # ë‚ ì§œë¥¼ ì•ˆì „í•˜ê²Œ ë¬¸ìì—´ë¡œ ë³€í™˜
                    date_str = str(idx)[:10] if str(idx) else "Unknown"
                    price_data = {
                        "date": date_str,
                        "open": round(float(row["ì‹œê°€"]), 2),
                        "high": round(float(row["ê³ ê°€"]), 2),
                        "low": round(float(row["ì €ê°€"]), 2),
                        "close": round(float(row["ì¢…ê°€"]), 2),
                        "volume": int(row["ê±°ë˜ëŸ‰"]),
                        "change": 0.0,
                        "change_percent": 0.0
                    }
                    price_history.append(price_data)
                    logger.debug(f"ğŸ“ˆ {price_data['date']}: ì¢…ê°€ {price_data['close']:,.0f}ì›")
                except Exception as date_error:
                    logger.warning(f"âš ï¸ ë‚ ì§œ ì²˜ë¦¬ ì‹¤íŒ¨: {date_error}")
                    continue
            
            # ë“±ë½ë¥  ê³„ì‚°
            logger.info(f"ğŸ“Š ë“±ë½ë¥  ê³„ì‚° ì¤‘...")
            for i in range(1, len(price_history)):
                prev_close = price_history[i-1]["close"]
                current_close = price_history[i]["close"]
                change = current_close - prev_close
                change_percent = (change / prev_close) * 100 if prev_close != 0 else 0.0
                
                price_history[i]["change"] = round(change, 2)
                price_history[i]["change_percent"] = round(change_percent, 2)
            
            logger.info(f"âœ… ì£¼ê°€ ë°ì´í„° ì¡°íšŒ ì™„ë£Œ: {len(price_history)}ì¼ ë°ì´í„°")
            if price_history:
                logger.info(f"ğŸ“Š Tì¼ ì¢…ê°€: {price_history[0]['close']:,.0f}ì›")
                logger.info(f"ğŸ“Š T+{len(price_history)-1}ì¼ ì¢…ê°€: {price_history[-1]['close']:,.0f}ì› (ë³€ë™: {price_history[-1]['change_percent']:.1f}%)")
            return price_history
            
        except Exception as e:
            logger.error(f"âŒ ì£¼ê°€ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
            log_error_with_traceback("ì£¼ê°€ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨", e, {
                "stock_code": stock_code,
                "news_date": str(news_date),
                "days": days
            })
            return []
    
    def format_price_history_for_telegram(self, price_history: List[Dict], similar_case: Dict) -> str:
        """ì£¼ê°€ ë°ì´í„°ë¥¼ í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ìš©ìœ¼ë¡œ í¬ë§·íŒ…"""
        try:
            if not price_history:
                return "ğŸ“ˆ ì£¼ê°€ ë°ì´í„°ë¥¼ ì¡°íšŒí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            # ê³¼ê±° ì‚¬ë¡€ ì •ë³´
            event_date = similar_case.get('published_date', '')
            event_title = similar_case.get('title', '')
            similarity_score = similar_case.get('similarity_score', 0.0)
            
            # ì£¼ê°€ ì¶”ì´ í¬ë§·íŒ…
            formatted_lines = []
            formatted_lines.append(f"ğŸ“Š <b>ê³¼ê±° ìœ ì‚¬ ì‚¬ë¡€ ì£¼ê°€ ì¶”ì´</b>")
            formatted_lines.append(f"ğŸ” ìœ ì‚¬ ì‚¬ë¡€: {event_title[:50]}...")
            formatted_lines.append(f"ğŸ“… ë°œìƒì¼: {event_date[:10]}")
            formatted_lines.append(f"ğŸ¯ ìœ ì‚¬ë„: {similarity_score:.2f}")
            formatted_lines.append("")
            formatted_lines.append("ğŸ“ˆ <b>ê³¼ê±° ìœ ì‚¬ ì‚¬ë¡€ 5ì¼ê°„ì˜ ì£¼ê°€ ì¶”ì´</b>")
            
            for i, day in enumerate(price_history):
                date = day['date']
                close_price = day['close']
                change = day['change']
                change_percent = day['change_percent']
                
                # ë“±ë½ì— ë”°ë¥¸ ì´ëª¨ì§€
                if change_percent > 0:
                    trend_emoji = "ğŸ“ˆ"
                    change_text = f"+{change:,.0f}ì› (+{change_percent:.1f}%)"
                elif change_percent < 0:
                    trend_emoji = "ğŸ“‰"
                    change_text = f"{change:,.0f}ì› ({change_percent:.1f}%)"
                else:
                    trend_emoji = "â¡ï¸"
                    change_text = "ë³€ë™ ì—†ìŒ"
                
                # Tì¼ë¶€í„° T+4ê¹Œì§€ í‘œì‹œ
                if i == 0:
                    day_label = "Tì¼"
                else:
                    day_label = f"T+{i}ì¼"
                formatted_lines.append(f"{trend_emoji} {day_label} ({date}): {close_price:,.0f}ì› {change_text}")
            
            # ì „ì²´ ìˆ˜ìµë¥  ê³„ì‚°
            if len(price_history) > 1:
                first_price = price_history[0]['close']
                last_price = price_history[-1]['close']
                total_return = ((last_price - first_price) / first_price) * 100
                
                if total_return > 0:
                    return_emoji = "ğŸš€"
                    return_text = f"+{total_return:.1f}%"
                elif total_return < 0:
                    return_emoji = "â¬‡ï¸"
                    return_text = f"{total_return:.1f}%"
                else:
                    return_emoji = "â¡ï¸"
                    return_text = "0.0%"
                
                formatted_lines.append("")
                formatted_lines.append(f"ğŸ’° <b>5ì¼ê°„ ìˆ˜ìµë¥ : {return_emoji} {return_text}</b>")
            
            return "\n".join(formatted_lines)
            
        except Exception as e:
            logger.error(f"âŒ ì£¼ê°€ ë°ì´í„° í¬ë§·íŒ… ì‹¤íŒ¨: {e}")
            return "ğŸ“ˆ ì£¼ê°€ ë°ì´í„° í¬ë§·íŒ… ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def format_trend_data_for_telegram(self, trend_data: Dict, similar_case: Dict) -> str:
        """StockTrendService ë°ì´í„°ë¥¼ í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ìš©ìœ¼ë¡œ í¬ë§·íŒ…"""
        try:
            if not trend_data or not trend_data.get('daily_data'):
                return "ğŸ“ˆ ì£¼ê°€ ë°ì´í„°ë¥¼ ì¡°íšŒí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            # ê³¼ê±° ì‚¬ë¡€ ì •ë³´
            event_date = similar_case.get('published_date', '')
            event_title = similar_case.get('title', '')
            similarity_score = similar_case.get('similarity_score', 0.0)
            
            # ë‚ ì§œ ì²˜ë¦¬ (datetime ê°ì²´ì¸ ê²½ìš° ë¬¸ìì—´ë¡œ ë³€í™˜)
            if hasattr(event_date, 'strftime'):
                event_date_str = event_date.strftime('%Y-%m-%d')
            elif isinstance(event_date, str):
                event_date_str = event_date[:10] if len(event_date) >= 10 else str(event_date)
            else:
                event_date_str = str(event_date)[:10]
            
            # ì£¼ê°€ ì¶”ì´ í¬ë§·íŒ…
            formatted_lines = []
            formatted_lines.append(f"ğŸ“Š <b>ê³¼ê±° ìœ ì‚¬ ì‚¬ë¡€ ì£¼ê°€ ì¶”ì´ (pykrx)</b>")
            formatted_lines.append(f"ğŸ” ìœ ì‚¬ ì‚¬ë¡€: {event_title[:50]}...")
            formatted_lines.append(f"ğŸ“… ë°œìƒì¼: {event_date_str}")
            formatted_lines.append(f"ğŸ¯ ìœ ì‚¬ë„: {similarity_score:.2f}")
            formatted_lines.append("")
            formatted_lines.append("ğŸ“ˆ <b>ì£¼ê°€ ì¶”ì´ (Tì¼ë¶€í„° T+4ê¹Œì§€)</b>")
            
            trend_data_list = trend_data['daily_data']
            
            for i, day in enumerate(trend_data_list):
                date = day.get('date', '')
                close_price = day.get('close', 0)
                change_rate = day.get('change_rate', 0)
                volume_man = day.get('volume_man', 0)
                trading_value_eok = day.get('trading_value_eok', 0)
                
                # ë“±ë½ì— ë”°ë¥¸ ì´ëª¨ì§€
                if change_rate > 0:
                    trend_emoji = "ğŸ“ˆ"
                    change_text = f"(+{change_rate:.1f}%)"
                elif change_rate < 0:
                    trend_emoji = "ğŸ“‰"
                    change_text = f"({change_rate:.1f}%)"
                else:
                    trend_emoji = "â¡ï¸"
                    change_text = "(0.0%)"
                
                # Tì¼ë¶€í„° T+4ê¹Œì§€ í‘œì‹œ
                if i == 0:
                    day_label = "Tì¼"
                else:
                    day_label = f"T+{i}ì¼"
                formatted_lines.append(f"{trend_emoji} {day_label} ({date}): {close_price:,.0f}ì› {change_text}")
                
                # ê±°ë˜ëŸ‰ ì •ë³´ë„ í¬í•¨
                if volume_man > 0:
                    formatted_lines.append(f"    ğŸ“Š ê±°ë˜ëŸ‰: {volume_man:,.1f}ë§Œì£¼ | ê±°ë˜ëŒ€ê¸ˆ: {trading_value_eok:,.1f}ì–µì›")
            
            # ì „ì²´ ìˆ˜ìµë¥  ê³„ì‚°
            if len(trend_data_list) > 1:
                first_price = trend_data_list[0].get('close', 0)
                last_price = trend_data_list[-1].get('close', 0)
                
                if first_price > 0:
                    total_return = ((last_price - first_price) / first_price) * 100
                    
                    if total_return > 0:
                        return_emoji = "ğŸš€"
                        return_text = f"+{total_return:.1f}%"
                    elif total_return < 0:
                        return_emoji = "â¬‡ï¸"
                        return_text = f"{total_return:.1f}%"
                    else:
                        return_emoji = "â¡ï¸"
                        return_text = "0.0%"
                    
                    formatted_lines.append("")
                    formatted_lines.append(f"ğŸ’° <b>5ì¼ê°„ ìˆ˜ìµë¥ : {return_emoji} {return_text}</b>")
            
            # ì¶”ê°€ ë¶„ì„ ì •ë³´
            if trend_data.get('summary'):
                summary = trend_data['summary']
                formatted_lines.append("")
                formatted_lines.append("ğŸ” <b>ì¶”ê°€ ë¶„ì„</b>")
                formatted_lines.append(f"â€¢ ê¸°ê°„ ìˆ˜ìµë¥ : {summary.get('total_change_rate', 0):+.2f}%")
                formatted_lines.append(f"â€¢ í‰ê·  ê±°ë˜ëŸ‰: {summary.get('avg_volume_man', 0):,.1f}ë§Œì£¼")
                formatted_lines.append(f"â€¢ í‰ê·  ê±°ë˜ëŒ€ê¸ˆ: {summary.get('avg_trading_value_eok', 0):,.1f}ì–µì›")
                formatted_lines.append(f"â€¢ ë³€ë™ì„±: {summary.get('volatility', 0):.2f}%")
            
            return "\n".join(formatted_lines)
            
        except Exception as e:
            logger.error(f"âŒ StockTrendService ë°ì´í„° í¬ë§·íŒ… ì‹¤íŒ¨: {e}")
            return "ğŸ“ˆ ì£¼ê°€ ë°ì´í„° í¬ë§·íŒ… ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def create_fallback_price_message(self, similar_case: Dict, stock_code: str, stock_name: Optional[str]) -> str:
        """ì£¼ê°€ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨ ì‹œ ëŒ€ì²´ ë©”ì‹œì§€ ìƒì„±"""
        try:
            event_title = similar_case.get('title', 'ê³¼ê±° ì‚¬ë¡€')
            event_date = similar_case.get('published_date', 'ë‚ ì§œ ë¶ˆëª…')
            similarity_score = similar_case.get('similarity_score', 0.0)
            
            formatted_lines = []
            formatted_lines.append(f"ğŸ“Š <b>ê³¼ê±° ìœ ì‚¬ ì‚¬ë¡€ ë¶„ì„ (ë°ì´í„° ì œí•œ)</b>")
            formatted_lines.append(f"ğŸ” ìœ ì‚¬ ì‚¬ë¡€: {event_title[:50]}...")
            formatted_lines.append(f"ğŸ“… ë°œìƒì¼: {event_date[:10]}")
            formatted_lines.append(f"ğŸ¯ ìœ ì‚¬ë„: {similarity_score:.3f}")
            formatted_lines.append("")
            formatted_lines.append("âš ï¸ <b>ì£¼ê°€ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨</b>")
            formatted_lines.append(f"â€¢ ì¢…ëª©: {stock_name} ({stock_code})")
            formatted_lines.append("â€¢ ê³¼ê±° ì£¼ê°€ ë°ì´í„°ë¥¼ ì¡°íšŒí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            formatted_lines.append("â€¢ ì‹œì¥ ìƒí™©ì´ë‚˜ ë°ì´í„° ì œê³µ ì œí•œìœ¼ë¡œ ì¸í•œ ë¬¸ì œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            formatted_lines.append("")
            formatted_lines.append("ğŸ’¡ <b>ë¶„ì„ ëŒ€ì•ˆ</b>")
            formatted_lines.append("â€¢ ìœ ì‚¬í•œ ê³¼ê±° ì‚¬ë¡€ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
            formatted_lines.append("â€¢ í•´ë‹¹ ì¢…ëª©ì˜ ìµœê·¼ ë™í–¥ì„ ë³„ë„ë¡œ í™•ì¸í•´ë³´ì„¸ìš”.")
            formatted_lines.append("â€¢ ì „ë¬¸ê°€ ì˜ê²¬ì´ë‚˜ ê¸°ìˆ ì  ë¶„ì„ì„ ì°¸ê³ í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.")
            
            return "\n".join(formatted_lines)
            
        except Exception as e:
            logger.error(f"âŒ ëŒ€ì²´ ë©”ì‹œì§€ ìƒì„± ì‹¤íŒ¨: {e}")
            return "ğŸ“ˆ ì£¼ê°€ ë°ì´í„° ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def save_news_to_db(self, news_item: Dict, impact_score: float, reasoning: str):
        """ë‰´ìŠ¤ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ (ë‹¨ìˆœí™”)"""
        try:
            logger.debug(f"ğŸ’¾ ë‰´ìŠ¤ DB ì €ì¥: {news_item['title'][:30]}...")
            
            # âœ… ë‹¨ìˆœí™”: ê¸°ë³¸ ë‰´ìŠ¤ ì •ë³´ë§Œ MySQLì— ì €ì¥
            query = """
            INSERT INTO news (
                stock_code, title, content, url, source, published_at,
                impact_score, reasoning, created_at
            ) VALUES (
                %s, %s, %s, %s, %s, %s, %s, %s, %s
            ) ON DUPLICATE KEY UPDATE
                impact_score = VALUES(impact_score),
                reasoning = VALUES(reasoning),
                updated_at = NOW()
            """
            
            params = (
                news_item["stock_code"],
                news_item["title"],
                news_item["content"],
                news_item["url"],
                news_item.get("source", ""),
                news_item["published_at"],
                impact_score,
                reasoning,
                datetime.now()
            )
            
            self.mysql_client.execute_query(query, params)
            logger.info(f"âœ… ë‰´ìŠ¤ ì €ì¥ ì™„ë£Œ: {news_item['title'][:50]}...")
            
        except Exception as e:
            logger.error(f"âŒ ë‰´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
            # âŒ ë³µì¡í•œ ì—ëŸ¬ ì²˜ë¦¬ ì œê±°
    
    def save_to_vector_db(self, news_item: Dict, impact_score: float):
        """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        try:
            # ê³ ì˜í–¥ ë‰´ìŠ¤ì¸ ê²½ìš° ì˜êµ¬ ì €ì¥
            if impact_score >= self.impact_threshold:  # 0.7 ì´ìƒ
                high_impact_data = {
                    "stock_code": news_item["stock_code"],
                    "stock_name": self.stock_names.get(news_item["stock_code"], news_item["stock_code"]),
                    "title": news_item["title"],
                    "summary": news_item.get("content", "")[:200],  # ì²« 200ìë¥¼ ìš”ì•½ìœ¼ë¡œ ì‚¬ìš©
                    "impact_score": impact_score,
                    "publication_date": news_item["published_at"],
                }
                
                # ê³ ì˜í–¥ ë‰´ìŠ¤ ì €ì¥
                self.vector_db.add_high_impact_news(high_impact_data)
                logger.info(f"ê³ ì˜í–¥ ë‰´ìŠ¤ ë²¡í„° DB ì €ì¥ ì™„ë£Œ: {news_item['title'][:50]}...")
            
            # ì¼ì¼ ë‰´ìŠ¤ ì €ì¥ì€ ì´ë¯¸ crawl_news_for_stockì—ì„œ ì²˜ë¦¬ë¨ (ì¤‘ë³µ ì €ì¥ ë°©ì§€)
            logger.debug(f"ì¼ì¼ ë‰´ìŠ¤ëŠ” ì´ë¯¸ ì €ì¥ë¨: {news_item['title'][:30]}...")
            
        except Exception as e:
            logger.error(f"ë²¡í„° DB ì €ì¥ ì‹¤íŒ¨: {e}")
            log_error_with_traceback("ë²¡í„° DB ì €ì¥ ì‹¤íŒ¨", e, {
                "stock_code": news_item.get("stock_code", ""),
                "title": news_item.get("title", "")[:50]
            })
    
    async def send_alert(self, news_item: Dict, impact_score: float, reasoning: str):
        """ê³ ì˜í–¥ ë‰´ìŠ¤ ì•Œë¦¼ ì „ì†¡ (ì‚¬ìš©ìë³„ ì„¤ì • í™•ì¸ + ì±„ë„ ì•Œë¦¼)"""
        # ğŸ”§ ìˆ˜ì •: datetime importë¥¼ í•¨ìˆ˜ ìµœìƒë‹¨ìœ¼ë¡œ ì´ë™í•˜ì—¬ UnboundLocalError í•´ê²°
        from datetime import datetime, timedelta
        
        try:
            stock_code = news_item["stock_code"]
            stock_name = self.stock_names.get(stock_code, stock_code)
            
            # ë‰´ìŠ¤ ìš”ì•½ ìƒì„± (ìƒì„¸ ë¡œê¹… ì¶”ê°€)
            logger.info("ğŸ“ ë‰´ìŠ¤ ìš”ì•½ ìƒì„± ì‹œì‘...")
            logger.debug(f"ğŸ“ ìš”ì•½ ëŒ€ìƒ ë‰´ìŠ¤: {news_item.get('title', 'No title')[:50]}...")
            
            summary_start_time = time.time()
            try:
                news_summary, summary_method = await self.generate_news_summary(news_item)
                summary_end_time = time.time()
                summary_duration = summary_end_time - summary_start_time
                
                # ì‹¤ì œ ìš”ì•½ ìƒì„± ë°©ì‹ì— ë”°ë¥¸ ì •í™•í•œ ë¡œê·¸ ì¶œë ¥
                if summary_method == "HYPERCLOVA_SUCCESS":
                    logger.info(f"âœ… HyperCLOVA ìš”ì•½ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {summary_duration:.2f}ì´ˆ)")
                elif summary_method == "TITLE_PROCESSING":
                    logger.info(f"âœ… ì œëª© ê°€ê³µ ìš”ì•½ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {summary_duration:.2f}ì´ˆ)")
                elif summary_method == "HYPERCLOVA_FALLBACK":
                    logger.warning(f"âš ï¸ HyperCLOVA ì‘ë‹µ ë¶€ì¡± - ì œëª© ì‚¬ìš© (ì†Œìš”ì‹œê°„: {summary_duration:.2f}ì´ˆ)")
                elif summary_method == "HYPERCLOVA_PARSE_ERROR":
                    logger.warning(f"âš ï¸ HyperCLOVA íŒŒì‹± ì‹¤íŒ¨ - ì œëª© ì‚¬ìš© (ì†Œìš”ì‹œê°„: {summary_duration:.2f}ì´ˆ)")
                elif summary_method == "HYPERCLOVA_API_ERROR":
                    logger.error(f"âŒ HyperCLOVA API ì‹¤íŒ¨ - ì œëª© ì‚¬ìš© (ì†Œìš”ì‹œê°„: {summary_duration:.2f}ì´ˆ)")
                else:
                    logger.error(f"âŒ ìš”ì•½ ìƒì„± ì‹¤íŒ¨ - ë°±ì—… ì‚¬ìš© (ì†Œìš”ì‹œê°„: {summary_duration:.2f}ì´ˆ)")
                
                logger.debug(f"ğŸ“ ìƒì„±ëœ ìš”ì•½ ë‚´ìš©: {news_summary[:100]}...")
                
                # ìš”ì•½ í’ˆì§ˆ ê²€ì¦ ë¡œê¹…
                if news_summary and len(news_summary.strip()) > 5:
                    logger.debug("âœ… ìš”ì•½ í’ˆì§ˆ ê²€ì¦ í†µê³¼")
                else:
                    logger.warning(f"âš ï¸ ìš”ì•½ í’ˆì§ˆ ë¬¸ì œ: '{news_summary}' (ê¸¸ì´: {len(news_summary) if news_summary else 0})")
                    
            except Exception as summary_error:
                summary_end_time = time.time()
                summary_duration = summary_end_time - summary_start_time
                logger.error(f"âŒ ìš”ì•½ ìƒì„± ì‹¤íŒ¨ (ì†Œìš”ì‹œê°„: {summary_duration:.2f}ì´ˆ): {summary_error}")
                log_error_with_traceback("ë‰´ìŠ¤ ìš”ì•½ ìƒì„± ì‹¤íŒ¨", summary_error, {
                    "stock_code": news_item.get("stock_code", ""),
                    "title": news_item.get("title", "")[:50],
                    "content_length": len(news_item.get("content", "")),
                    "duration": summary_duration
                })
                # ë°±ì—… ìš”ì•½ ìƒì„±
                news_summary = news_item.get('title', 'ìš”ì•½ ìƒì„± ì‹¤íŒ¨')[:50]
                if len(news_summary) > 50:
                    news_summary += "..."
                summary_method = "ERROR_FALLBACK"
            
            # ê³¼ê±° ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰ ë° ì£¼ê°€ ì¶”ì´ ë¶„ì„ (ê°•í™”ëœ ë²„ì „)
            logger.info("ğŸ“Š ê³¼ê±° ìœ ì‚¬ ì‚¬ë¡€ ë° ì£¼ê°€ ì¶”ì´ ë¶„ì„ ì‹œì‘...")
            similar_cases = self.search_similar_historical_cases(news_item)
            price_history_message = ""
            
            # ê³¼ê±° ìœ ì‚¬ ì‚¬ë¡€ ì²˜ë¦¬ (ê°œì„ ëœ ë°©ì‹)
            if similar_cases and len(similar_cases) > 0:
                # ê°€ì¥ ìœ ì‚¬í•œ ì‚¬ë¡€ ì„ íƒ
                most_similar_case = similar_cases[0]
                similarity_score = most_similar_case.get('similarity_score', 0.0)
                
                logger.info(f"ğŸ” ê°€ì¥ ìœ ì‚¬í•œ ì‚¬ë¡€ ë°œê²¬: '{most_similar_case.get('title', 'No Title')[:50]}...'")
                logger.info(f"ğŸ” ìœ ì‚¬ë„: {similarity_score:.3f}")
                logger.info(f"ğŸ“… ì‚¬ë¡€ ë‚ ì§œ: {most_similar_case.get('published_date', 'Unknown')}")
                
                # ìœ ì‚¬ë„ ê¸°ì¤€ ì™„í™” (0.1)ë¡œ ë” ë§ì€ ê²½ìš°ì— ë¶„ì„ ìˆ˜í–‰
                if similarity_score >= 0.0:
                    logger.info(f"ğŸš€ ìœ ì‚¬ë„ {similarity_score:.3f} >= 0.0 - ì£¼ê°€ ì¶”ì´ ë¶„ì„ ì‹œì‘")
                    
                    try:
                        # ê³¼ê±° ì‚¬ë¡€ ë‚ ì§œ ì¶”ì¶œ ë° ê²€ì¦
                        event_date = most_similar_case.get('published_date', '')
                        logger.debug(f"ğŸ“… ê³¼ê±° ì‚¬ë¡€ ë‚ ì§œ: {event_date}")
                        
                        # ë‚ ì§œ ì²˜ë¦¬ ê°•í™”
                        event_date_str = ""
                        if event_date:
                            try:
                                # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ ì§€ì›
                                if isinstance(event_date, str):
                                    if len(event_date) >= 10:
                                        event_date_str = event_date[:10]
                                    else:
                                        event_date_str = event_date
                                elif hasattr(event_date, 'strftime'):
                                    event_date_str = event_date.strftime('%Y-%m-%d')
                                else:
                                    event_date_str = str(event_date)[:10]
                                
                                logger.debug(f"ğŸ“… ì •ê·œí™”ëœ ë‚ ì§œ: {event_date_str}")
                                
                                # ë‚ ì§œ í˜•ì‹ ìœ íš¨ì„± ê²€ì¦
                                datetime.strptime(event_date_str, '%Y-%m-%d')
                                
                                # ì£¼ê°€ ì¶”ì´ ë°ì´í„° ì¡°íšŒ
                                price_history = self.get_stock_price_history(stock_code, event_date_str, 5)
                                
                                if price_history and len(price_history) > 0:
                                    logger.info(f"ğŸ“Š ì£¼ê°€ ì¶”ì´ ë°ì´í„° ì¡°íšŒ ì„±ê³µ: {len(price_history)}ê±´")
                                    price_history_message = self.format_price_history_for_telegram(price_history, most_similar_case)
                                else:
                                    logger.warning(f"âš ï¸ ì£¼ê°€ ì¶”ì´ ë°ì´í„° ì—†ìŒ: {event_date_str}")
                                    price_history_message = self.create_fallback_price_message(most_similar_case, stock_code, stock_name)
                                    
                            except ValueError as date_error:
                                logger.warning(f"âš ï¸ ë‚ ì§œ í˜•ì‹ ì˜¤ë¥˜: {event_date} - {date_error}")
                                price_history_message = self.create_fallback_price_message(most_similar_case, stock_code, stock_name)
                            except Exception as history_error:
                                logger.error(f"âŒ ì£¼ê°€ ì¶”ì´ ì¡°íšŒ ì‹¤íŒ¨: {history_error}")
                                price_history_message = self.create_fallback_price_message(most_similar_case, stock_code, stock_name)
                        else:
                            logger.warning(f"âš ï¸ ê³¼ê±° ì‚¬ë¡€ ë‚ ì§œ ì—†ìŒ")
                            price_history_message = self.create_fallback_price_message(most_similar_case, stock_code, stock_name)
                            
                    except Exception as analysis_error:
                        logger.error(f"âŒ ê³¼ê±° ì‚¬ë¡€ ë¶„ì„ ì‹¤íŒ¨: {analysis_error}")
                        price_history_message = self.create_fallback_price_message(most_similar_case, stock_code, stock_name)
                else:
                    logger.info(f"âš ï¸ ìœ ì‚¬ë„ {similarity_score:.3f} < 0.0 - ì£¼ê°€ ì¶”ì´ ë¶„ì„ ìƒëµ")
                    price_history_message = ""
            else:
                logger.info("âš ï¸ ìœ ì‚¬í•œ ê³¼ê±° ì‚¬ë¡€ ì—†ìŒ - ì£¼ê°€ ì¶”ì´ ë¶„ì„ ìƒëµ")
                price_history_message = ""
            
            # ì¢…ëª© ì •ë³´ ì¡°íšŒ
            stock_info = self.get_stock_info_for_code(stock_code)
            current_price = stock_info.get('current_price', 'N/A')
            prev_close = stock_info.get('prev_close', 'N/A')
            open_price = stock_info.get('open_price', 'N/A')
            high_price = stock_info.get('high_price', 'N/A')
            market_cap = stock_info.get('market_cap', 'N/A')
            per_ratio = stock_info.get('per_ratio', 'N/A')
            
            # ì˜í–¥ë„ ë ˆë²¨ ê²°ì •
            if impact_score >= 0.9:
                impact_level = "ğŸ”¥ ë§¤ìš° ë†’ìŒ"
                impact_emoji = "ğŸš¨"
            elif impact_score >= 0.7:
                impact_level = "ğŸ”¥ ë†’ìŒ"
                impact_emoji = "ğŸ”¥"
            elif impact_score >= 0.5:
                impact_level = "âš¡ ì¤‘ê°„"
                impact_emoji = "âš¡"
            else:
                impact_level = "ğŸ“Š ë³´í†µ"
                impact_emoji = "ğŸ“Š"
            
            # ì‹œê°„ í¬ë§·íŒ…
            pub_time = news_item['published_at']
            if isinstance(pub_time, str):
                time_str = pub_time
            else:
                time_str = pub_time.strftime('%Y-%m-%d %H:%M:%S')
            
            # ìš”ì•½ ìƒì„± ë°©ì‹ì— ë”°ë¥¸ ì„¤ëª… ì¶”ê°€
            summary_source = {
                "HYPERCLOVA_SUCCESS": "HyperCLOVA AI ë¶„ì„",
                "TITLE_PROCESSING": "ì œëª© ê°€ê³µ",
                "HYPERCLOVA_FALLBACK": "HyperCLOVA ë°±ì—…",
                "HYPERCLOVA_PARSE_ERROR": "HyperCLOVA íŒŒì‹± ì‹¤íŒ¨",
                "HYPERCLOVA_API_ERROR": "HyperCLOVA API ì‹¤íŒ¨",
                "ERROR_FALLBACK": "ì˜¤ë¥˜ ë°±ì—…"
            }.get(summary_method, "ì•Œ ìˆ˜ ì—†ëŠ” ë°©ì‹")
            
            # ë©”ì‹œì§€ êµ¬ì„± (ì£¼ê°€ ì¶”ì´ ì •ë³´ í¬í•¨)
            message_parts = []
            
            # ê¸°ë³¸ ì•Œë¦¼ ì •ë³´
            message_parts.append(f"{impact_emoji} <b>ê³ ì˜í–¥ ë‰´ìŠ¤ ì•Œë¦¼</b>")
            message_parts.append("")
            message_parts.append("ğŸ“Š <b>ì¢…ëª© í˜„í™©</b>")
            message_parts.append(f"â€¢ ì¢…ëª©: <b>{stock_name}</b> ({stock_code})")
            message_parts.append(f"â€¢ í˜„ì¬ê°€: <b>{current_price}</b> ì›")
            message_parts.append(f"â€¢ ì „ì¼ì¢…ê°€: {prev_close} ì›")
            message_parts.append(f"â€¢ ì‹œê°€: {open_price} ì› | ê³ ê°€: {high_price} ì›")
            message_parts.append("")
            message_parts.append("ğŸ“ˆ <b>ê¸°ì—… ì •ë³´</b>")
            message_parts.append(f"â€¢ ì‹œê°€ì´ì•¡: {market_cap}")
            message_parts.append(f"â€¢ PER: {per_ratio}")
            message_parts.append("")
            message_parts.append("ğŸ“° <b>ë‰´ìŠ¤ ì •ë³´</b>")
            message_parts.append(f"â€¢ ì œëª©: {news_item['title']}")
            message_parts.append(f"â€¢ ë°œí–‰: {time_str}")
            message_parts.append(f"â€¢ ì¶œì²˜: {news_item.get('source', 'N/A')}")
            message_parts.append("")
            message_parts.append(f"ğŸ” <b>í•µì‹¬ ìš”ì•½</b> ({summary_source})")
            message_parts.append(news_summary)
            message_parts.append("")
            message_parts.append("ğŸ’¡ <b>ë¶„ì„ ì§€í‘œ</b>")
            message_parts.append(f"â€¢ ì˜í–¥ë„: <b>{impact_score:.2f}/1.0</b> ({impact_level})")
            message_parts.append("â€¢ í•„í„°ë§: 3ë‹¨ê³„ í†µê³¼")
            message_parts.append(f"â€¢ ë¶„ì„ì‹œê°„: {datetime.now().strftime('%H:%M:%S')}")
            
            # ê³¼ê±° ì‚¬ë¡€ ì£¼ê°€ ì¶”ì´ ì¶”ê°€
            if price_history_message:
                message_parts.append("")
                message_parts.append("ğŸ“ˆ <b>ê³¼ê±° ìœ ì‚¬ ì‚¬ë¡€ ì£¼ê°€ ì¶”ì´</b>")
                message_parts.append(price_history_message)
            
            # ìµœì¢… ë©”ì‹œì§€ ì¡°í•©
            final_message = "\n".join(message_parts)
            
            # ğŸ†• ì‚¬ìš©ìë³„ ì•Œë¦¼ ì „ì†¡ (ì„¤ì • í™•ì¸)
            await self._send_user_notifications(news_item, final_message, impact_score)
            
            # ğŸ†• ì±„ë„ ì•Œë¦¼ ì „ì†¡ (ê¸°ì¡´ ë°©ì‹ ìœ ì§€)
            await self._send_channel_notification(final_message)
            
            logger.info(f"âœ… ë‰´ìŠ¤ ì•Œë¦¼ ì „ì†¡ ì™„ë£Œ: {stock_name} ({stock_code})")
            
        except Exception as e:
            logger.error(f"âŒ ë‰´ìŠ¤ ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨: {e}")
            log_error_with_traceback("ë‰´ìŠ¤ ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨", e, {
                "stock_code": news_item.get("stock_code", ""),
                "title": news_item.get("title", "")[:50],
                "impact_score": impact_score
            })
    
    async def _send_user_notifications(self, news_item: Dict, message: str, impact_score: float):
        """ì‚¬ìš©ìë³„ ì•Œë¦¼ ì „ì†¡ (ì„¤ì • í™•ì¸ + ì¢…ëª© í•„í„°ë§)"""
        try:
            # UserConfigLoader import
            from shared.service_config.user_config_loader import UserConfigLoader
            
            config_loader = UserConfigLoader()
            stock_code = news_item.get("stock_code", "")
            
            # ëª¨ë“  í™œì„± ì‚¬ìš©ì ì¡°íšŒ (í˜„ì¬ëŠ” í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ê³ ì • ì‚¬ìš©ì)
            # TODO: ì‹¤ì œë¡œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í™œì„± ì‚¬ìš©ì ëª©ë¡ì„ ì¡°íšŒí•´ì•¼ í•¨
            test_users = ["1"]  # í…ŒìŠ¤íŠ¸ìš© ì‚¬ìš©ì ID
            
            for user_id in test_users:
                try:
                    # ğŸ†• ì‚¬ìš©ìê°€ ì´ ì¢…ëª©ì— ê´€ì‹¬ì´ ìˆëŠ”ì§€ í™•ì¸
                    is_interested = await config_loader.is_user_interested_in_stock(user_id, stock_code)
                    if not is_interested:
                        logger.debug(f"âš ï¸ ì‚¬ìš©ìê°€ ì¢…ëª©ì— ê´€ì‹¬ ì—†ìŒ: {user_id} - {stock_code}")
                        continue
                    
                    # ì‚¬ìš©ìë³„ ì•Œë¦¼ ì„¤ì • ì¡°íšŒ
                    notification_settings = await config_loader.get_user_notification_settings(user_id)
                    
                    # ë‰´ìŠ¤ ì•Œë¦¼ì´ í™œì„±í™”ë˜ì–´ ìˆê³ , ì „ì²´ ì•Œë¦¼ì´ í™œì„±í™”ëœ ê²½ìš°ë§Œ ì „ì†¡
                    if (notification_settings.get("enabled", True) and 
                        notification_settings.get("news_alerts", True)):
                        
                        # ì‚¬ìš©ìë³„ í…”ë ˆê·¸ë¨ ì„¤ì • ì¡°íšŒ
                        telegram_config = await config_loader.get_user_telegram_config(user_id)
                        if telegram_config and telegram_config.get("enabled", True):
                            # ê°œë³„ ì‚¬ìš©ìì—ê²Œ ì•Œë¦¼ ì „ì†¡
                            await self._send_user_notification(user_id, message, telegram_config)
                            logger.info(f"âœ… ì‚¬ìš©ì ë‰´ìŠ¤ ì•Œë¦¼ ì „ì†¡ ì™„ë£Œ: {user_id} - {stock_code}")
                        else:
                            logger.debug(f"âš ï¸ ì‚¬ìš©ì í…”ë ˆê·¸ë¨ ë¹„í™œì„±í™”: {user_id}")
                    else:
                        logger.debug(f"âš ï¸ ì‚¬ìš©ì ë‰´ìŠ¤ ì•Œë¦¼ ë¹„í™œì„±í™”: {user_id}")
                        
                except Exception as user_error:
                    logger.error(f"âŒ ì‚¬ìš©ì ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨: {user_id} - {user_error}")
                    
        except Exception as e:
            logger.error(f"âŒ ì‚¬ìš©ìë³„ ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨: {e}")
    
    async def _send_user_notification(self, user_id: str, message: str, telegram_config: Dict):
        """ê°œë³„ ì‚¬ìš©ìì—ê²Œ ì•Œë¦¼ ì „ì†¡"""
        try:
            # ì‚¬ìš©ìë³„ ì±„íŒ… ID ì‚¬ìš©
            chat_id = telegram_config.get("chat_id")
            if chat_id:
                # í…”ë ˆê·¸ë¨ ë´‡ìœ¼ë¡œ ê°œë³„ ì‚¬ìš©ìì—ê²Œ ì „ì†¡
                from shared.apis.telegram_api import TelegramBotClient
                telegram_bot = TelegramBotClient()
                telegram_bot.send_message(message, str(chat_id))
                logger.info(f"âœ… ê°œë³„ ì‚¬ìš©ì ì•Œë¦¼ ì „ì†¡ ì™„ë£Œ: {user_id} -> {chat_id}")
            else:
                logger.warning(f"âš ï¸ ì‚¬ìš©ì ì±„íŒ… ID ì—†ìŒ: {user_id}")
                
        except Exception as e:
            logger.error(f"âŒ ê°œë³„ ì‚¬ìš©ì ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨: {user_id} - {e}")
    
    async def _send_channel_notification(self, message: str):
        """ì±„ë„ ì•Œë¦¼ ì „ì†¡ (ê¸°ì¡´ ë°©ì‹)"""
        try:
            from shared.apis.telegram_api import TelegramBotClient
            telegram_bot = TelegramBotClient()
            telegram_bot.send_message(message)
            logger.info("âœ… ì±„ë„ ì•Œë¦¼ ì „ì†¡ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ ì±„ë„ ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨: {e}")

    async def crawl_news_for_stock(self, stock_code: str):
        """ì¢…ëª©ë³„ ë‰´ìŠ¤ í¬ë¡¤ë§ (4ë‹¨ê³„ ì¤€ë¹„)"""
        try:
            logger.info(f"ğŸ“ˆ ì¢…ëª© {stock_code} ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘")
            
            # ì¢…ëª© ì •ë³´ ìˆ˜ì§‘
            logger.debug(f"ğŸ“Š ì¢…ëª© ì •ë³´ ìˆ˜ì§‘ ìš”ì²­: {stock_code}")
            stock_info = self.get_stock_info_for_code(stock_code)
            
            # ì¢…ëª© ì •ë³´ ìˆ˜ì§‘ ê²°ê³¼ ë¡œê¹…
            stock_name = stock_info.get("ì¢…ëª©ëª…", "Unknown")
            if stock_name != "Unknown":
                logger.debug(f"âœ… ì¢…ëª© ì •ë³´ ìˆ˜ì§‘ ì„±ê³µ: {stock_name} ({stock_code})")
                logger.debug(f"ğŸ”® 4ë‹¨ê³„ ì¤€ë¹„: {stock_name} ì¢…ëª© ì •ë³´ ì¤€ë¹„ ì™„ë£Œ")
            else:
                logger.warning(f"âš ï¸ ì¢…ëª© ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {stock_code} - ê¸°ë³¸ê°’ìœ¼ë¡œ ì§„í–‰")
                logger.debug(f"ğŸ”® 4ë‹¨ê³„ ì¤€ë¹„: ì¢…ëª© ì •ë³´ ì—†ì´ ì§„í–‰ - ë‚˜ì¤‘ì— ë³´ì™„ í•„ìš”")
            
            # API ë°©ì‹ìœ¼ë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘ (í˜ì´ì§€ 3ê°œ, í˜ì´ì§€ë‹¹ 20ê°œ)
            logger.debug(f"ğŸ“° ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘: {stock_code} (API ë°©ì‹)")
            news_list = self.crawl_naver_finance_news(stock_code, pages=2, size=3)
            logger.debug(f"ğŸ“° ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ: {len(news_list)}ê°œ ìˆ˜ì§‘")
            
            # ê° ë‰´ìŠ¤ì— ì¢…ëª© ì •ë³´ ì¶”ê°€
            logger.debug(f"ğŸ“Š ë‰´ìŠ¤ì— ì¢…ëª© ì •ë³´ ì¶”ê°€ ì¤‘...")
            for news_item in news_list:
                news_item.update({
                    "stock_info": stock_info,
                    "stock_name": stock_name
                })
                logger.debug(f"ğŸ“Š ë‰´ìŠ¤ì— ì¢…ëª© ì •ë³´ ì¶”ê°€: '{news_item['title'][:30]}...' -> {stock_name}")
            
            logger.debug(f"ğŸ”® 4ë‹¨ê³„ ì¤€ë¹„: {len(news_list)}ê°œ ë‰´ìŠ¤ì— ì¢…ëª© ì •ë³´ í†µí•© ì™„ë£Œ")
            
            # ğŸ”¥ ì¤‘ìš”: ëª¨ë“  ë‰´ìŠ¤ë¥¼ daily_news ChromaDBì— ì €ì¥ (í•„í„°ë§ ì—†ì´)
            logger.info(f"ğŸ“š ëª¨ë“  ë‰´ìŠ¤ë¥¼ daily_news ChromaDBì— ì €ì¥ ì‹œì‘: {len(news_list)}ê°œ ë‰´ìŠ¤")
            successful_saves = 0
            failed_saves = 0
            
            for i, news_item in enumerate(news_list, 1):
                try:
                    logger.debug(f"ğŸ“š [{i}/{len(news_list)}] daily_news ì €ì¥ ì‹œë„: {news_item['title'][:50]}...")
                    
                    daily_news_data = {
                        "stock_code": news_item["stock_code"],
                        "stock_name": stock_name,
                        "title": news_item["title"],
                        "content": news_item.get("content", ""),
                        "url": news_item["url"],
                        "publication_date": news_item["published_at"]
                    }
                    
                    # ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬
                    if not daily_news_data["title"].strip():
                        logger.warning(f"âš ï¸ [{i}] ì œëª©ì´ ë¹„ì–´ìˆëŠ” ë‰´ìŠ¤ ê±´ë„ˆëœ€")
                        failed_saves += 1
                        continue
                    
                    if not daily_news_data["stock_code"].strip():
                        logger.warning(f"âš ï¸ [{i}] ì¢…ëª©ì½”ë“œê°€ ë¹„ì–´ìˆëŠ” ë‰´ìŠ¤ ê±´ë„ˆëœ€")
                        failed_saves += 1
                        continue
                    
                    # ëª¨ë“  ë‰´ìŠ¤ë¥¼ daily_newsì— ì €ì¥ (í•„í„°ë§ ì—†ì´)
                    result_id = self.vector_db.add_daily_news(daily_news_data)
                    
                    if result_id:
                        successful_saves += 1
                        logger.debug(f"âœ… [{i}] daily_news ì €ì¥ ì„±ê³µ: {result_id}")
                    else:
                        failed_saves += 1
                        logger.error(f"âŒ [{i}] daily_news ì €ì¥ ê²°ê³¼ ID ì—†ìŒ")
                    
                except Exception as e:
                    failed_saves += 1
                    logger.error(f"âŒ [{i}] daily_news ì €ì¥ ì‹¤íŒ¨: {e}")
                    logger.error(f"âŒ ì‹¤íŒ¨í•œ ë‰´ìŠ¤ ë°ì´í„°: {news_item.get('title', 'NO_TITLE')[:50]}")
                    import traceback
                    logger.error(f"âŒ ìƒì„¸ ì—ëŸ¬: {traceback.format_exc()}")
                    continue
            
            # ì €ì¥ ê²°ê³¼ ìš”ì•½
            logger.info(f"ğŸ“š daily_news ChromaDB ì €ì¥ ì™„ë£Œ:")
            logger.info(f"  âœ… ì„±ê³µ: {successful_saves}ê°œ")
            logger.info(f"  âŒ ì‹¤íŒ¨: {failed_saves}ê°œ")
            logger.info(f"  ğŸ“Š ì´ê³„: {len(news_list)}ê°œ (ì„±ê³µë¥ : {successful_saves/len(news_list)*100:.1f}%)")
            
            # ChromaDB ì €ì¥ ê²€ì¦ (ì‹¤ì œë¡œ ì €ì¥ë˜ì—ˆëŠ”ì§€ í™•ì¸)
            try:
                verification_stats = self.vector_db.get_collection_stats()
                daily_news_count = verification_stats.get("daily_news", {}).get("count", 0)
                logger.info(f"ğŸ” ChromaDB ê²€ì¦: daily_news ì»¬ë ‰ì…˜ì— ì‹¤ì œ {daily_news_count}ê°œ ë¬¸ì„œ ì¡´ì¬")
            except Exception as e:
                logger.error(f"âŒ ChromaDB ê²€ì¦ ì‹¤íŒ¨: {e}")
            
            # ë‰´ìŠ¤ ì²˜ë¦¬ íƒœìŠ¤í¬ ìƒì„± (í•„í„°ë§ ë° ë¶„ì„ìš©)
            logger.debug(f"ğŸ”„ ë‰´ìŠ¤ ì²˜ë¦¬ íƒœìŠ¤í¬ ìƒì„± ì¤‘...")
            tasks = []
            for news_item in news_list:
                task = asyncio.create_task(self.process_news_item(news_item))
                tasks.append(task)

            if tasks:
                logger.debug(f"ï¿½ï¿½ {len(tasks)}ê°œ ë‰´ìŠ¤ ì²˜ë¦¬ íƒœìŠ¤í¬ ì‹¤í–‰ ì¤‘...")
                await asyncio.gather(*tasks, return_exceptions=True)
                logger.debug(f"âœ… ëª¨ë“  ë‰´ìŠ¤ ì²˜ë¦¬ íƒœìŠ¤í¬ ì™„ë£Œ")

            logger.info(f"âœ… ì¢…ëª© {stock_code} ë‰´ìŠ¤ í¬ë¡¤ë§ ì™„ë£Œ: {len(news_list)}ê°œ ({stock_name})")

        except Exception as e:
            logger.error(f"âŒ ì¢…ëª© {stock_code} ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            log_error_with_traceback("ì¢…ëª©ë³„ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨", e, {"stock_code": stock_code})

    def clear_session_cache(self):
        """ì„¸ì…˜ ìºì‹œ ì •ë¦¬"""
        try:
            with self.cache_lock:
                cache_size = len(self.news_cache)
                self.news_cache.clear()
                logger.info(f"ì„¸ì…˜ ìºì‹œ ì •ë¦¬ ì™„ë£Œ: {cache_size}ê°œ í•­ëª©")
        except Exception as e:
            logger.error(f"ì„¸ì…˜ ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")

    async def crawl_all_news(self):
        """ëª¨ë“  ì¢…ëª© ë‰´ìŠ¤ í¬ë¡¤ë§"""
        try:
            logger.info("ì „ì²´ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘")
            
            # ì„¸ì…˜ ì‹œì‘ ì‹œ ìºì‹œ ì´ˆê¸°í™”
            self.clear_session_cache()

            for stock_code in self.stock_codes:
                await self.crawl_news_for_stock(stock_code)

            logger.info("ì „ì²´ ë‰´ìŠ¤ í¬ë¡¤ë§ ì™„ë£Œ")

            # --- [ì¶”ê°€] ê³ ì˜í–¥ ë‰´ìŠ¤ê°€ í•˜ë‚˜ë„ ì—†ì„ ë•Œ ì„ì˜ ë‰´ìŠ¤ í…”ë ˆê·¸ë¨ ì „ì†¡ ---
            stats = self.calculate_daily_stats()
            if stats["high_impact_news"] == 0 and stats["total_news"] > 0:
                logger.info("âš ï¸ ê³ ì˜í–¥ ë‰´ìŠ¤ê°€ ì—†ì–´ ì„ì˜ ë‰´ìŠ¤ë¡œ í…”ë ˆê·¸ë¨ í…ŒìŠ¤íŠ¸ ì „ì†¡ ì‹œì‘")
                
                # CSVì—ì„œ ì„ì˜ ë‰´ìŠ¤ 1ê°œ ì„ íƒ
                if self.daily_csv_file.exists():
                    with open(self.daily_csv_file, 'r', encoding='utf-8') as f:
                        reader = csv.DictReader(f)
                        rows = list(reader)
                        if rows:
                            row = rows[0]  # ì²« ë²ˆì§¸ ë‰´ìŠ¤
                            logger.info(f"ğŸ“° ì„ì˜ ë‰´ìŠ¤ ì„ íƒ: {row.get('title', 'Unknown')[:50]}...")
                            
                            # dict â†’ news_item ë³€í™˜
                            news_item = dict(row)
                            
                            # í•„ë“œ íƒ€ì… ë³´ì •
                            news_item["impact_score"] = float(news_item.get("impact_score", 0.0))
                            news_item["published_at"] = news_item.get("published_at", datetime.now())
                            
                            # stock_info ë³´ì¶©
                            if "stock_info" not in news_item or not news_item["stock_info"]:
                                stock_info = self.get_stock_info_for_code(news_item["stock_code"])
                                news_item["stock_info"] = stock_info
                                logger.info(f"ğŸ“Š ì¢…ëª© ì •ë³´ ë³´ì¶©: {stock_info.get('ì¢…ëª©ëª…', 'Unknown')}")
                            
                            # í…”ë ˆê·¸ë¨ ì „ì†¡
                            await self.send_alert(news_item, news_item["impact_score"], news_item.get("reasoning", "ì„ì˜ ë‰´ìŠ¤ í…ŒìŠ¤íŠ¸"))
                            logger.info("âœ… ì„ì˜ ë‰´ìŠ¤ í…”ë ˆê·¸ë¨ ì „ì†¡ ì™„ë£Œ")

                            # --- ê³¼ê±° ìœ ì‚¬ ì‚¬ë¡€ë„ ì„ì˜ 1ê°œ ì „ì†¡ ---
                            logger.info("ğŸ” ê³¼ê±° ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰ ì‹œì‘...")
                            similar_cases = self.search_similar_historical_cases(news_item)
                            if similar_cases:
                                case = similar_cases[0]
                                logger.info(f"ğŸ“š ê³¼ê±° ì‚¬ë¡€ ì„ íƒ: {case.get('title', 'Unknown')[:50]}...")
                                
                                # ê³¼ê±° ì‚¬ë¡€ í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ í¬ë§·
                                msg_parts = []
                                msg_parts.append("ğŸ•°ï¸ <b>ê³¼ê±° ìœ ì‚¬ ì‚¬ë¡€ í…ŒìŠ¤íŠ¸</b>")
                                msg_parts.append("")
                                msg_parts.append(f"ğŸ“° <b>í˜„ì¬ ë‰´ìŠ¤</b>")
                                msg_parts.append(f"â€¢ ì œëª©: {news_item.get('title', 'Unknown')}")
                                msg_parts.append(f"â€¢ ì¢…ëª©: {news_item.get('stock_name', news_item.get('stock_code', 'Unknown'))}")
                                msg_parts.append("")
                                msg_parts.append(f"ğŸ“š <b>ê³¼ê±° ìœ ì‚¬ ì‚¬ë¡€</b>")
                                msg_parts.append(f"â€¢ ì œëª©: {case.get('title', 'Unknown')}")
                                msg_parts.append(f"â€¢ ë‚ ì§œ: {case.get('published_date', 'Unknown')}")
                                msg_parts.append(f"â€¢ ìœ ì‚¬ë„: {case.get('similarity_score', 0.0):.3f}")
                                if case.get('summary'):
                                    msg_parts.append(f"â€¢ ìš”ì•½: {case.get('summary', '')}")
                                msg_parts.append("")
                                msg_parts.append("ğŸ” <b>RAG ê²€ìƒ‰ ê²°ê³¼</b>")
                                msg_parts.append(f"â€¢ ê²€ìƒ‰ëœ ì‚¬ë¡€ ìˆ˜: {len(similar_cases)}ê°œ")
                                msg_parts.append(f"â€¢ í¬ë¡œë§ˆDB í†µì‹ : ì„±ê³µ")
                                msg_parts.append("")
                                msg_parts.append("âš ï¸ <i>ì´ ë¶„ì„ì€ ì°¸ê³ ìš©ì´ë©°, íˆ¬ì ê²°ì •ì€ ì‹ ì¤‘í•˜ê²Œ í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.</i>")
                                
                                msg = "\n".join(msg_parts)
                                await self.telegram_bot.send_message_async(msg, parse_mode="HTML")
                                
                                # ìµœê·¼ ì•ŒëŒ ë©”ì‹œì§€ ì €ì¥
                                await save_latest_signal(msg)
                                logger.info("âœ… ì„ì˜ ê³¼ê±° ì‚¬ë¡€ í…”ë ˆê·¸ë¨ ì „ì†¡ ì™„ë£Œ")
                            else:
                                logger.warning("âš ï¸ ê³¼ê±° ìœ ì‚¬ ì‚¬ë¡€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                        else:
                            logger.warning("âš ï¸ CSV íŒŒì¼ì— ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŒ")
                else:
                    logger.warning("âš ï¸ ì¼ì¼ CSV íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ")

        except Exception as e:
            logger.error(f"ì „ì²´ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        finally:
            self.close_driver()

    async def process_news_item(self, news_item: Dict):
        """ë‰´ìŠ¤ ì•„ì´í…œ ì²˜ë¦¬ (ì™„ì „ í†µí•© ë²„ì „)"""
        start_time = time.time()
        telegram_sent = False
        filter_stage = "ì²˜ë¦¬ ì‹œì‘"
        
        try:
            # 1. 3ë‹¨ê³„ í•„í„°ë§
            is_filtered, filter_reason, filter_result = await self.check_three_stage_filtering(news_item)
            if is_filtered:
                filter_stage = f"í•„í„°ë§ë¨: {filter_reason}"
                processing_time = time.time() - start_time
                
                # í•„í„°ë§ëœ ë‰´ìŠ¤ë„ CSVì— ê¸°ë¡ (í†µê³„ìš©)
                self.save_to_csv(news_item, 0.0, filter_reason, filter_stage, processing_time, False)
                
                logger.info(f"í•„í„°ë§ëœ ë‰´ìŠ¤ ì œì™¸: {news_item['title']} (ì‚¬ìœ : {filter_reason})")
                return

            filter_stage = "3ë‹¨ê³„ í•„í„°ë§ í†µê³¼"

            # 2. ì˜í–¥ë ¥ í‰ê°€
            impact_score, reasoning = await self.evaluate_impact_with_rag(news_item)
            filter_stage = f"ì˜í–¥ë ¥ í‰ê°€ ì™„ë£Œ ({impact_score:.2f})"

            # 3. ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
            self.save_news_to_db(news_item, impact_score, reasoning)

            # 4. ë²¡í„° DB ì €ì¥
            self.save_to_vector_db(news_item, impact_score)

            # 5. ê³ ì˜í–¥ ë‰´ìŠ¤ ì•Œë¦¼
            if impact_score >= self.impact_threshold:
                # ê³ ì˜í–¥ ë‰´ìŠ¤ ìƒì„¸ ë¡œê¹…
                self.log_high_impact_news_details(news_item, impact_score, reasoning)
                
                # í…”ë ˆê·¸ë¨ ì•Œë¦¼ ì „ì†¡
                await self.send_alert(news_item, impact_score, reasoning)
                telegram_sent = True
                
                # ê³ ì˜í–¥ ë‰´ìŠ¤ ì „ìš© CSV ì €ì¥
                processing_time_temp = time.time() - start_time
                self.save_high_impact_news_to_csv(news_item, impact_score, reasoning, processing_time_temp)
                
                filter_stage = f"ê³ ì˜í–¥ ë‰´ìŠ¤ ì•Œë¦¼ ì „ì†¡ ({impact_score:.2f})"
                logger.info(f"ğŸš¨ ê³ ì˜í–¥ ë‰´ìŠ¤ ì•Œë¦¼ ë°œì†¡: {news_item['title']} (ì ìˆ˜: {impact_score:.2f})")

            # 6. ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
            processing_time = time.time() - start_time

            # 7. CSV ì €ì¥ (ìµœì¢… ê²°ê³¼)
            self.save_to_csv(news_item, impact_score, reasoning, filter_stage, processing_time, telegram_sent)

            logger.info(f"âœ… ë‰´ìŠ¤ ì²˜ë¦¬ ì™„ë£Œ: {news_item['title'][:50]}... (ì˜í–¥ë„: {impact_score:.2f}, ì²˜ë¦¬ì‹œê°„: {processing_time:.2f}s)")

        except Exception as e:
            processing_time = time.time() - start_time
            error_message = f"ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}"
            
            # ì˜¤ë¥˜ë„ CSVì— ê¸°ë¡
            self.save_to_csv(news_item, 0.0, error_message, "ì²˜ë¦¬ ì˜¤ë¥˜", processing_time, False)
            
            logger.error(f"âŒ ë‰´ìŠ¤ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            log_error_with_traceback("ë‰´ìŠ¤ ì²˜ë¦¬ ì‹¤íŒ¨", e, {
                "title": news_item.get("title", "Unknown"),
                "stock_code": news_item.get("stock_code", "Unknown"),
                "url": news_item.get("url", "Unknown")
            })

    def init_daily_csv(self):
        """ì¼ì¼ CSV íŒŒì¼ ì´ˆê¸°í™”"""
        try:
            # íŒŒì¼ì´ ì—†ìœ¼ë©´ í—¤ë” ìƒì„±
            if not self.daily_csv_file.exists():
                with open(self.daily_csv_file, 'w', newline='', encoding='utf-8') as f:
                    writer = csv.writer(f)
                    writer.writerow(self.csv_headers)
                logger.info(f"âœ… ì¼ì¼ CSV íŒŒì¼ ìƒì„±: {self.daily_csv_file}")
        except Exception as e:
            logger.error(f"âŒ CSV íŒŒì¼ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def save_to_csv(self, news_item: Dict, impact_score: float, reasoning: str, 
                   filter_stage: str, processing_time: float, telegram_sent: bool = False):
        """í•„í„°ë§ ê²°ê³¼ë¥¼ CSV íŒŒì¼ì— ì €ì¥"""
        try:
            stock_code = news_item["stock_code"]
            stock_name = self.stock_names.get(stock_code, stock_code)
            stock_info = news_item.get("stock_info", {})
            
            # CSV í–‰ ë°ì´í„° êµ¬ì„±
            row_data = [
                datetime.now().strftime('%Y-%m-%d %H:%M:%S'),  # timestamp
                stock_code,
                stock_name,
                news_item["title"],
                news_item["content"][:500] + "..." if len(news_item["content"]) > 500 else news_item["content"],
                news_item["url"],
                news_item.get("source", ""),
                news_item["published_at"].strftime('%Y-%m-%d %H:%M:%S') if isinstance(news_item["published_at"], datetime) else news_item["published_at"],
                f"{impact_score:.3f}",
                reasoning[:200] + "..." if len(reasoning) > 200 else reasoning,
                f"{news_item.get('relevance_score', 0.0):.3f}",
                f"{news_item.get('similarity_score', 0.0):.3f}",
                filter_stage,
                stock_info.get("í˜„ì¬ê°€", "N/A"),
                stock_info.get("ë“±ë½ë¥ ", "N/A"),
                "Yes" if telegram_sent else "No",
                f"{processing_time:.2f}s"
            ]
            
            # CSV íŒŒì¼ì— ì €ì¥
            with open(self.daily_csv_file, 'a', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow(row_data)
            
            logger.debug(f"ğŸ“Š CSV ì €ì¥ ì™„ë£Œ: {stock_name} - {news_item['title'][:30]}...")
            
        except Exception as e:
            logger.error(f"âŒ CSV ì €ì¥ ì‹¤íŒ¨: {e}")
            log_error_with_traceback("CSV ì €ì¥ ì‹¤íŒ¨", e, {
                "stock_code": news_item.get("stock_code", ""),
                "title": news_item.get("title", "")[:50]
            })
    
    def save_high_impact_news_to_csv(self, news_item: Dict, impact_score: float, reasoning: str, processing_time: float):
        """ê³ ì˜í–¥ ë‰´ìŠ¤ ì „ìš© CSV ì €ì¥"""
        try:
            # ê³ ì˜í–¥ ë‰´ìŠ¤ ì „ìš© CSV íŒŒì¼ ê²½ë¡œ
            high_impact_csv_file = self.csv_output_dir / f"high_impact_news_{datetime.now().strftime('%Y%m%d')}.csv"
            
            # íŒŒì¼ì´ ì—†ìœ¼ë©´ í—¤ë” ìƒì„±
            if not high_impact_csv_file.exists():
                with open(high_impact_csv_file, 'w', newline='', encoding='utf-8') as f:
                    writer = csv.writer(f)
                    writer.writerow([
                        "alert_time", "stock_code", "stock_name", "title", "content_preview", 
                        "url", "source", "published_at", "impact_score", "reasoning", 
                        "current_price", "change_rate", "telegram_success", "processing_time"
                    ])
            
            stock_code = news_item["stock_code"]
            stock_name = self.stock_names.get(stock_code, stock_code)
            stock_info = news_item.get("stock_info", {})
            
            # ê³ ì˜í–¥ ë‰´ìŠ¤ CSV í–‰ ë°ì´í„°
            high_impact_row = [
                datetime.now().strftime('%Y-%m-%d %H:%M:%S'),  # alert_time
                stock_code,
                stock_name,
                news_item["title"],
                news_item["content"][:200] + "..." if len(news_item["content"]) > 200 else news_item["content"],
                news_item["url"],
                news_item.get("source", ""),
                news_item["published_at"].strftime('%Y-%m-%d %H:%M:%S') if isinstance(news_item["published_at"], datetime) else news_item["published_at"],
                f"{impact_score:.3f}",
                reasoning[:300],  # ë” ìì„¸í•œ ë¶„ì„ ë‚´ìš©
                stock_info.get("í˜„ì¬ê°€", "N/A"),
                stock_info.get("ë“±ë½ë¥ ", "N/A"),
                "Yes",  # ê³ ì˜í–¥ ë‰´ìŠ¤ëŠ” í•­ìƒ í…”ë ˆê·¸ë¨ ë°œì†¡ ì‹œë„
                f"{processing_time:.2f}s"
            ]
            
            # ê³ ì˜í–¥ ë‰´ìŠ¤ CSVì— ì €ì¥
            with open(high_impact_csv_file, 'a', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow(high_impact_row)
            
            logger.info(f"ğŸš¨ ê³ ì˜í–¥ ë‰´ìŠ¤ CSV ì €ì¥: {stock_name} - {news_item['title'][:30]}...")
            
        except Exception as e:
            logger.error(f"âŒ ê³ ì˜í–¥ ë‰´ìŠ¤ CSV ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def log_high_impact_news_details(self, news_item: Dict, impact_score: float, reasoning: str):
        """ê³ ì˜í–¥ ë‰´ìŠ¤ ìƒì„¸ ë¡œê¹…"""
        try:
            stock_code = news_item["stock_code"]
            stock_name = self.stock_names.get(stock_code, stock_code)
            stock_info = news_item.get("stock_info", {})
            
            # êµ¬ë¶„ì„ ìœ¼ë¡œ ëˆˆì— ë„ê²Œ ë¡œê¹…
            logger.info("=" * 80)
            logger.info("ğŸš¨ğŸ”¥ ê³ ì˜í–¥ ë‰´ìŠ¤ ìƒì„¸ ë¶„ì„ ğŸ”¥ğŸš¨")
            logger.info("=" * 80)
            logger.info(f"ğŸ“Š ì¢…ëª© ì •ë³´:")
            logger.info(f"   â€¢ ì¢…ëª©ëª…: {stock_name} ({stock_code})")
            logger.info(f"   â€¢ í˜„ì¬ê°€: {stock_info.get('í˜„ì¬ê°€', 'N/A')}")
            logger.info(f"   â€¢ ë“±ë½ë¥ : {stock_info.get('ë“±ë½ë¥ ', 'N/A')}")
            logger.info(f"   â€¢ ê±°ë˜ëŸ‰: {stock_info.get('ê±°ë˜ëŸ‰', 'N/A')}")
            logger.info(f"ğŸ“° ë‰´ìŠ¤ ì •ë³´:")
            logger.info(f"   â€¢ ì œëª©: {news_item['title']}")
            logger.info(f"   â€¢ ë°œí–‰ì‹œê°„: {news_item['published_at']}")
            logger.info(f"   â€¢ ì¶œì²˜: {news_item.get('source', 'N/A')}")
            logger.info(f"   â€¢ URL: {news_item['url']}")
            logger.info(f"ğŸ” ì˜í–¥ë„ ë¶„ì„:")
            logger.info(f"   â€¢ ì˜í–¥ë„ ì ìˆ˜: {impact_score:.3f}/1.0")
            logger.info(f"   â€¢ ë¶„ì„ ê²°ê³¼: {reasoning}")
            logger.info(f"ğŸ“± ì•Œë¦¼ ì •ë³´:")
            logger.info(f"   â€¢ í…”ë ˆê·¸ë¨ ë°œì†¡: ì˜ˆì •")
            logger.info(f"   â€¢ í¬ë¡œë§ˆ DB ì €ì¥: ì˜ˆì •")
            logger.info(f"   â€¢ ë¶„ì„ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            logger.info("=" * 80)
            
        except Exception as e:
            logger.error(f"âŒ ê³ ì˜í–¥ ë‰´ìŠ¤ ìƒì„¸ ë¡œê¹… ì‹¤íŒ¨: {e}")
    
    def create_summary_csv(self) -> str:
        """ì¼ì¼ ìš”ì•½ CSV ìƒì„±"""
        try:
            summary_file = self.csv_output_dir / f"daily_summary_{datetime.now().strftime('%Y%m%d')}.csv"
            
            # í†µê³„ ê³„ì‚°
            stats = self.calculate_daily_stats()
            
            # ìš”ì•½ CSV ìƒì„±
            with open(summary_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                
                # í—¤ë”
                writer.writerow(["êµ¬ë¶„", "ê°’"])
                
                # í†µê³„ ë°ì´í„°
                writer.writerow(["ì „ì²´ ë‰´ìŠ¤ ìˆ˜", stats["total_news"]])
                writer.writerow(["ê³ ì˜í–¥ ë‰´ìŠ¤ ìˆ˜", stats["high_impact_news"]])
                writer.writerow(["í…”ë ˆê·¸ë¨ ì•Œë¦¼ ìˆ˜", stats["telegram_alerts"]])
                writer.writerow(["í‰ê·  ì˜í–¥ë„", f"{stats['avg_impact']:.3f}"])
                writer.writerow(["ì²˜ë¦¬ ì‹œê°„ (ì´ˆ)", f"{stats['total_processing_time']:.2f}"])
                writer.writerow(["ìƒì„± ì‹œê°„", datetime.now().strftime('%Y-%m-%d %H:%M:%S')])
                
                # ì¢…ëª©ë³„ í†µê³„
                writer.writerow([])
                writer.writerow(["ì¢…ëª©ë³„ ë‰´ìŠ¤ ìˆ˜"])
                writer.writerow(["ì¢…ëª©ì½”ë“œ", "ì¢…ëª©ëª…", "ë‰´ìŠ¤ìˆ˜", "í‰ê· ì˜í–¥ë„"])
                
                for stock_code, stock_stats in stats["by_stock"].items():
                    stock_name = self.stock_names.get(stock_code, stock_code)
                    writer.writerow([
                        stock_code, 
                        stock_name, 
                        stock_stats["count"], 
                        f"{stock_stats['avg_impact']:.3f}"
                    ])
            
            logger.info(f"âœ… ì¼ì¼ ìš”ì•½ CSV ìƒì„± ì™„ë£Œ: {summary_file}")
            return str(summary_file)
            
        except Exception as e:
            logger.error(f"âŒ ìš”ì•½ CSV ìƒì„± ì‹¤íŒ¨: {e}")
            return ""
    
    def calculate_daily_stats(self) -> Dict:
        """ì¼ì¼ í†µê³„ ê³„ì‚°"""
        try:
            if not self.daily_csv_file.exists():
                return {
                    "total_news": 0,
                    "high_impact_news": 0,
                    "telegram_alerts": 0,
                    "avg_impact": 0.0,
                    "total_processing_time": 0.0,
                    "by_stock": {}
                }
            
            stats = {
                "total_news": 0,
                "high_impact_news": 0,
                "telegram_alerts": 0,
                "total_impact": 0.0,
                "total_processing_time": 0.0,
                "by_stock": {}
            }
            
            with open(self.daily_csv_file, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                
                for row in reader:
                    stats["total_news"] += 1
                    
                    # ì˜í–¥ë„ í†µê³„
                    impact_score = float(row["impact_score"])
                    stats["total_impact"] += impact_score
                    
                    if impact_score >= 0.7:
                        stats["high_impact_news"] += 1
                    
                    # í…”ë ˆê·¸ë¨ ì•Œë¦¼ í†µê³„
                    if row["telegram_sent"] == "Yes":
                        stats["telegram_alerts"] += 1
                    
                    # ì²˜ë¦¬ ì‹œê°„ í†µê³„
                    processing_time = float(row["processing_time"].replace('s', ''))
                    stats["total_processing_time"] += processing_time
                    
                    # ì¢…ëª©ë³„ í†µê³„
                    stock_code = row["stock_code"]
                    if stock_code not in stats["by_stock"]:
                        stats["by_stock"][stock_code] = {
                            "count": 0,
                            "total_impact": 0.0
                        }
                    
                    stats["by_stock"][stock_code]["count"] += 1
                    stats["by_stock"][stock_code]["total_impact"] += impact_score
            
            # í‰ê·  ê³„ì‚°
            stats["avg_impact"] = stats["total_impact"] / stats["total_news"] if stats["total_news"] > 0 else 0.0
            
            # ì¢…ëª©ë³„ í‰ê·  ê³„ì‚°
            for stock_code, stock_stats in stats["by_stock"].items():
                stock_stats["avg_impact"] = stock_stats["total_impact"] / stock_stats["count"]
            
            return stats
            
        except Exception as e:
            logger.error(f"âŒ ì¼ì¼ í†µê³„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {
                "total_news": 0,
                "high_impact_news": 0,
                "telegram_alerts": 0,
                "avg_impact": 0.0,
                "total_processing_time": 0.0,
                "by_stock": {}
            }


def get_news_service():
    """ë‰´ìŠ¤ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì§€ì—° ì´ˆê¸°í™”)"""
    global news_service_instance
    if news_service_instance is None:
        try:
            news_service_instance = NewsService()
            return news_service_instance
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    return news_service_instance

async def save_latest_signal(message: str):
    """ìµœê·¼ ì•ŒëŒ ë©”ì‹œì§€ ì €ì¥"""
    global latest_signal_message
    latest_signal_message = {
        "message": message,
        "timestamp": datetime.now().isoformat(),
        "service": "news"
    }

# === FastAPI ì—”ë“œí¬ì¸íŠ¸ ===

@app.get("/health")
async def health_check():
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}

@app.get("/signal")
async def get_latest_signal():
    """ìµœê·¼ ì•ŒëŒ ë©”ì‹œì§€ ì¡°íšŒ"""
    global latest_signal_message
    if latest_signal_message:
        return latest_signal_message
    else:
        return {
            "message": "ì•„ì§ ì•ŒëŒì´ ë°œìƒí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
            "timestamp": datetime.now().isoformat(),
            "service": "news"
        }

def is_peak_hours(current_time: datetime) -> bool:
    """ë‰´ìŠ¤ ì„œë¹„ìŠ¤ í”¼í¬ ì‹œê°„ëŒ€ í™•ì¸ (7:30-9:30, 14:30-16:30)"""
    time_only = current_time.time()
    
    # ì˜¤ì „ í”¼í¬: 7:30-9:30
    morning_start = datetime.strptime("07:30", "%H:%M").time()
    morning_end = datetime.strptime("09:30", "%H:%M").time()
    
    # ì˜¤í›„ í”¼í¬: 14:30-16:30  
    afternoon_start = datetime.strptime("14:30", "%H:%M").time()
    afternoon_end = datetime.strptime("16:30", "%H:%M").time()
    
    return (morning_start <= time_only <= morning_end) or (afternoon_start <= time_only <= afternoon_end)

def should_execute_now() -> Tuple[bool, str]:
    """í˜„ì¬ ì‹¤í–‰í•  ì‹œê°„ì¸ì§€ íŒë‹¨"""
    global last_execution_time
    
    now = datetime.now()
    
    # ì²« ì‹¤í–‰ì¸ ê²½ìš°
    if last_execution_time is None:
        return True, "ì²« ì‹¤í–‰"
    
    # ë§ˆì§€ë§‰ ì‹¤í–‰ìœ¼ë¡œë¶€í„° ê²½ê³¼ ì‹œê°„ ê³„ì‚°
    time_diff = (now - last_execution_time).total_seconds()
    
    # ì‹œê°„ëŒ€ë³„ ì‹¤í–‰ ê°„ê²© í™•ì¸
    if is_peak_hours(now):
        # í”¼í¬ ì‹œê°„ëŒ€: 10ë¶„ ê°„ê²©
        required_interval = 600  # 10ë¶„ = 600ì´ˆ
        interval_name = "í”¼í¬ ì‹œê°„ëŒ€ 10ë¶„ ê°„ê²©"
    else:
        # ì¼ë°˜ ì‹œê°„ëŒ€: 1ì‹œê°„ ê°„ê²©
        required_interval = 3600  # 1ì‹œê°„ = 3600ì´ˆ
        interval_name = "ì¼ë°˜ ì‹œê°„ëŒ€ 1ì‹œê°„ ê°„ê²©"
    
    if time_diff >= required_interval:
        return True, f"{interval_name} - ë§ˆì§€ë§‰ ì‹¤í–‰: {last_execution_time.strftime('%H:%M')}"
    else:
        remaining = int(required_interval - time_diff)
        return False, f"{interval_name} - {remaining}ì´ˆ í›„ ì‹¤í–‰ ê°€ëŠ¥"

async def execute_news_crawling() -> Dict:
    """ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤í–‰"""
    global last_execution_time
    try:
        logger.info("ğŸš€ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤í–‰ ì‹œì‘")
        
        # ë‰´ìŠ¤ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ í™•ì¸ ë° ì´ˆê¸°í™”
        global news_service_instance
        if news_service_instance is None:
            logger.warning("âš ï¸ ë‰´ìŠ¤ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ê°€ ì—†ìŒ - ì§€ì—° ì´ˆê¸°í™” ì‹œë„")
            try:
                news_service_instance = get_news_service()
                logger.info("âœ… ë‰´ìŠ¤ ì„œë¹„ìŠ¤ ì§€ì—° ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                logger.error(f"âŒ ë‰´ìŠ¤ ì„œë¹„ìŠ¤ ì§€ì—° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                return {"success": False, "error": f"ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}"}
        
        # ëª¨ë“  ì¢…ëª©ì— ëŒ€í•´ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤í–‰
        total_news = 0
        processed_stocks = []
        
        for stock_code in news_service_instance.stock_codes:
            try:
                # ì¢…ëª©ë³„ ë‰´ìŠ¤ í¬ë¡¤ë§
                logger.info(f"ğŸ“° {stock_code} ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘")
                news_list = news_service_instance.crawl_naver_finance_news(stock_code, pages=1, size=3)
        
                if news_list:
                    # ê° ë‰´ìŠ¤ì— ëŒ€í•´ í•„í„°ë§ ë° ë¶„ì„
                    for news_item in news_list:
                        try:
                            # 3ë‹¨ê³„ í•„í„°ë§
                            is_filtered, filter_reason, filter_result = await news_service_instance.check_three_stage_filtering(news_item)
        
                            if not is_filtered:
                                # RAG ë°©ì‹ ì˜í–¥ë„ í‰ê°€
                                impact_score, reasoning = await news_service_instance.evaluate_impact_with_rag(news_item)
        
                                # ê³ ì˜í–¥ë„ ë‰´ìŠ¤ì¸ ê²½ìš° ì•Œë¦¼ ì „ì†¡
                                if impact_score >= news_service_instance.impact_threshold:
                                    await news_service_instance.send_alert(news_item, impact_score, reasoning)
                                    logger.info(f"ğŸ“¢ ê³ ì˜í–¥ë„ ë‰´ìŠ¤ ì•Œë¦¼ ì „ì†¡: {news_item['title'][:30]}...")
                                
                                total_news += 1
                                
                        except Exception as e:
                            logger.error(f"âŒ ë‰´ìŠ¤ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                            continue
                
                processed_stocks.append(stock_code)
                logger.info(f"âœ… {stock_code} ë‰´ìŠ¤ í¬ë¡¤ë§ ì™„ë£Œ: {len(news_list)}ê°œ")
                
            except Exception as e:
                logger.error(f"âŒ {stock_code} ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                continue
        
        # ì‹¤í–‰ ì‹œê°„ ì—…ë°ì´íŠ¸
        last_execution_time = datetime.now()
        
        result = {
            "success": True,
            "processed_stocks": len(processed_stocks),
            "total_news": total_news,
            "execution_time": last_execution_time.isoformat(),
        }
        
        logger.info(f"âœ… ë‰´ìŠ¤ í¬ë¡¤ë§ ì™„ë£Œ: {len(processed_stocks)}ê°œ ì¢…ëª©, {total_news}ê°œ ë‰´ìŠ¤")
        return result
        
    except Exception as e:
        logger.error(f"âŒ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        return {"success": False, "error": str(e)}

# FastAPI ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€
@app.post("/set-user/{user_id}")
async def set_user_id_endpoint(user_id: str):
    """ì‚¬ìš©ì ID ì„¤ì • ì—”ë“œí¬ì¸íŠ¸"""
    try:
        news_service = get_news_service()
        await news_service.set_user_id(user_id)
        return {
            "success": True,
            "message": f"ì‚¬ìš©ì ID ì„¤ì • ì™„ë£Œ: {user_id}",
            "user_id": user_id
        }
    except Exception as e:
        logger.error(f"âŒ ì‚¬ìš©ì ID ì„¤ì • ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail="ì‚¬ìš©ì ID ì„¤ì •ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")

@app.get("/user-config/{user_id}")
async def get_user_config_endpoint(user_id: str):
    """ì‚¬ìš©ì ì„¤ì • ì¡°íšŒ ì—”ë“œí¬ì¸íŠ¸"""
    try:
        news_service = get_news_service()
        await news_service.set_user_id(user_id)
        
        # ì‚¬ìš©ì ì„¤ì • ì¡°íšŒ
        user_config = await news_service.user_config_manager.get_user_config(user_id)
        
        return {
            "success": True,
            "user_id": user_id,
            "config": {
                "stocks": user_config.get("stocks", []),
                "news_similarity_threshold": user_config.get("news_similarity_threshold"),
                "news_impact_threshold": user_config.get("news_impact_threshold")
            }
        }
    except Exception as e:
        logger.error(f"âŒ ì‚¬ìš©ì ì„¤ì • ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail="ì‚¬ìš©ì ì„¤ì • ì¡°íšŒì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")

@app.post("/execute")
async def execute_news_analysis(request: Request):
    """ë‰´ìŠ¤ ë¶„ì„ ì‹¤í–‰ - ì‚¬ìš©ìë³„ ë™ì  ì²˜ë¦¬"""
    try:
        # Headerì—ì„œ user_id ì¶”ì¶œ (ë¬¸ìì—´ë¡œ ì²˜ë¦¬)
        user_id = request.headers.get("X-User-ID", "1")
        
        # ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ì˜ user_id ë™ì  ì—…ë°ì´íŠ¸
        service = get_news_service()
        if service.current_user_id != user_id:
            await service.set_user_id(user_id)
            logger.info(f"ğŸ”„ ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ë³€ê²½: {user_id}")
        
        # ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤í–‰
        result = await execute_news_crawling()
        return result
        
    except Exception as e:
        logger.error(f"âŒ ë‰´ìŠ¤ ë¶„ì„ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        return {"success": False, "error": str(e)}

@app.post("/check-schedule")
async def check_schedule():
    """ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì²´í¬ ì‹ í˜¸ ìˆ˜ì‹  - ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰ ì‹œê°„ íŒë‹¨"""
    try:
        should_run, reason = should_execute_now()
        
        if should_run:
            # ì‹¤í–‰ ì¡°ê±´ ë§Œì¡± ì‹œ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤í–‰
            result = await execute_news_crawling()
            
            if result["success"]:
                return {
                    "executed": True,
                    "message": f"ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤í–‰ ì™„ë£Œ - {reason}",
                    "details": result
                }
            else:
                return {
                    "executed": False,
                    "message": f"ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤í–‰ ì‹¤íŒ¨ - {result.get('error', 'Unknown')}",
                    "reason": reason
                }
        else:
            return {
                "executed": False,
                "message": reason,
                "next_execution": "ì¡°ê±´ ë§Œì¡± ì‹œ"
        }
        
    except Exception as e:
        logger.error(f"âŒ ìŠ¤ì¼€ì¤„ ì²´í¬ ì‹¤íŒ¨: {e}")
        return {
            "executed": False,
            "message": f"ìŠ¤ì¼€ì¤„ ì²´í¬ ì˜¤ë¥˜: {str(e)}"
        }

    # === ì‚¬ìš©ìë³„ ê°œì¸í™” ê¸°ëŠ¥ ===
    
    async def initialize_user_personalization(self):
        """ì‚¬ìš©ì ê°œì¸í™” ì„¤ì • ì´ˆê¸°í™”"""
        try:
            self.user_config_loader = await get_config_loader()
            logger.info("âœ… ì‚¬ìš©ì ê°œì¸í™” ë¡œë” ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ ì‚¬ìš©ì ê°œì¸í™” ë¡œë” ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.user_config_loader = None

    async def get_personalized_config(self, user_id: str) -> Dict[str, Any]:
        """ì‚¬ìš©ìë³„ ê°œì¸í™” ì„¤ì • ì¡°íšŒ"""
        try:
            if not self.user_config_loader:
                logger.warning("âš ï¸ ì‚¬ìš©ì ì„¤ì • ë¡œë”ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ - ê¸°ë³¸ê°’ ì‚¬ìš©")
                return self._get_default_config()
            
            # ìºì‹œì—ì„œ ë¨¼ì € í™•ì¸
            if user_id in self.personalized_configs:
                return self.personalized_configs[user_id]
            
            # APIë¥¼ í†µí•´ ì‚¬ìš©ì ì„¤ì • ë¡œë“œ
            config = await self.user_config_loader.load_user_config(user_id)
            if config:
                # ë‰´ìŠ¤ ì„œë¹„ìŠ¤ì— íŠ¹í™”ëœ ì„¤ì • ì¶”ì¶œ
                personalized_config = {
                    "user_id": user_id,
                    "stocks": [stock["stock_code"] for stock in config.get("stocks", [])],
                    "model_type": config.get("model_type", "hyperclova"),
                    "news_similarity_threshold": config.get("news_similarity_threshold", 0.7),
                    "news_impact_threshold": config.get("news_impact_threshold", 0.8),
                    "active_service": config.get("active_services", {}).get("news_service", 0) == 1
                }
                
                # ìºì‹œì— ì €ì¥
                self.personalized_configs[user_id] = personalized_config
                logger.info(f"âœ… ì‚¬ìš©ì ê°œì¸í™” ì„¤ì • ë¡œë“œ ì™„ë£Œ: {user_id}")
                return personalized_config
            else:
                logger.warning(f"âš ï¸ ì‚¬ìš©ì ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {user_id} - ê¸°ë³¸ê°’ ì‚¬ìš©")
                return self._get_default_config()
                
        except Exception as e:
            logger.error(f"âŒ ì‚¬ìš©ì ê°œì¸í™” ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {user_id} - {e}")
            return self._get_default_config()

    def _get_default_config(self) -> Dict[str, Any]:
        """ê¸°ë³¸ ì„¤ì • ë°˜í™˜"""
        return {
            "user_id": "default",
            "stocks": ["005930", "000660"],  # ê¸°ë³¸ ì¢…ëª©: ì‚¼ì„±ì „ì, SKí•˜ì´ë‹‰ìŠ¤
            "model_type": "hyperclova",
            "news_similarity_threshold": 0.7,
            "news_impact_threshold": 0.8,
            "active_service": True
        }

    async def should_analyze_for_user(self, user_id: str, stock_code: str) -> bool:
        """íŠ¹ì • ì‚¬ìš©ìì— ëŒ€í•´ í•´ë‹¹ ì¢…ëª©ì„ ë¶„ì„í•´ì•¼ í•˜ëŠ”ì§€ í™•ì¸"""
        try:
            config = await self.get_personalized_config(user_id)
            
            # ì„œë¹„ìŠ¤ê°€ ë¹„í™œì„±í™”ëœ ê²½ìš°
            if not config.get("active_service", True):
                return False
            
            # ì‚¬ìš©ìê°€ ì„ íƒí•œ ì¢…ëª©ì— í¬í•¨ë˜ì§€ ì•Šì€ ê²½ìš°
            user_stocks = config.get("stocks", [])
            if stock_code not in user_stocks:
                return False
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ì‚¬ìš©ìë³„ ë¶„ì„ í•„ìš”ì„± í™•ì¸ ì‹¤íŒ¨: {user_id}, {stock_code} - {e}")
            return True  # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ì ìœ¼ë¡œ ë¶„ì„ ì§„í–‰

    async def get_user_analysis_model(self, user_id: str) -> str:
        """ì‚¬ìš©ìê°€ ì„ íƒí•œ AI ëª¨ë¸ ë°˜í™˜"""
        try:
            config = await self.get_personalized_config(user_id)
            return config.get("model_type", "hyperclova")
        except Exception as e:
            logger.error(f"âŒ ì‚¬ìš©ì AI ëª¨ë¸ ì¡°íšŒ ì‹¤íŒ¨: {user_id} - {e}")
            return "hyperclova"

    def clear_user_cache(self, user_id: Optional[str] = None):
        """ì‚¬ìš©ì ì„¤ì • ìºì‹œ í´ë¦¬ì–´"""
        if user_id:
            self.personalized_configs.pop(user_id, None)
            if self.user_config_loader:
                self.user_config_loader.clear_cache(user_id)
            logger.debug(f"ğŸ§¹ ì‚¬ìš©ì ì„¤ì • ìºì‹œ í´ë¦¬ì–´: {user_id}")
        else:
            self.personalized_configs.clear()
            if self.user_config_loader:
                self.user_config_loader.clear_cache()
            logger.debug("ğŸ§¹ ëª¨ë“  ì‚¬ìš©ì ì„¤ì • ìºì‹œ í´ë¦¬ì–´")

def main():
    """ë©”ì¸ í•¨ìˆ˜ - API ì„œë²„ë§Œ ì‹œì‘, ì‹¤ì œ ì„œë¹„ìŠ¤ ë¡œì§ì€ ìŠ¤ì¼€ì¤„ë§ì— ë”°ë¼ ì‹¤í–‰"""
    try:
        logger.info("ğŸš€ ë‰´ìŠ¤ ì„œë¹„ìŠ¤ API ì„œë²„ ì‹œì‘ (í¬íŠ¸: 8001)")
        logger.info("ğŸ“‹ ìŠ¤ì¼€ì¤„ë§ ì •ì±…:")
        logger.info("   â€¢ í”¼í¬ì‹œê°„(7:30-9:30, 14:30-16:30): 10ë¶„ ê°„ê²©")
        logger.info("   â€¢ ì¼ë°˜ì‹œê°„: 1ì‹œê°„ ê°„ê²©")
        
        # FastAPI ì„œë²„ ì‹¤í–‰
        uvicorn.run(app, host="0.0.0.0", port=8001)
        
    except Exception as e:
        error_type = type(e).__name__
        logger.error(f"ë‰´ìŠ¤ ì„œë¹„ìŠ¤ ì‹œì‘ ì‹¤íŒ¨ [{error_type}]: {e}")
        log_error_with_traceback("ë‰´ìŠ¤ ì„œë¹„ìŠ¤ ì‹œì‘ ì‹¤íŒ¨", e)
        raise


if __name__ == "__main__":
    main()