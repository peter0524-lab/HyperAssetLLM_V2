import asyncio
import time
import re
import os
from datetime import datetime
from io import BytesIO
import aiohttp # For async HTTP requests
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from urllib.parse import unquote, urlparse, parse_qs
from webdriver_manager.chrome import ChromeDriverManager
from unstructured.partition.pdf import partition_pdf

# from unstructured.partition.pdf import partition_pdf  # ì„ì‹œ ì£¼ì„ì²˜ë¦¬
import logging

# Configure logging
logger = logging.getLogger(__name__)

class ResearchCrawler:
    def __init__(self):
        # Initialize any necessary components, e.g., WebDriver options
        self.options = webdriver.ChromeOptions()
        self.options.add_argument("--headless") # Run in headless mode
        self.options.add_argument("--no-sandbox")
        self.options.add_argument("--disable-dev-shm-usage")
        # ChromeDriverManager().install() is a synchronous operation,
        # so it's fine to call it in __init__ or wrap it in to_thread if __init__ itself is async.
        # For simplicity, keeping it here as it's typically a one-time setup.
        self.service = Service(ChromeDriverManager().install())

    @staticmethod
    async def _extract_filtered_korean_elements_first_page_from_url(pdf_url: str) -> tuple[str, list[str]]:
        """
        PDF ë§í¬ì—ì„œ ì²« í˜ì´ì§€ì˜ í•„í„°ë§ëœ í•œê¸€ ìš”ì†Œë¥¼ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.
        """
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
            "Referer": "https://m.stock.naver.com/"
        }

        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(pdf_url, headers=headers, timeout=30) as response:
                    if response.status != 200:
                        logger.error(f"âŒ PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {pdf_url}, ìƒíƒœ ì½”ë“œ: {response.status}")
                        return "", []

                    pdf_content = await response.read()
                    pdf_file = BytesIO(pdf_content)
                    logger.info(f"ğŸ“¥ HTTP ìƒíƒœ ì½”ë“œ: {response.status}")
                    logger.info(f"ğŸ“ ì‘ë‹µ Content-Type: {response.headers.get('Content-Type')}")
                    logger.info(f"ğŸ“„ PDF ë°”ì´íŠ¸ í¬ê¸°: {len(pdf_content)} bytes")

            # ì„ì‹œë¡œ PDF íŒŒì‹± ê¸°ëŠ¥ ë¹„í™œì„±í™”
            elements = await asyncio.to_thread(
                partition_pdf,
                file=pdf_file,
                filename=None,
                languages=["ko"],
                strategy="fast",
                detect_languages=False,
                extract_images_in_pdf=False,
                include_page_breaks=True,
                hi_nr_of_pages=1
            )
            

            extracted_text_elements = []
            korean_pattern = re.compile('[ê°€-í£]+')

            logger.info("--- unstructured 1í˜ì´ì§€ í•„í„°ë§ëœ í•œê¸€ í¬í•¨ ìš”ì†Œ ì¶”ì¶œ ê²°ê³¼ ---")
            if not elements:
                logger.warning("1í˜ì´ì§€ì—ì„œ ì¶”ì¶œëœ ìš”ì†Œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return "", []

            for el in elements:
                if el.category == "PageBreak":
                    break
                if el.category in ["Title", "NarrativeText", "UncategorizedText"] and korean_pattern.search(el.text):
                    extracted_text_elements.append(el.text)

            if extracted_text_elements:
                full_extracted_text = "\n".join(extracted_text_elements)
                logger.info(f"ì¶”ì¶œëœ í…ìŠ¤íŠ¸ (ì¼ë¶€): {full_extracted_text[:200]}...")
            else:
                full_extracted_text = ""
                logger.warning("ì§€ì •ëœ ì¹´í…Œê³ ë¦¬ì—ì„œ í•œê¸€ í¬í•¨ ìš”ì†Œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

            return full_extracted_text, extracted_text_elements
        except aiohttp.ClientError as e:
            logger.error(f"PDF ë‹¤ìš´ë¡œë“œ ì¤‘ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ (aiohttp): {e}")
            return "", []
        except Exception as e:
            logger.error(f"PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return "", []

    async def get_preprocessed_text_for_stock(self, stock_code: str) -> str:
        """
        ì¢…ëª© ì½”ë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì‹  ë¦¬ì„œì¹˜ PDFë¥¼ ë¹„ë™ê¸°ì ìœ¼ë¡œ í¬ë¡¤ë§í•˜ê³  ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        url = f"https://m.stock.naver.com/domestic/stock/{stock_code}/research"
        
        # Selenium operations are synchronous and I/O-bound (interacting with browser).
        # Wrap the entire Selenium block in asyncio.to_thread to make it non-blocking.
        def _sync_selenium_crawl():
            driver = None
            try:
                driver = webdriver.Chrome(service=self.service, options=self.options)
                logger.info(f"ğŸ” URL ì ‘ì†: {url}")
                driver.get(url)

                logger.info("â³ ìš”ì†Œ ë¡œë”© ëŒ€ê¸° ì¤‘...")
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "li.ResearchList_item__suwvv"))
                )
                logger.info("âœ… ìš”ì†Œ ë¡œë”© ì™„ë£Œ!")

                spans = driver.find_elements(By.CLASS_NAME, "Vitem_desc__EbOC4")
                date_regex = re.compile(r"\d{4}\.\d{2}\.\d{2}")
                date_elements = [(span, span.text.strip()) for span in spans if date_regex.fullmatch(span.text.strip())]

                if not date_elements:
                    logger.warning("âŒ ë‚ ì§œ í˜•ì‹ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì§€ ëª»í•¨")
                    return ""

                date_elements.sort(key=lambda x: datetime.strptime(x[1], "%Y.%m.%d"), reverse=True)
                latest_span = date_elements[0][0]
                logger.info(f"âœ… ìµœì‹  ë‚ ì§œ ë¦¬ì„œì¹˜: {latest_span.text}")

                clickable = latest_span.find_element(By.XPATH, "./ancestor::a")
                clickable.click()
                time.sleep(2) # Use time.sleep here as it's within a separate thread
                logger.info("âœ… ìµœì‹  ë¦¬ì„œì¹˜ í´ë¦­ ì™„ë£Œ")

                detail_url = driver.current_url
                logger.info(f"ğŸ“„ ì´ë™í•œ ìƒì„¸ ë¦¬í¬íŠ¸ URL: {detail_url}")

                # ğŸ”— PDF ë§í¬ ì°¾ê¸°
                pdf_button = WebDriverWait(driver, 5).until(
                    EC.element_to_be_clickable((By.CLASS_NAME, "ResearchContent_link__zeiGC"))
                )
                pdf_url = pdf_button.get_attribute("href")
                logger.info(f"ğŸ“ PDF ë‹¤ìš´ë¡œë“œ ë§í¬: {pdf_url}")
                
                # ì§„ì§œ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥í•œ PDF ì›ë³¸ URL ì¶”ì¶œ
                parsed = urlparse(pdf_url)
                query_params = parse_qs(parsed.query)
                real_pdf_url = unquote(query_params.get("url", [""])[0])

                logger.info(f"ğŸ“¥ ì§„ì§œ PDF ì›ë³¸ ë§í¬: {real_pdf_url}")
                return real_pdf_url
            except Exception as e:
                logger.error(f"Selenium í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                return ""
            finally:
                if driver:
                    driver.quit()

        real_pdf_url = await asyncio.to_thread(_sync_selenium_crawl)
        if not real_pdf_url:
            return ""

        # ğŸ§  ì „ì²˜ë¦¬ ì‹¤í–‰ (already async)
        filtered_text, elements = await self._extract_filtered_korean_elements_first_page_from_url(real_pdf_url)

        logger.info(f"\nğŸ”¹ ìµœì¢… ì¶”ì¶œëœ í…ìŠ¤íŠ¸ (ì¼ë¶€) ğŸ”¹\n{filtered_text[50:250]}...")
        logger.info(f"\nì´ ì¶”ì¶œëœ ìš”ì†Œ ìˆ˜: {len(elements)}")

        return filtered_text
