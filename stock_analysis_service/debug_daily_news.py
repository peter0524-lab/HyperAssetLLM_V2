#!/usr/bin/env python3
"""
daily_news ChromaDB ì €ì¥ ë¬¸ì œ ë””ë²„ê¹… ìŠ¤í¬ë¦½íŠ¸
"""

import asyncio
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from shared.database.vector_db import VectorDBClient
from services.news_service.main import NewsService
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

async def debug_daily_news():
    """daily_news ì €ì¥ ë¬¸ì œ ë””ë²„ê¹…"""
    try:
        # 1. ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        logger.info("ğŸ”§ ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”")
        vector_db = VectorDBClient()
        
        # 2. daily_news ì»¬ë ‰ì…˜ í´ë¦°ì—…
        logger.info("ğŸ§¹ daily_news ì»¬ë ‰ì…˜ í´ë¦°ì—… ì‹œì‘")
        cleanup_count = vector_db.cleanup_daily_news()
        logger.info(f"ğŸ§¹ í´ë¦°ì—… ì™„ë£Œ: {cleanup_count}ê°œ ì‚­ì œ")
        
        # 3. í´ë¦°ì—… í›„ ìƒíƒœ í™•ì¸
        stats = vector_db.get_collection_stats()
        daily_news_count = stats.get("daily_news", {}).get("count", 0)
        logger.info(f"ğŸ” í´ë¦°ì—… í›„ daily_news ê°œìˆ˜: {daily_news_count}")
        
        # 4. ë‰´ìŠ¤ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        logger.info("ğŸ“° ë‰´ìŠ¤ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”")
        news_service = NewsService()
        
        # 5. í•œ ì¢…ëª©ë§Œ í…ŒìŠ¤íŠ¸ (006800 - ë¯¸ë˜ì—ì…‹ì¦ê¶Œ)
        test_stock_code = "006800"
        logger.info(f"ğŸ¯ í…ŒìŠ¤íŠ¸ ì¢…ëª©: {test_stock_code}")
        
        # 6. ë‰´ìŠ¤ í¬ë¡¤ë§ ë° ì €ì¥
        logger.info("ğŸš€ ë‰´ìŠ¤ í¬ë¡¤ë§ ë° ì €ì¥ ì‹œì‘")
        await news_service.crawl_news_for_stock(test_stock_code)
        
        # 7. ì €ì¥ í›„ ìƒíƒœ í™•ì¸
        stats_after = vector_db.get_collection_stats()
        daily_news_count_after = stats_after.get("daily_news", {}).get("count", 0)
        logger.info(f"ğŸ” ì €ì¥ í›„ daily_news ê°œìˆ˜: {daily_news_count_after}")
        
        # 8. ì‹¤ì œ ì €ì¥ëœ ë¬¸ì„œ í™•ì¸
        logger.info("ğŸ“‹ ì‹¤ì œ ì €ì¥ëœ ë¬¸ì„œ í™•ì¸")
        documents = vector_db.get_all_documents("daily_news", limit=100)
        
        logger.info(f"ğŸ“Š ì‹¤ì œ ì¡°íšŒëœ ë¬¸ì„œ ìˆ˜: {len(documents)}")
        
        # 9. ë¬¸ì„œ ìƒì„¸ ì •ë³´ ì¶œë ¥
        for i, doc in enumerate(documents[:10], 1):  # ìƒìœ„ 10ê°œë§Œ ì¶œë ¥
            logger.info(f"ğŸ“„ [{i}] ID: {doc.get('id', 'NO_ID')}")
            logger.info(f"    ì œëª©: {doc.get('metadata', {}).get('title', 'NO_TITLE')[:50]}")
            logger.info(f"    ì¢…ëª©: {doc.get('metadata', {}).get('stock_code', 'NO_STOCK')}")
            logger.info(f"    ì‹œê°„: {doc.get('metadata', {}).get('created_at', 'NO_TIME')}")
            logger.info(f"    ---")
        
        # 10. ID ì¤‘ë³µ í™•ì¸
        ids = [doc.get('id') for doc in documents]
        unique_ids = set(ids)
        logger.info(f"ğŸ” ID ì¤‘ë³µ í™•ì¸: ì´ {len(ids)}ê°œ ì¤‘ ê³ ìœ í•œ ID {len(unique_ids)}ê°œ")
        
        if len(ids) != len(unique_ids):
            logger.warning("âš ï¸ ID ì¤‘ë³µ ë°œê²¬!")
            duplicates = [id for id in ids if ids.count(id) > 1]
            logger.warning(f"ì¤‘ë³µ ID: {set(duplicates)}")
        else:
            logger.info("âœ… ID ì¤‘ë³µ ì—†ìŒ")
        
        # 11. ì¢…ë£Œ
        news_service.close_driver()
        logger.info("âœ… ë””ë²„ê¹… ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"âŒ ë””ë²„ê¹… ì‹¤íŒ¨: {e}")
        import traceback
        logger.error(traceback.format_exc())

if __name__ == "__main__":
    asyncio.run(debug_daily_news()) 