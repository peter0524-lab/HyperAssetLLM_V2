"""
LLM Manager - í†µí•© LLM í´ë¼ì´ì–¸íŠ¸ ê´€ë¦¬ì
ì‚¬ìš©ìê°€ ì„ íƒí•œ ëª¨ë¸ì— ë”°ë¼ ì ì ˆí•œ í´ë¼ì´ì–¸íŠ¸ë¥¼ ë°˜í™˜
"""
import os
import logging
from typing import Optional, Dict, Any
from .hyperclova_client import HyperCLOVAClient
from .gemini_client import GeminiClient
from .gemini_api_client import GeminiAPIClient
from .chat_gpt import OpenAIClient
from .claude_client import ClaudeClient

logger = logging.getLogger(__name__)


class LLMManager:
    """í†µí•© LLM ê´€ë¦¬ì - ì‚¬ìš©ì ëª¨ë¸ ì„ íƒì— ë”°ë¼ ì ì ˆí•œ í´ë¼ì´ì–¸íŠ¸ ë°˜í™˜"""
    
    def __init__(self):
        """LLM í´ë¼ì´ì–¸íŠ¸ë“¤ ì´ˆê¸°í™”"""
        self.clients: Dict[str, Any] = {}
        self._initialize_clients()
        self.logger = logging.getLogger(__name__)
    
    def _initialize_clients(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ LLM í´ë¼ì´ì–¸íŠ¸ë“¤ ì´ˆê¸°í™”"""
        try:
            # ê° í´ë¼ì´ì–¸íŠ¸ê°€ ìì²´ì ìœ¼ë¡œ API í‚¤ë¥¼ ê°€ì ¸ì˜´
            self.clients["hyperclova"] = HyperCLOVAClient()
            self.clients["gemini"] = GeminiAPIClient()  # API ë°©ì‹ìœ¼ë¡œ ë³€ê²½
            self.clients["openai"] = OpenAIClient()
            self.clients["claude"] = ClaudeClient()
            
            # ì¶”í›„ êµ¬í˜„ ì˜ˆì •
            # self.clients["grok"] = GrokClient()
            
        except Exception as e:
            self.logger.error(f"âŒ LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            # ìµœì†Œí•œ HyperCLOVAëŠ” ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •
            if "hyperclova" not in self.clients:
                self.clients["hyperclova"] = HyperCLOVAClient()
    
    async def get_client_for_user(self, user_id) -> Any:
        """ì‚¬ìš©ìê°€ ì„ íƒí•œ ëª¨ë¸ì˜ í´ë¼ì´ì–¸íŠ¸ ë°˜í™˜
        
        Args:
            user_id: ì‚¬ìš©ì ID
            
        Returns:
            ì„ íƒëœ ëª¨ë¸ì˜ LLM í´ë¼ì´ì–¸íŠ¸
        """
        try:
            # MySQLì—ì„œ ì‚¬ìš©ì ëª¨ë¸ ì¡°íšŒ
            from shared.user_config.user_config_manager import user_config_manager
            user_model = await user_config_manager.get_user_model(user_id)
            
            # ì„ íƒëœ ëª¨ë¸ì˜ í´ë¼ì´ì–¸íŠ¸ ë°˜í™˜
            if user_model in self.clients:
                client = self.clients[user_model]
                self.logger.debug(f"ğŸ“± ì‚¬ìš©ì {user_id}ì˜ ì„ íƒ ëª¨ë¸: {user_model}")
                return client
            else:
                # ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì¸ ê²½ìš° HyperCLOVAë¡œ ëŒ€ì²´
                self.logger.warning(f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ {user_model}, HyperCLOVAë¡œ ëŒ€ì²´")
                return self.clients["hyperclova"]
                
        except Exception as e:
            self.logger.error(f"âŒ ì‚¬ìš©ì ëª¨ë¸ ì¡°íšŒ ì‹¤íŒ¨: {e}, HyperCLOVAë¡œ ëŒ€ì²´")
            return self.clients["hyperclova"]
    
    async def generate_response(self, user_id, prompt: str, **kwargs) -> Optional[str]:
        """í†µí•© ì‘ë‹µ ìƒì„± (í”„ë¡¬í”„íŠ¸ëŠ” ì„œë¹„ìŠ¤ì—ì„œ ì „ë‹¬ë°›ìŒ)
        
        Args:
            user_id: ì‚¬ìš©ì ID
            prompt: ì…ë ¥ í”„ë¡¬í”„íŠ¸ (ì„œë¹„ìŠ¤ì—ì„œ ìƒì„±ëœ ì™„ì „í•œ í”„ë¡¬í”„íŠ¸)
            **kwargs: ê° í´ë¼ì´ì–¸íŠ¸ë³„ ì¶”ê°€ íŒŒë¼ë¯¸í„°
            
        Returns:
            LLM ì‘ë‹µ
        """
        try:
            client = await self.get_client_for_user(user_id)
            
            # í´ë¼ì´ì–¸íŠ¸ê°€ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸
            if not client.is_available():
                self.logger.warning(f"âš ï¸ {client.__class__.__name__} ì‚¬ìš© ë¶ˆê°€, ê¸°ë³¸ í´ë¼ì´ì–¸íŠ¸ë¡œ ëŒ€ì²´")
                client = self.clients["hyperclova"]
            
            # LLM ì‘ë‹µ ìƒì„±
            response = await client.generate_response(prompt, **kwargs)
            
            self.logger.debug(f"âœ… LLM ì‘ë‹µ ìƒì„± ì™„ë£Œ: {client.__class__.__name__}")
            return response
            
        except Exception as e:
            self.logger.error(f"âŒ LLM ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    async def generate_text(self, user_id, prompt: str, max_tokens: int = 1000) -> str:
        """í†µí•© í…ìŠ¤íŠ¸ ìƒì„± (í˜¸í™˜ì„±ì„ ìœ„í•œ ë©”ì„œë“œ)
        
        Args:
            user_id: ì‚¬ìš©ì ID
            prompt: ì…ë ¥ í”„ë¡¬í”„íŠ¸
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
            
        Returns:
            ìƒì„±ëœ í…ìŠ¤íŠ¸
        """
        try:
            client = await self.get_client_for_user(user_id)
            
            if not client.is_available():
                client = self.clients["hyperclova"]
            
            response = await client.generate_text(prompt, max_tokens)
            return response
            
        except Exception as e:
            self.logger.error(f"âŒ í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return "í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨"
    
    def get_available_models(self) -> Dict[str, Dict[str, Any]]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜
        
        Returns:
            ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤ì˜ ì •ë³´
        """
        available_models = {}
        for model_name, client in self.clients.items():
            available_models[model_name] = {
                "available": client.is_available(),
                "client_info": client.get_client_info()
            }
        return available_models
    
    async def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        for client in self.clients.values():
            if hasattr(client, 'close'):
                try:
                    await client.close()
                except Exception as e:
                    self.logger.error(f"âŒ í´ë¼ì´ì–¸íŠ¸ ì¢…ë£Œ ì‹¤íŒ¨: {e}")


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
llm_manager = LLMManager()


async def get_llm_manager() -> LLMManager:
    """LLM ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    return llm_manager 