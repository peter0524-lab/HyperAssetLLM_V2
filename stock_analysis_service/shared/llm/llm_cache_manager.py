"""
LLM ê²°ê³¼ ê³µìœ  ìºì‹± ë§¤ë‹ˆì € - Production Ready Version
- ê´€ì‹¬ì‚¬ ë¶„ë¦¬ì™€ ì˜ì¡´ì„± ì—­ì „ ì›ì¹™ ì ìš©
- í™•ì¥ ê°€ëŠ¥í•œ ë¶„ì‚° ìºì‹œ ì•„í‚¤í…ì²˜
- ì¢…í•©ì ì¸ ê´€ì°° ê°€ëŠ¥ì„±ê³¼ ëª¨ë‹ˆí„°ë§
- í…ŒìŠ¤íŠ¸ ê°€ëŠ¥í•œ ì„¤ê³„
"""

import asyncio
import hashlib
import json
import logging
import os
import pickle
import re
import time
import zlib
from abc import ABC, abstractmethod
from collections import OrderedDict
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from typing import Dict, List, Optional, Any, Tuple, Protocol, Callable, Set
from typing_extensions import runtime_checkable

import aioredis
import msgpack
import psutil
from fastapi import HTTPException
from opentelemetry import trace
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

logger = logging.getLogger(__name__)
tracer = trace.get_tracer(__name__)

# ==================== íƒ€ì… ì •ì˜ ====================

class CacheType(Enum):
    """ìºì‹œ íƒ€ì…"""
    NEWS_ANALYSIS = "news_analysis"
    CHART_ANALYSIS = "chart_analysis"
    FLOW_ANALYSIS = "flow_analysis"
    DISCLOSURE_ANALYSIS = "disclosure_analysis"
    REPORT_ANALYSIS = "report_analysis"

class EvictionPolicy(Enum):
    """ìºì‹œ ì œê±° ì •ì±…"""
    LRU = "lru"
    LFU = "lfu"
    ADAPTIVE = "adaptive"

@dataclass
class CacheConfig:
    """ìºì‹œ ì„¤ì •"""
    redis_url: str = "redis://localhost:6379"
    redis_max_connections: int = 10
    local_cache_max_size: int = 500
    default_ttl: int = 3600
    enable_local_fallback: bool = True
    enable_compression: bool = True
    compression_threshold: int = 1024  # 1KB
    enable_metrics: bool = True
    enable_tracing: bool = True
    
    @classmethod
    def from_env(cls) -> 'CacheConfig':
        return cls(
            redis_url=os.getenv("REDIS_URL", cls.redis_url),
            redis_max_connections=int(os.getenv("REDIS_MAX_CONNECTIONS", cls.redis_max_connections)),
            local_cache_max_size=int(os.getenv("LOCAL_CACHE_MAX_SIZE", cls.local_cache_max_size)),
            default_ttl=int(os.getenv("CACHE_DEFAULT_TTL", cls.default_ttl)),
            enable_local_fallback=os.getenv("ENABLE_LOCAL_FALLBACK", "true").lower() == "true",
            enable_compression=os.getenv("ENABLE_COMPRESSION", "true").lower() == "true",
            compression_threshold=int(os.getenv("COMPRESSION_THRESHOLD", cls.compression_threshold)),
            enable_metrics=os.getenv("ENABLE_METRICS", "true").lower() == "true",
            enable_tracing=os.getenv("ENABLE_TRACING", "true").lower() == "true"
        )

@dataclass
class CacheMetrics:
    """ìºì‹œ ë©”íŠ¸ë¦­"""
    hits: int = 0
    misses: int = 0
    errors: int = 0
    serialization_time: float = 0.0
    network_time: float = 0.0
    cache_size: int = 0
    compression_ratio: float = 1.0
    avg_response_time: float = 0.0
    
    @property
    def hit_rate(self) -> float:
        total = self.hits + self.misses
        return self.hits / total if total > 0 else 0.0

# ==================== í”„ë¡œí† ì½œ ì •ì˜ ====================

@runtime_checkable
class CacheStorage(Protocol):
    """ìºì‹œ ì €ì¥ì†Œ í”„ë¡œí† ì½œ"""
    async def get(self, key: str) -> Optional[bytes]: ...
    async def set(self, key: str, value: bytes, ttl: int) -> bool: ...
    async def delete(self, key: str) -> bool: ...
    async def exists(self, key: str) -> bool: ...
    async def ping(self) -> bool: ...

@runtime_checkable
class Serializer(Protocol):
    """ì§ë ¬í™” í”„ë¡œí† ì½œ"""
    def serialize(self, obj: Any) -> bytes: ...
    def deserialize(self, data: bytes) -> Any: ...

@runtime_checkable
class CacheKeyGenerator(Protocol):
    """ìºì‹œ í‚¤ ìƒì„± í”„ë¡œí† ì½œ"""
    def generate(self, cache_type: CacheType, **kwargs) -> str: ...

@runtime_checkable
class MetricsCollector(Protocol):
    """ë©”íŠ¸ë¦­ ìˆ˜ì§‘ í”„ë¡œí† ì½œ"""
    async def record_hit(self, response_time: float): ...
    async def record_miss(self): ...
    async def record_error(self, error_type: str): ...
    async def record_serialization_time(self, time: float): ...
    def get_metrics(self) -> CacheMetrics: ...

# ==================== ì»¤ìŠ¤í…€ ì˜ˆì™¸ ====================

class CacheError(Exception):
    """ìºì‹œ ê´€ë ¨ ì»¤ìŠ¤í…€ ì˜ˆì™¸"""
    pass

class SerializationError(CacheError):
    """ì§ë ¬í™” ê´€ë ¨ ì˜ˆì™¸"""
    pass

class ConnectionError(CacheError):
    """ì—°ê²° ê´€ë ¨ ì˜ˆì™¸"""
    pass

# ==================== êµ¬í˜„ì²´ë“¤ ====================

class OptimizedSerializer:
    """ìµœì í™”ëœ ì§ë ¬í™”ê¸°"""
    
    def __init__(self, enable_compression: bool = True, compression_threshold: int = 1024):
        self.enable_compression = enable_compression
        self.compression_threshold = compression_threshold
    
    def serialize(self, obj: Any) -> bytes:
        """ì•ˆì „í•œ ì§ë ¬í™”"""
        try:
            # MessagePackìœ¼ë¡œ íš¨ìœ¨ì  ì§ë ¬í™”
            packed = msgpack.packb(obj, use_bin_type=True)
            
            # ì••ì¶• (í¬ê¸°ê°€ í´ ë•Œë§Œ)
            if self.enable_compression and len(packed) > self.compression_threshold:
                compressed = zlib.compress(packed)
                # ì••ì¶•ë¥ ì´ ì¢‹ì„ ë•Œë§Œ ì••ì¶• ì‚¬ìš©
                if len(compressed) < len(packed) * 0.9:
                    return compressed
            return packed
            
        except (TypeError, ValueError) as e:
            # MessagePack ì‹¤íŒ¨ ì‹œ JSON ì‚¬ìš©
            try:
                json_data = json.dumps(obj, ensure_ascii=False, default=str)
                return json_data.encode('utf-8')
            except Exception as json_e:
                # JSONë„ ì‹¤íŒ¨ ì‹œ pickle ì‚¬ìš©
                try:
                    return pickle.dumps(obj)
                except Exception as pickle_e:
                    raise SerializationError(f"ëª¨ë“  ì§ë ¬í™” ë°©ë²• ì‹¤íŒ¨: {e}, {json_e}, {pickle_e}")
    
    def deserialize(self, data: bytes) -> Any:
        """ì•ˆì „í•œ ì—­ì§ë ¬í™”"""
        try:
            # ì••ì¶• í•´ì œ ì‹œë„
            try:
                decompressed = zlib.decompress(data)
                return msgpack.unpackb(decompressed, raw=False)
            except zlib.error:
                # ì••ì¶•ë˜ì§€ ì•Šì€ ë°ì´í„°
                return msgpack.unpackb(data, raw=False)
        except Exception as e:
            # MessagePack ì‹¤íŒ¨ ì‹œ JSON ì‹œë„
            try:
                return json.loads(data.decode('utf-8'))
            except Exception as json_e:
                # JSONë„ ì‹¤íŒ¨ ì‹œ pickle ì‹œë„
                try:
                    return pickle.loads(data)
                except Exception as pickle_e:
                    raise SerializationError(f"ëª¨ë“  ì—­ì§ë ¬í™” ë°©ë²• ì‹¤íŒ¨: {e}, {json_e}, {pickle_e}")

class SafeCacheKeyGenerator:
    """ì•ˆì „í•œ ìºì‹œ í‚¤ ìƒì„±ê¸°"""
    
    def generate(self, cache_type: CacheType, **kwargs) -> str:
        """ì•ˆì „í•œ ìºì‹œ í‚¤ ìƒì„±"""
        # íŠ¹ìˆ˜ë¬¸ì ì´ìŠ¤ì¼€ì´í”„
        safe_components = []
        for key, value in kwargs.items():
            if isinstance(value, str):
                safe_value = re.sub(r'[^\w\-_]', '_', value)
            else:
                safe_value = str(value)
            safe_components.append(f"{key}={safe_value}")
        
        key_components = [
            "llm_cache",
            cache_type.value,
            *safe_components
        ]
        
        key_string = ":".join(key_components)
        
        # í‚¤ê°€ ë„ˆë¬´ ê¸¸ë©´ SHA256 í•´ì‹œ ì‚¬ìš©
        if len(key_string) > 250:  # Redis í‚¤ ê¸¸ì´ ì œí•œ
            return f"llm_cache:{hashlib.sha256(key_string.encode()).hexdigest()}"
        
        return key_string

class ThreadSafeLocalCache:
    """ìŠ¤ë ˆë“œ ì•ˆì „ ë¡œì»¬ ìºì‹œ"""
    
    def __init__(self, max_size: int = 500, eviction_policy: EvictionPolicy = EvictionPolicy.LRU):
        self._cache = OrderedDict()
        self._lock = asyncio.Lock()
        self.max_size = max_size
        self.eviction_policy = eviction_policy
        self._access_count = {}  # LFUë¥¼ ìœ„í•œ ì ‘ê·¼ íšŸìˆ˜
    
    async def get(self, key: str) -> Optional[Any]:
        async with self._lock:
            if key in self._cache:
                entry = self._cache[key]
                if entry['expires'] > time.time():
                    # LRUë¥¼ ìœ„í•´ ì¬ì •ë ¬
                    self._cache.move_to_end(key)
                    # LFU ì¹´ìš´íŠ¸ ì¦ê°€
                    self._access_count[key] = self._access_count.get(key, 0) + 1
                    return entry['data']
                else:
                    del self._cache[key]
                    if key in self._access_count:
                        del self._access_count[key]
            return None
    
    async def set(self, key: str, data: Any, ttl: int):
        async with self._lock:
            # ë§Œë£Œëœ í•­ëª©ë“¤ ì •ë¦¬
            await self._cleanup_expired()
            
            # í¬ê¸° ì œí•œ í™•ì¸
            if len(self._cache) >= self.max_size:
                await self._evict_item()
            
            self._cache[key] = {
                'data': data,
                'expires': time.time() + ttl
            }
            self._access_count[key] = 0
    
    async def _cleanup_expired(self):
        current_time = time.time()
        expired_keys = [
            k for k, v in self._cache.items() 
            if v['expires'] <= current_time
        ]
        for key in expired_keys:
            del self._cache[key]
            if key in self._access_count:
                del self._access_count[key]
    
    async def _evict_item(self):
        if self.eviction_policy == EvictionPolicy.LRU:
            # LRU: ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
            self._cache.popitem(last=False)
        elif self.eviction_policy == EvictionPolicy.LFU:
            # LFU: ê°€ì¥ ì ê²Œ ì‚¬ìš©ëœ í•­ëª© ì œê±°
            if self._access_count:
                least_used_key = min(self._access_count.keys(), 
                                   key=lambda k: self._access_count[k])
                del self._cache[least_used_key]
                del self._access_count[least_used_key]
    
    async def size(self) -> int:
        async with self._lock:
            return len(self._cache)

class RedisStorage:
    """Redis ì €ì¥ì†Œ êµ¬í˜„"""
    
    def __init__(self, config: CacheConfig):
        self.config = config
        self.redis = None
        self._connection_pool = None
    
    async def initialize(self):
        """Redis ì—°ê²° ì´ˆê¸°í™”"""
        try:
            self.redis = await aioredis.from_url(
                self.config.redis_url,
                max_connections=self.config.redis_max_connections,
                retry_on_timeout=True,
                health_check_interval=30,
                socket_keepalive=True,
                socket_keepalive_options={}
            )
            await self.redis.ping()
            logger.info("âœ… Redis ì—°ê²° ì„±ê³µ")
        except Exception as e:
            logger.warning(f"âš ï¸ Redis ì—°ê²° ì‹¤íŒ¨: {e}")
            self.redis = None
            raise ConnectionError(f"Redis ì—°ê²° ì‹¤íŒ¨: {e}")
    
    @retry(
        retry=retry_if_exception_type((aioredis.ConnectionError, aioredis.TimeoutError)),
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10)
    )
    async def get(self, key: str) -> Optional[bytes]:
        if not self.redis:
            raise ConnectionError("Redis not available")
        return await self.redis.get(key)
    
    @retry(
        retry=retry_if_exception_type((aioredis.ConnectionError, aioredis.TimeoutError)),
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10)
    )
    async def set(self, key: str, value: bytes, ttl: int) -> bool:
        if not self.redis:
            raise ConnectionError("Redis not available")
        return await self.redis.setex(key, ttl, value)
    
    async def delete(self, key: str) -> bool:
        if not self.redis:
            return False
        return await self.redis.delete(key) > 0
    
    async def exists(self, key: str) -> bool:
        if not self.redis:
            return False
        return await self.redis.exists(key) > 0
    
    async def ping(self) -> bool:
        if not self.redis:
            return False
        try:
            await self.redis.ping()
            return True
        except Exception:
            return False
    
    async def close(self):
        if self.redis:
            await self.redis.close()

class InstrumentedMetricsCollector:
    """ê³„ì¸¡ëœ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸°"""
    
    def __init__(self):
        self._metrics = CacheMetrics()
        self._lock = asyncio.Lock()
    
    async def record_hit(self, response_time: float):
        async with self._lock:
            self._metrics.hits += 1
            self._metrics.avg_response_time = (
                (self._metrics.avg_response_time * (self._metrics.hits - 1) + response_time) 
                / self._metrics.hits
            )
    
    async def record_miss(self):
        async with self._lock:
            self._metrics.misses += 1
    
    async def record_error(self, error_type: str):
        async with self._lock:
            self._metrics.errors += 1
    
    async def record_serialization_time(self, time: float):
        async with self._lock:
            self._metrics.serialization_time += time
    
    def get_metrics(self) -> CacheMetrics:
        return self._metrics

# ==================== ë©”ì¸ ìºì‹œ ë§¤ë‹ˆì € ====================

class ProductionLLMCacheManager:
    """Production Ready LLM ìºì‹œ ë§¤ë‹ˆì €"""
    
    def __init__(
        self,
        storage: CacheStorage,
        serializer: Serializer,
        key_generator: CacheKeyGenerator,
        metrics: MetricsCollector,
        config: CacheConfig,
        clock: Callable[[], float] = time.time
    ):
        self._storage = storage
        self._serializer = serializer
        self._key_generator = key_generator
        self._metrics = metrics
        self.config = config
        self._clock = clock
        
        # ë™ì‹œì„± ì œì–´
        self._analysis_locks: Dict[str, asyncio.Lock] = {}
        self._lock = asyncio.Lock()
        
        # ë¡œì»¬ ìºì‹œ (fallback)
        self._local_cache = ThreadSafeLocalCache(
            max_size=config.local_cache_max_size
        )
        
        # ìºì‹œ íƒ€ì…ë³„ TTL ì„¤ì •
        self.cache_ttl = {
            CacheType.NEWS_ANALYSIS: 1800,      # 30ë¶„
            CacheType.CHART_ANALYSIS: 7200,      # 2ì‹œê°„
            CacheType.FLOW_ANALYSIS: 3600,      # 1ì‹œê°„
            CacheType.DISCLOSURE_ANALYSIS: 14400, # 4ì‹œê°„
            CacheType.REPORT_ANALYSIS: 86400     # 24ì‹œê°„
        }
    
    async def initialize(self):
        """ì´ˆê¸°í™”"""
        if hasattr(self._storage, 'initialize'):
            await self._storage.initialize()
    
    async def get_cached_analysis(
        self, 
        cache_type: CacheType, 
        stock_code: str, 
        model_type: str, 
        analysis_type: str = None
    ) -> Optional[Dict]:
        """ìºì‹œëœ LLM ë¶„ì„ ê²°ê³¼ ì¡°íšŒ"""
        
        with tracer.start_as_current_span("llm_cache.get") as span:
            span.set_attribute("cache_type", cache_type.value)
            span.set_attribute("stock_code", stock_code)
            span.set_attribute("model_type", model_type)
            
            start_time = self._clock()
            
            try:
                # ìºì‹œ í‚¤ ìƒì„±
                cache_key = self._key_generator.generate(
                    cache_type=cache_type,
                    stock_code=stock_code,
                    model_type=model_type,
                    analysis_type=analysis_type
                )
                
                # 1ì°¨: Redisì—ì„œ ì¡°íšŒ
                if hasattr(self._storage, 'ping') and await self._storage.ping():
                    cached_data = await self._storage.get(cache_key)
                    if cached_data:
                        result = self._serializer.deserialize(cached_data)
                        if result:
                            response_time = self._clock() - start_time
                            await self._metrics.record_hit(response_time)
                            logger.info(f"âœ… Redis ìºì‹œ íˆíŠ¸: {cache_type.value} - {stock_code}")
                            return result
                        else:
                            # ì†ìƒëœ ìºì‹œ ì‚­ì œ
                            await self._storage.delete(cache_key)
                            logger.warning(f"âš ï¸ ì†ìƒëœ ìºì‹œ ì‚­ì œ: {cache_key}")
                
                # 2ì°¨: ë¡œì»¬ ìºì‹œì—ì„œ ì¡°íšŒ
                if self.config.enable_local_fallback:
                    local_result = await self._local_cache.get(cache_key)
                    if local_result:
                        response_time = self._clock() - start_time
                        await self._metrics.record_hit(response_time)
                        logger.info(f"âœ… ë¡œì»¬ ìºì‹œ íˆíŠ¸: {cache_type.value} - {stock_code}")
                        return local_result
                
                await self._metrics.record_miss()
                return None
                
            except Exception as e:
                await self._metrics.record_error(type(e).__name__)
                logger.error(f"âŒ ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                raise CacheError(f"Cache retrieval failed: {e}")
    
    async def cache_analysis_result(
        self, 
        cache_type: CacheType, 
        stock_code: str, 
        model_type: str, 
        analysis_result: Dict, 
        analysis_type: str = None
    ) -> bool:
        """LLM ë¶„ì„ ê²°ê³¼ ìºì‹œ ì €ì¥"""
        
        with tracer.start_as_current_span("llm_cache.set") as span:
            span.set_attribute("cache_type", cache_type.value)
            span.set_attribute("stock_code", stock_code)
            span.set_attribute("model_type", model_type)
            
            try:
                # ìºì‹œ í‚¤ ìƒì„±
                cache_key = self._key_generator.generate(
                    cache_type=cache_type,
                    stock_code=stock_code,
                    model_type=model_type,
                    analysis_type=analysis_type
                )
                
                # ì§ë ¬í™”
                serialization_start = self._clock()
                serialized_data = self._serializer.serialize(analysis_result)
                serialization_time = self._clock() - serialization_start
                await self._metrics.record_serialization_time(serialization_time)
                
                ttl = self.cache_ttl.get(cache_type, self.config.default_ttl)
                
                # Redisì— ì €ì¥
                if hasattr(self._storage, 'ping') and await self._storage.ping():
                    success = await self._storage.set(cache_key, serialized_data, ttl)
                    if success:
                        logger.info(f"ğŸ’¾ Redis ìºì‹œ ì €ì¥: {cache_type.value} - {stock_code}")
                
                # ë¡œì»¬ ìºì‹œì—ë„ ì €ì¥ (fallback)
                if self.config.enable_local_fallback:
                    await self._local_cache.set(cache_key, analysis_result, ttl)
                
                return True
                
            except Exception as e:
                await self._metrics.record_error(type(e).__name__)
                logger.error(f"âŒ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
                return False
    
    async def get_or_create_analysis(
        self, 
        cache_type: CacheType, 
        stock_code: str,
        model_type: str, 
        analysis_func: Callable, 
        analysis_type: str = None, 
        **kwargs
    ) -> Dict:
        """ìºì‹œëœ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ë°˜í™˜, ì—†ìœ¼ë©´ ìƒì„± í›„ ìºì‹œ"""
        
        with tracer.start_as_current_span("llm_cache.get_or_create") as span:
            span.set_attribute("cache_type", cache_type.value)
            span.set_attribute("stock_code", stock_code)
            span.set_attribute("model_type", model_type)
            
            # ìºì‹œ í‚¤ ìƒì„±
            cache_key = self._key_generator.generate(
                cache_type=cache_type,
                stock_code=stock_code,
                model_type=model_type,
                analysis_type=analysis_type
            )
            
            # 1ì°¨ ìºì‹œ í™•ì¸
            cached_result = await self.get_cached_analysis(
                cache_type, stock_code, model_type, analysis_type
            )
            if cached_result:
                return cached_result
            
            # ë™ì¼í•œ ë¶„ì„ì— ëŒ€í•œ ë™ì‹œ ì‹¤í–‰ ë°©ì§€
            async with self._lock:
                if cache_key not in self._analysis_locks:
                    self._analysis_locks[cache_key] = asyncio.Lock()
                analysis_lock = self._analysis_locks[cache_key]
            
            async with analysis_lock:
                # 2ì°¨ ìºì‹œ í™•ì¸ (ë‹¤ë¥¸ ìŠ¤ë ˆë“œê°€ ì´ë¯¸ ì²˜ë¦¬í–ˆì„ ìˆ˜ ìˆìŒ)
                cached_result = await self.get_cached_analysis(
                    cache_type, stock_code, model_type, analysis_type
                )
                if cached_result:
                    return cached_result
                
                # LLM ë¶„ì„ ì‹¤í–‰
                logger.info(f"ğŸ”„ LLM ë¶„ì„ ì‹¤í–‰: {cache_type.value} - {stock_code}")
                analysis_result = await analysis_func(**kwargs)
                
                # ê²°ê³¼ ìºì‹œ ì €ì¥
                await self.cache_analysis_result(
                    cache_type, stock_code, model_type, analysis_result, analysis_type
                )
                
                return analysis_result
    
    async def invalidate_cache(self, cache_type: CacheType = None, stock_code: str = None):
        """ìºì‹œ ë¬´íš¨í™”"""
        with tracer.start_as_current_span("llm_cache.invalidate") as span:
            if cache_type:
                span.set_attribute("cache_type", cache_type.value)
            if stock_code:
                span.set_attribute("stock_code", stock_code)
            
            try:
                # Redis ìºì‹œ ë¬´íš¨í™”
                if hasattr(self._storage, 'ping') and await self._storage.ping():
                    if cache_type and stock_code:
                        pattern = f"llm_cache:{cache_type.value}:stock_code={stock_code}:*"
                    elif cache_type:
                        pattern = f"llm_cache:{cache_type.value}:*"
                    else:
                        pattern = "llm_cache:*"
                    
                    # Redisì—ì„œ íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ í‚¤ ì‚­ì œ
                    # (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” Redis SCAN ëª…ë ¹ì–´ ì‚¬ìš©)
                    logger.info(f"ğŸ—‘ï¸ ìºì‹œ ë¬´íš¨í™”: {pattern}")
                
                # ë¡œì»¬ ìºì‹œ ë¬´íš¨í™”
                # (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ë¡œì»¬ ìºì‹œë„ ì •ë¦¬)
                
            except Exception as e:
                logger.error(f"âŒ ìºì‹œ ë¬´íš¨í™” ì‹¤íŒ¨: {e}")
                raise CacheError(f"Cache invalidation failed: {e}")
    
    async def get_cache_stats(self) -> Dict:
        """ìºì‹œ í†µê³„ ì¡°íšŒ"""
        metrics = await self._metrics.get_metrics()
        local_cache_size = await self._local_cache.size()
        
        return {
            "metrics": {
                "hits": metrics.hits,
                "misses": metrics.misses,
                "errors": metrics.errors,
                "hit_rate": metrics.hit_rate,
                "avg_response_time_ms": metrics.avg_response_time * 1000,
                "serialization_time_ms": metrics.serialization_time * 1000
            },
            "storage": {
                "local_cache_size": local_cache_size,
                "redis_available": await self._storage.ping() if hasattr(self._storage, 'ping') else False
            },
            "system": {
                "memory_usage_mb": psutil.Process().memory_info().rss / 1024 / 1024,
                "cpu_percent": psutil.Process().cpu_percent()
            }
        }
    
    async def health_check(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ê±´ê°• ìƒíƒœ í™•ì¸"""
        checks = {}
        
        # Redis ì—°ê²° ìƒíƒœ
        try:
            redis_healthy = await self._storage.ping() if hasattr(self._storage, 'ping') else False
            checks["redis"] = {"status": "healthy" if redis_healthy else "unhealthy"}
        except Exception as e:
            checks["redis"] = {"status": "unhealthy", "error": str(e)}
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        memory_info = psutil.Process().memory_info()
        checks["memory"] = {
            "rss_mb": memory_info.rss / 1024 / 1024,
            "vms_mb": memory_info.vms / 1024 / 1024
        }
        
        # ìºì‹œ ì„±ëŠ¥
        metrics = await self._metrics.get_metrics()
        checks["performance"] = {
            "hit_rate": metrics.hit_rate,
            "avg_response_time_ms": metrics.avg_response_time * 1000,
            "total_requests": metrics.hits + metrics.misses
        }
        
        return checks
    
    async def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if hasattr(self._storage, 'close'):
            await self._storage.close()

# ==================== íŒ©í† ë¦¬ í•¨ìˆ˜ ====================

def create_llm_cache_manager(config: Optional[CacheConfig] = None) -> ProductionLLMCacheManager:
    """LLM ìºì‹œ ë§¤ë‹ˆì € íŒ©í† ë¦¬ í•¨ìˆ˜"""
    config = config or CacheConfig.from_env()
    
    # ì»´í¬ë„ŒíŠ¸ ìƒì„±
    storage = RedisStorage(config)
    serializer = OptimizedSerializer(
        enable_compression=config.enable_compression,
        compression_threshold=config.compression_threshold
    )
    key_generator = SafeCacheKeyGenerator()
    metrics = InstrumentedMetricsCollector()
    
    # ìºì‹œ ë§¤ë‹ˆì € ìƒì„±
    cache_manager = ProductionLLMCacheManager(
        storage=storage,
        serializer=serializer,
        key_generator=key_generator,
        metrics=metrics,
        config=config
    )
    
    return cache_manager

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
llm_cache_manager = create_llm_cache_manager() 