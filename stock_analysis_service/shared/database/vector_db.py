"""
ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ í´ë¼ì´ì–¸íŠ¸ ëª¨ë“ˆ
ChromaDB + FAISSë¥¼ í™œìš©í•œ ë‰´ìŠ¤, í‚¤ì›Œë“œ, ê³¼ê±° ì‚¬ë¡€ ë²¡í„° ì €ì¥ ë° ê²€ìƒ‰ ê¸°ëŠ¥ ì œê³µ
"""

import chromadb
from chromadb.config import Settings
from chromadb.utils import embedding_functions
from typing import Dict, List, Optional, Any
import logging
from datetime import datetime, date, timedelta
import os
from sentence_transformers import SentenceTransformer
from config.env_local import get_env_var

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


class VectorDBClient:
    """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ í´ë¼ì´ì–¸íŠ¸ í´ë˜ìŠ¤"""

    def __init__(self):
        """ChromaDB í´ë¼ì´ì–¸íŠ¸ ë° ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
        self.client: Optional[Any] = None
        self.embedding_model: Optional[SentenceTransformer] = None
        self.embedding_function: Optional[Any] = None
        self.collections: Dict[str, Any] = {}
        self._initialize_client()
        self._initialize_embedding_model()
        self._initialize_collections()

    def _initialize_client(self) -> None:
        """ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        try:
            persist_directory = get_env_var(
                "CHROMADB_PERSIST_DIRECTORY", "./data/chroma"
            )

            # ë””ë ‰í† ë¦¬ ìƒì„±
            os.makedirs(persist_directory, exist_ok=True)

            # ChromaDB í´ë¼ì´ì–¸íŠ¸ ìƒì„±
            self.client = chromadb.PersistentClient(
                path=persist_directory,
                settings=Settings(anonymized_telemetry=False, allow_reset=True),
            )

            logger.info(f"ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ: {persist_directory}")

        except Exception as e:
            logger.error(f"ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    def _initialize_embedding_model(self) -> None:
        """ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            model_name = get_env_var("EMBEDDING_MODEL_NAME", "jhgan/ko-sbert-nli")
            device = get_env_var("EMBEDDING_MODEL_DEVICE", "cpu")

            # SentenceTransformer ëª¨ë¸ ë¡œë“œ
            self.embedding_model = SentenceTransformer(model_name)
            self.embedding_model.to(device)

            # ChromaDB ì„ë² ë”© í•¨ìˆ˜ ìƒì„±
            self.embedding_function = (
                embedding_functions.SentenceTransformerEmbeddingFunction(
                    model_name=model_name, device=device
                )
            )

            logger.info(f"ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ: {model_name} ({device})")

        except Exception as e:
            logger.error(f"ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    def _initialize_collections(self) -> None:
        """ë²¡í„° ì»¬ë ‰ì…˜ ì´ˆê¸°í™”"""
        collection_configs = {
            "high_impact_news": {
                "name": get_env_var("VECTOR_DB_HIGH_IMPACT_NEWS", "high_impact_news"),
                "description": "ê³ ì˜í–¥ ë‰´ìŠ¤ ë²¡í„° ì €ì¥ì†Œ (ì˜í–¥ë ¥ 0.7 ì´ìƒ)",
            },
            "past_events": {
                "name": get_env_var("VECTOR_DB_PAST_EVENTS", "past_events"),
                "description": "ê³¼ê±° ì¤‘ìš” ì‚¬ê±´ ë²¡í„° ì €ì¥ì†Œ (10% ë“±ë½ ë˜ëŠ” 1000ë§Œì£¼ ì´ìƒ)",
            },
            "daily_news": {
                "name": get_env_var("VECTOR_DB_DAILY_NEWS", "daily_news"),
                "description": "ì¼ì¼ ë‰´ìŠ¤ ì„ì‹œ ì €ì¥ì†Œ (ë§¤ì¼ ìì • ì‚­ì œ)",
            },
            "keywords": {
                "name": get_env_var("VECTOR_DB_KEYWORDS", "keywords"),
                "description": "ì£¼ê°„ í•µì‹¬ í‚¤ì›Œë“œ ì €ì¥ì†Œ",
            },
        }

        try:
            if not self.client:
                raise Exception("ChromaDB í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                
            for key, config in collection_configs.items():
                # ë©”íƒ€ë°ì´í„° ê¸°ë³¸ê°’ ì„¤ì • (ë¹ˆ ë©”íƒ€ë°ì´í„° ë°©ì§€)
                metadata = {
                    "description": config["description"],
                    "created_at": datetime.now().isoformat(),
                    "version": "1.0",
                    "type": key
                }
                
                collection = self.client.get_or_create_collection(
                    name=config["name"],
                    embedding_function=self.embedding_function,
                    metadata=metadata,
                )
                self.collections[key] = collection
                logger.info(f"ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì™„ë£Œ: {config['name']}")

        except Exception as e:
            logger.error(f"ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    def add_high_impact_news(self, news_data: Dict) -> str:
        """ê³ ì˜í–¥ ë‰´ìŠ¤ ì¶”ê°€"""
        try:
            # ğŸ”§ ì¤‘ë³µ ì²´í¬: ê°™ì€ ì œëª©ì˜ ë‰´ìŠ¤ê°€ ì´ë¯¸ ì €ì¥ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            existing_news = self.collections["high_impact_news"].get(
                where={"title": news_data["title"]},
                limit=1
            )
            
            if existing_news and len(existing_news['ids']) > 0:
                logger.warning(f"ì¤‘ë³µ ë‰´ìŠ¤ ë°œê²¬ - ì €ì¥ ê±´ë„ˆëœ€: {news_data['title'][:50]}...")
                return existing_news['ids'][0]  # ê¸°ì¡´ ID ë°˜í™˜
            
            # ğŸ”§ ì¤‘ë³µ ID ë¬¸ì œ í•´ê²°: ë§ˆì´í¬ë¡œì´ˆ + ë‰´ìŠ¤ ì œëª© í•´ì‹œê°’ ì¶”ê°€
            import hashlib
            import time
            
            # í˜„ì¬ ì‹œê°„ + ë§ˆì´í¬ë¡œì´ˆ + ë‰´ìŠ¤ ì œëª© í•´ì‹œê°’ìœ¼ë¡œ ê³ ìœ  ID ìƒì„±
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            microseconds = int(time.time() * 1000000) % 1000000  # ë§ˆì´í¬ë¡œì´ˆ 6ìë¦¬
            title_hash = hashlib.md5(news_data['title'].encode('utf-8')).hexdigest()[:8]
            
            news_id = f"news_{news_data['stock_code']}_{timestamp}_{microseconds:06d}_{title_hash}"

            # ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ìƒì„±
            text = f"{news_data['title']} {news_data.get('summary', '')}"

            # ë©”íƒ€ë°ì´í„° ìƒì„±
            metadata = {
                "stock_code": news_data["stock_code"],
                "stock_name": news_data["stock_name"],
                "title": news_data["title"],
                "impact_score": news_data["impact_score"],
                "publication_date": (
                    news_data["publication_date"].isoformat()
                    if isinstance(news_data["publication_date"], date)
                    else str(news_data["publication_date"])
                ),
                "created_at": datetime.now().isoformat(),
                "type": "high_impact_news",
            }

            # ì»¬ë ‰ì…˜ì— ì¶”ê°€
            self.collections["high_impact_news"].add(
                documents=[text], metadatas=[metadata], ids=[news_id]
            )

            logger.info(f"ê³ ì˜í–¥ ë‰´ìŠ¤ ì¶”ê°€ ì™„ë£Œ: {news_id}")
            return news_id

        except Exception as e:
            logger.error(f"ê³ ì˜í–¥ ë‰´ìŠ¤ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            raise

    def add_past_event(self, event_data: Dict) -> str:
        """ê³¼ê±° ì¤‘ìš” ì‚¬ê±´ ì¶”ê°€"""
        try:
            # ğŸ”§ ì¤‘ë³µ ì²´í¬: ê°™ì€ ì œëª©ì˜ ì´ë²¤íŠ¸ê°€ ì´ë¯¸ ì €ì¥ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            title = event_data.get('title', event_data['event_type'])
            existing_event = self.collections["past_events"].get(
                where={"title": title},
                limit=1
            )
            
            if existing_event and len(existing_event['ids']) > 0:
                logger.warning(f"ì¤‘ë³µ ê³¼ê±° ì´ë²¤íŠ¸ ë°œê²¬ - ì €ì¥ ê±´ë„ˆëœ€: {title[:50]}...")
                return existing_event['ids'][0]  # ê¸°ì¡´ ID ë°˜í™˜
            
            # ğŸ”§ ê³ ìœ  ID ìƒì„± (ì¤‘ë³µ ë°©ì§€)
            import hashlib
            import time
            
            timestamp = event_data['event_date'].strftime('%Y%m%d_%H%M%S')
            microseconds = int(time.time() * 1000000) % 1000000
            title_hash = hashlib.md5(title[:50].encode('utf-8')).hexdigest()[:6]
            
            event_id = f"event_{event_data['stock_code']}_{timestamp}_{microseconds:06d}_{title_hash}"

            # ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ìƒì„± (ì œëª©ê³¼ ë³¸ë¬¸ ëª¨ë‘ í¬í•¨)
            description = event_data.get('description', '')
            text = f"{title} {description}"

            # ë©”íƒ€ë°ì´í„° ìƒì„± (ì œëª© í•„ë“œ ì¶”ê°€)
            metadata = {
                "stock_code": event_data["stock_code"],
                "stock_name": event_data["stock_name"],
                "title": title,  # ì œëª© í•„ë“œ ì¶”ê°€
                "event_type": event_data["event_type"],
                "event_date": event_data["event_date"].isoformat(),
                "price_change": event_data["price_change"],
                "volume": event_data["volume"],
                "description": description,
                "created_at": datetime.now().isoformat(),
                "type": "past_event",
            }

            # ì»¬ë ‰ì…˜ì— ì¶”ê°€
            self.collections["past_events"].add(
                documents=[text], metadatas=[metadata], ids=[event_id]
            )

            logger.info(f"ê³¼ê±° ì‚¬ê±´ ì¶”ê°€ ì™„ë£Œ: {event_id}")
            return event_id

        except Exception as e:
            logger.error(f"ê³¼ê±° ì‚¬ê±´ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            raise

    def add_daily_news(self, news_data: Dict) -> str:
        """ì¼ì¼ ë‰´ìŠ¤ ì¶”ê°€"""
        try:
            # ğŸ”§ ì¤‘ë³µ ì²´í¬: ê°™ì€ ì œëª©ì˜ ë‰´ìŠ¤ê°€ ì´ë¯¸ ì €ì¥ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            existing_news = self.collections["daily_news"].get(
                where={"title": news_data["title"]},
                limit=1
            )
            
            if existing_news and len(existing_news['ids']) > 0:
                logger.warning(f"ì¤‘ë³µ ì¼ì¼ ë‰´ìŠ¤ ë°œê²¬ - ì €ì¥ ê±´ë„ˆëœ€: {news_data['title'][:50]}...")
                return existing_news['ids'][0]  # ê¸°ì¡´ ID ë°˜í™˜
            
            # ğŸ”§ ì¤‘ë³µ ID ë¬¸ì œ í•´ê²°: ë§ˆì´í¬ë¡œì´ˆ + ë‰´ìŠ¤ ì œëª© í•´ì‹œê°’ ì¶”ê°€
            import hashlib
            import time
            
            # í˜„ì¬ ì‹œê°„ + ë§ˆì´í¬ë¡œì´ˆ + ë‰´ìŠ¤ ì œëª© í•´ì‹œê°’ìœ¼ë¡œ ê³ ìœ  ID ìƒì„±
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            microseconds = int(time.time() * 1000000) % 1000000  # ë§ˆì´í¬ë¡œì´ˆ 6ìë¦¬
            title_hash = hashlib.md5(news_data['title'].encode('utf-8')).hexdigest()[:8]
            
            news_id = f"daily_{news_data['stock_code']}_{timestamp}_{microseconds:06d}_{title_hash}"

            # ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ìƒì„±
            text = f"{news_data['title']} {news_data.get('content', '')[:500]}"

            # ë©”íƒ€ë°ì´í„° ìƒì„±
            metadata = {
                "stock_code": news_data["stock_code"],
                "stock_name": news_data["stock_name"],
                "title": news_data["title"],
                "url": news_data.get("url", ""),
                "publication_date": (
                    news_data["publication_date"].isoformat()
                    if isinstance(news_data["publication_date"], date)
                    else str(news_data["publication_date"])
                ),
                "created_at": datetime.now().isoformat(),
                "type": "daily_news",
            }

            # ì»¬ë ‰ì…˜ì— ì¶”ê°€
            self.collections["daily_news"].add(
                documents=[text], metadatas=[metadata], ids=[news_id]
            )

            logger.info(f"ì¼ì¼ ë‰´ìŠ¤ ì¶”ê°€ ì™„ë£Œ: {news_id}")
            return news_id

        except Exception as e:
            logger.error(f"ì¼ì¼ ë‰´ìŠ¤ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            raise

    def add_keywords(self, keyword_data: Dict) -> str:
        """í•µì‹¬ í‚¤ì›Œë“œ ì¶”ê°€"""
        try:
            import json
            import hashlib
            import time
            
            # ğŸ”§ ì¤‘ë³µ ì²´í¬: ê°™ì€ ì£¼ì°¨ì˜ í‚¤ì›Œë“œê°€ ì´ë¯¸ ì €ì¥ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            week_start_str = keyword_data["week_start"].isoformat()
            
            # ChromaDBì˜ ì˜¬ë°”ë¥¸ where ì¡°ê±´ ë¬¸ë²• ì‚¬ìš©
            existing_keywords = self.collections["keywords"].get(
                where={
                    "$and": [
                        {"stock_code": {"$eq": keyword_data["stock_code"]}},
                        {"week_start": {"$eq": week_start_str}}
                    ]
                },
                limit=1
            )
            
            if existing_keywords and len(existing_keywords['ids']) > 0:
                logger.warning(f"ì¤‘ë³µ í‚¤ì›Œë“œ ë°œê²¬ - ì €ì¥ ê±´ë„ˆëœ€: {keyword_data['stock_code']} {week_start_str}")
                return existing_keywords['ids'][0]  # ê¸°ì¡´ ID ë°˜í™˜
            
            # ê³ ìœ  ID ìƒì„± (ì¤‘ë³µ ë°©ì§€)
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            microseconds = int(time.time() * 1000000) % 1000000
            keywords_hash = hashlib.md5(",".join(keyword_data["keywords"]).encode('utf-8')).hexdigest()[:6]
            
            keyword_id = f"keyword_{keyword_data['stock_code']}_{timestamp}_{microseconds:06d}_{keywords_hash}"

            # ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ìƒì„±
            text = " ".join(keyword_data["keywords"])

            # ë©”íƒ€ë°ì´í„° ìƒì„± (ë¦¬ìŠ¤íŠ¸ë¥¼ JSON ë¬¸ìì—´ë¡œ ë³€í™˜)
            metadata = {
                "stock_code": keyword_data["stock_code"],
                "stock_name": keyword_data["stock_name"],
                "keywords_json": json.dumps(keyword_data["keywords"], ensure_ascii=False),  # JSON ë¬¸ìì—´ë¡œ ì €ì¥
                "keywords_text": ", ".join(keyword_data["keywords"]),  # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ë¡œë„ ì €ì¥
                "keywords_count": len(keyword_data["keywords"]),  # í‚¤ì›Œë“œ ê°œìˆ˜
                "week_start": week_start_str,
                "week_end": keyword_data["week_end"].isoformat(),
                "importance_scores_json": json.dumps(keyword_data.get("importance_scores", []), ensure_ascii=False),  # JSON ë¬¸ìì—´ë¡œ ì €ì¥
                "created_at": datetime.now().isoformat(),
                "type": "keywords",
            }

            # ì»¬ë ‰ì…˜ì— ì¶”ê°€
            self.collections["keywords"].add(
                documents=[text], metadatas=[metadata], ids=[keyword_id]
            )

            logger.info(f"í•µì‹¬ í‚¤ì›Œë“œ ì¶”ê°€ ì™„ë£Œ: {keyword_id}")
            return keyword_id

        except Exception as e:
            logger.error(f"í•µì‹¬ í‚¤ì›Œë“œ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            raise

    def search_similar_news(
        self, query_text: str, stock_code: str, top_k: int = 5, threshold: float = 0.7
    ) -> List[Dict]:
        """ìœ ì‚¬ ë‰´ìŠ¤ ê²€ìƒ‰"""
        try:
            # ê³ ì˜í–¥ ë‰´ìŠ¤ì—ì„œ ê²€ìƒ‰
            results = self.collections["high_impact_news"].query(
                query_texts=[query_text],
                n_results=top_k,
                where={"stock_code": stock_code},
                include=["documents", "metadatas", "distances"],
            )

            # ê²°ê³¼ í¬ë§·íŒ…
            similar_news = []
            if results["documents"][0]:
                for i, (doc, metadata, distance) in enumerate(
                    zip(
                        results["documents"][0],
                        results["metadatas"][0],
                        results["distances"][0],
                    )
                ):
                    # ìœ ì‚¬ë„ ê³„ì‚° (ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜)
                    similarity = 1 - distance

                    if similarity >= threshold:
                        similar_news.append(
                            {
                                "document": doc,
                                "metadata": metadata,
                                "similarity": similarity,
                                "rank": i + 1,
                            }
                        )

            logger.info(f"ìœ ì‚¬ ë‰´ìŠ¤ ê²€ìƒ‰ ì™„ë£Œ: {len(similar_news)}ê°œ ë°œê²¬")
            return similar_news

        except Exception as e:
            logger.error(f"ìœ ì‚¬ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def search_past_events(
        self, query_text: str, stock_code: str, top_k: int = 3
    ) -> List[Dict]:
        """ê³¼ê±° ìœ ì‚¬ ì‚¬ê±´ ê²€ìƒ‰"""
        try:
            results = self.collections["past_events"].query(
                query_texts=[query_text],
                n_results=top_k,
                where={"stock_code": stock_code},
                include=["documents", "metadatas", "distances"],
            )

            # ê²°ê³¼ í¬ë§·íŒ…
            past_events = []
            if results["documents"][0]:
                for i, (doc, metadata, distance) in enumerate(
                    zip(
                        results["documents"][0],
                        results["metadatas"][0],
                        results["distances"][0],
                    )
                ):
                    similarity = 1 - distance
                    past_events.append(
                        {
                            "document": doc,
                            "metadata": metadata,
                            "similarity": similarity,
                            "rank": i + 1,
                        }
                    )

            logger.info(f"ê³¼ê±° ì‚¬ê±´ ê²€ìƒ‰ ì™„ë£Œ: {len(past_events)}ê°œ ë°œê²¬")
            return past_events

        except Exception as e:
            logger.error(f"ê³¼ê±° ì‚¬ê±´ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def get_current_keywords(self, stock_code: str) -> List[str]:
        """í˜„ì¬ ì£¼ê°„ í•µì‹¬ í‚¤ì›Œë“œ ì¡°íšŒ"""
        try:
            import json
            
            # ìµœê·¼ 1ì£¼ì¼ ì´ë‚´ í‚¤ì›Œë“œ ê²€ìƒ‰ (ë‹¨ìˆœ ì¢…ëª© ì½”ë“œë¡œë§Œ ê²€ìƒ‰)
            results = self.collections["keywords"].query(
                query_texts=[""],
                n_results=5,  # ë” ë§ì€ ê²°ê³¼ ì¡°íšŒ
                where={"stock_code": stock_code},
                include=["metadatas"],
            )

            # ê²°ê³¼ê°€ ìˆëŠ”ì§€ í™•ì¸
            if results["metadatas"] and results["metadatas"][0]:
                for metadata in results["metadatas"][0]:
                    try:
                        # JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥ëœ í‚¤ì›Œë“œ íŒŒì‹±
                        keywords_json = metadata.get("keywords_json", "[]")
                        keywords = json.loads(keywords_json)
                        
                        if keywords:  # ë¹ˆ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê²½ìš°
                            logger.info(f"í˜„ì¬ í‚¤ì›Œë“œ ì¡°íšŒ ì™„ë£Œ: {len(keywords)}ê°œ - {keywords}")
                            return keywords
                            
                    except (json.JSONDecodeError, ValueError) as parse_error:
                        logger.warning(f"í‚¤ì›Œë“œ JSON íŒŒì‹± ì‹¤íŒ¨: {parse_error}")
                        # ë°±ì—…: í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ì €ì¥ëœ í‚¤ì›Œë“œ ì‚¬ìš©
                        keywords_text = metadata.get("keywords_text", "")
                        if keywords_text:
                            keywords = [k.strip() for k in keywords_text.split(",")]
                            logger.info(f"ë°±ì—… í‚¤ì›Œë“œ ì¡°íšŒ ì™„ë£Œ: {len(keywords)}ê°œ - {keywords}")
                            return keywords
                        continue

            logger.warning(f"ì¢…ëª© {stock_code}ì— ëŒ€í•œ í‚¤ì›Œë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []

        except Exception as e:
            logger.error(f"í˜„ì¬ í‚¤ì›Œë“œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []

    def cleanup_daily_news(self) -> int:
        """ì¼ì¼ ë‰´ìŠ¤ ì •ë¦¬ (ë§¤ì¼ ìì • ì‹¤í–‰)"""
        try:
            # ì–´ì œ ì´ì „ ë°ì´í„° ì‚­ì œ
            yesterday = (datetime.now() - timedelta(days=1)).isoformat()

            # ì „ì²´ ë°ì´í„° ì¡°íšŒ (ì‚­ì œí•  ë°ì´í„° ì°¾ê¸°)
            results = self.collections["daily_news"].get(
                where={"created_at": {"$lt": yesterday}}, include=["metadatas"]
            )

            if results["ids"]:
                # ì¼ê´„ ì‚­ì œ
                self.collections["daily_news"].delete(ids=results["ids"])
                deleted_count = len(results["ids"])
                logger.info(f"ì¼ì¼ ë‰´ìŠ¤ ì •ë¦¬ ì™„ë£Œ: {deleted_count}ê°œ ì‚­ì œ")
                return deleted_count

            return 0

        except Exception as e:
            logger.error(f"ì¼ì¼ ë‰´ìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return 0

    def get_collection_stats(self) -> Dict:
        """ì»¬ë ‰ì…˜ í†µê³„ ì¡°íšŒ"""
        try:
            stats = {}
            for key, collection in self.collections.items():
                count = collection.count()
                stats[key] = {"count": count, "collection_name": collection.name}

            logger.info("ì»¬ë ‰ì…˜ í†µê³„ ì¡°íšŒ ì™„ë£Œ")
            return stats

        except Exception as e:
            logger.error(f"ì»¬ë ‰ì…˜ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}

    def collection_exists(self, collection_name: str) -> bool:
        """ì»¬ë ‰ì…˜ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
        try:
            if not self.client:
                return False
            collections = self.client.list_collections()
            return any(collection.name == collection_name for collection in collections)
        except Exception as e:
            logger.error(f"ì»¬ë ‰ì…˜ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ì‹¤íŒ¨: {e}")
            return False

    def create_collection(self, collection_name: str, description: Optional[str] = None) -> bool:
        """ìƒˆë¡œìš´ ì»¬ë ‰ì…˜ ìƒì„±"""
        try:
            if not self.client:
                logger.error("ChromaDB í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return False
                
            # ë©”íƒ€ë°ì´í„° ê¸°ë³¸ê°’ ì„¤ì • (ë¹ˆ ë©”íƒ€ë°ì´í„° ë°©ì§€)
            metadata = {
                "description": description or f"Collection for {collection_name}",
                "created_at": datetime.now().isoformat(),
                "version": "1.0"
            }

            collection = self.client.create_collection(
                name=collection_name,
                embedding_function=self.embedding_function,
                metadata=metadata,
            )

            logger.info(f"ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ: {collection_name}")
            return True

        except Exception as e:
            logger.error(f"ì»¬ë ‰ì…˜ ìƒì„± ì‹¤íŒ¨ ({collection_name}): {e}")
            return False

    def delete_collection(self, collection_name: str) -> bool:
        """ì»¬ë ‰ì…˜ ì‚­ì œ"""
        try:
            if not self.client:
                logger.error("ChromaDB í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return False
            self.client.delete_collection(name=collection_name)
            logger.info(f"ì»¬ë ‰ì…˜ ì‚­ì œ ì™„ë£Œ: {collection_name}")
            return True
        except Exception as e:
            logger.error(f"ì»¬ë ‰ì…˜ ì‚­ì œ ì‹¤íŒ¨ ({collection_name}): {e}")
            return False

    def health_check(self) -> Dict:
        """ë²¡í„° DB ìƒíƒœ í™•ì¸"""
        try:
            # ëª¨ë“  ì»¬ë ‰ì…˜ ìƒíƒœ í™•ì¸
            collections_status = {}
            for key, collection in self.collections.items():
                try:
                    count = collection.count()
                    collections_status[key] = {"status": "healthy", "count": count}
                except Exception as e:
                    collections_status[key] = {"status": "unhealthy", "error": str(e)}

            # ì„ë² ë”© ëª¨ë¸ ìƒíƒœ í™•ì¸
            embedding_status = "healthy"
            try:
                if self.embedding_model:
                    test_embedding = self.embedding_model.encode(["í…ŒìŠ¤íŠ¸"])
                    if len(test_embedding) == 0:
                        embedding_status = "unhealthy"
                else:
                    embedding_status = "unhealthy"
            except Exception:
                embedding_status = "unhealthy"

            return {
                "status": "healthy",
                "collections": collections_status,
                "embedding_model": embedding_status,
                "timestamp": datetime.now().isoformat(),
            }

        except Exception as e:
            logger.error(f"ë²¡í„° DB ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
            return {
                "status": "unhealthy",
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
            }


    def search_similar_documents(self, query: str, collection_name: str = "high_impact_news", top_k: int = 5) -> List[Dict]:
        """ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ (ì¼ë°˜ìš©)"""
        try:
            if collection_name in self.collections:
                collection = self.collections[collection_name]
                results = collection.query(
                    query_texts=[query],
                    n_results=top_k,
                    include=['documents', 'metadatas', 'distances']
                )
                
                # ê²°ê³¼ í¬ë§·íŒ…
                formatted_results = []
                if results['documents'] and results['documents'][0]:
                    for i, doc in enumerate(results['documents'][0]):
                        formatted_results.append({
                            'document': doc,
                            'metadata': results['metadatas'][0][i] if results['metadatas'] and results['metadatas'][0] else {},
                            'distance': results['distances'][0][i] if results['distances'] and results['distances'][0] else 0.0
                        })
                
                return formatted_results
            else:
                logger.warning(f"ì»¬ë ‰ì…˜ '{collection_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return []
                
        except Exception as e:
            logger.error(f"ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def add_documents(self, documents: List[str], metadatas: List[Dict], collection_name: str = "high_impact_news", ids: Optional[List[str]] = None) -> bool:
        """ë¬¸ì„œ ì¶”ê°€ (ì¼ë°˜ìš©)"""
        try:
            if collection_name in self.collections:
                collection = self.collections[collection_name]
                
                # ID ìƒì„± (ì œê³µë˜ì§€ ì•Šì€ ê²½ìš°)
                if ids is None:
                    ids = [f"doc_{collection_name}_{i}_{datetime.now().strftime('%Y%m%d_%H%M%S')}" for i in range(len(documents))]
                
                collection.add(
                    documents=documents,
                    metadatas=metadatas,
                    ids=ids
                )
                
                logger.info(f"ë¬¸ì„œ {len(documents)}ê°œë¥¼ ì»¬ë ‰ì…˜ '{collection_name}'ì— ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
                return True
            else:
                logger.error(f"ì»¬ë ‰ì…˜ '{collection_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return False
                
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return False

    def add_document(self, document: str, metadata: Dict, collection_name: str = "high_impact_news", doc_id: Optional[str] = None) -> bool:
        """ë‹¨ì¼ ë¬¸ì„œ ì¶”ê°€"""
        try:
            if doc_id is None:
                doc_id = f"doc_{collection_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
            return self.add_documents([document], [metadata], collection_name, [doc_id])
        except Exception as e:
            logger.error(f"ë‹¨ì¼ ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return False
    
    def get_all_documents(self, collection_name: str, limit: int = 100) -> List[Dict]:
        """ì»¬ë ‰ì…˜ì˜ ëª¨ë“  ë¬¸ì„œ ì¡°íšŒ"""
        try:
            # ì»¬ë ‰ì…˜ ì´ë¦„ ë§¤í•‘
            collection_map = {
                "high_impact_news": "high_impact_news",
                "past_events": "past_events", 
                "daily_news": "daily_news",
                "weekly_keywords": "keywords",
                "keywords": "keywords"
            }
            
            mapped_name = collection_map.get(collection_name, collection_name)
            
            if mapped_name not in self.collections:
                logger.warning(f"ì»¬ë ‰ì…˜ '{collection_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            collection = self.collections[mapped_name]
            
            # ChromaDBì—ì„œ ì „ì²´ ë¬¸ì„œ ì¡°íšŒ (get ë©”ì†Œë“œ ì‚¬ìš©)
            results = collection.get(
                limit=limit,
                include=['documents', 'metadatas']
            )
            
            documents = []
            for i in range(len(results['ids'])):
                doc_data = {
                    'id': results['ids'][i],
                    'document': results['documents'][i] if 'documents' in results and i < len(results['documents']) else '',
                    'metadata': results['metadatas'][i] if 'metadatas' in results and i < len(results['metadatas']) else {}
                }
                documents.append(doc_data)
            
            logger.info(f"ì»¬ë ‰ì…˜ '{collection_name}'ì—ì„œ {len(documents)}ê°œ ë¬¸ì„œ ì¡°íšŒ ì™„ë£Œ")
            return documents
            
        except Exception as e:
            logger.error(f"ì „ì²´ ë¬¸ì„œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []

    def encode(self, texts: List[str]) -> List[List[float]]:
        """í…ìŠ¤íŠ¸ ì¸ì½”ë”© (ì„ë² ë”© ìƒì„±)"""
        try:
            if not self.embedding_model:
                logger.error("ì„ë² ë”© ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return []
                
            embeddings = self.embedding_model.encode(texts)
            return embeddings.tolist()
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ì¸ì½”ë”© ì‹¤íŒ¨: {e}")
            return []

    def close(self) -> None:
        """Vector DB í´ë¼ì´ì–¸íŠ¸ ì •ë¦¬"""
        try:
            if self.client:
                logger.info("Vector DB í´ë¼ì´ì–¸íŠ¸ ì •ë¦¬ ì¤‘...")
                self.client = None
                self.collections = {}
                logger.info("Vector DB í´ë¼ì´ì–¸íŠ¸ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"Vector DB í´ë¼ì´ì–¸íŠ¸ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")


# ì „ì—­ ë²¡í„° DB í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤
vector_db_client = VectorDBClient()
