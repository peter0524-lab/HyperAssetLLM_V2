#!/usr/bin/env python3
"""
ChromaDB ì—°ê²° í…ŒìŠ¤íŠ¸ ë° ì„ë² ë”© í™•ì¸ ìŠ¤í¬ë¦½íŠ¸
"""

from shared.database.vector_db import VectorDBClient
from sentence_transformers import SentenceTransformer
from datetime import datetime, timedelta
import traceback
import numpy as np

def test_chromadb_with_embeddings():
    """ChromaDB ì—°ê²° ë° ì„ë² ë”© í…ŒìŠ¤íŠ¸"""
    try:
        print("=== ChromaDB ì—°ê²° ë° ì„ë² ë”© í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
        
        # VectorDB í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        print("1. VectorDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”...")
        vector_db = VectorDBClient()
        print("âœ… í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì„±ê³µ!")
        
        # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        print("\n2. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ...")
        embedding_model = SentenceTransformer("jhgan/ko-sroberta-multitask")
        print("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
        
        # ìƒíƒœ í™•ì¸
        print("\n3. ìƒíƒœ í™•ì¸...")
        health = vector_db.health_check()
        print(f"ìƒíƒœ: {health['status']}")
        if health['status'] == 'healthy':
            print(f"âœ… ChromaDB ì—°ê²° ì„±ê³µ!")
            print(f"ì»¬ë ‰ì…˜ ìˆ˜: {health.get('collections_count', 'Unknown')}")
        else:
            print(f"âŒ ì—°ê²° ì‹¤íŒ¨: {health.get('error', 'Unknown')}")
            return False
        
        # ì»¬ë ‰ì…˜ í†µê³„
        print("\n4. ì»¬ë ‰ì…˜ í†µê³„ í™•ì¸...")
        stats = vector_db.get_collection_stats()
        for collection, count in stats.items():
            print(f"  - {collection}: {count}ê°œ ë¬¸ì„œ")
        
        # 4ê°œ ì»¬ë ‰ì…˜ì— í…ŒìŠ¤íŠ¸ ë°ì´í„° ê°ê° 1ê°œì”© ì¶”ê°€
        print("\n5. 4ê°œ ì»¬ë ‰ì…˜ì— í…ŒìŠ¤íŠ¸ ë°ì´í„° ê°ê° 1ê°œì”© ì¶”ê°€...")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ì˜
        test_data = {
            "high_impact_news": {
                "text": "ì‚¼ì„±ì „ì 3ë¶„ê¸° ì‹¤ì  ë°œí‘œ, ì˜ì—…ì´ìµ 12ì¡°ì› ë‹¬ì„±ìœ¼ë¡œ ì‹œì¥ ê¸°ëŒ€ ìƒíšŒ",
                "metadata": {
                    "title": "ì‚¼ì„±ì „ì Q3 ì‹¤ì  ë°œí‘œ",
                    "content": "ì‚¼ì„±ì „ì 3ë¶„ê¸° ì‹¤ì  ë°œí‘œ, ì˜ì—…ì´ìµ 12ì¡°ì› ë‹¬ì„±ìœ¼ë¡œ ì‹œì¥ ê¸°ëŒ€ ìƒíšŒ",
                    "stock_code": "005930",
                    "impact_score": 0.85,
                    "published_at": datetime.now().isoformat(),
                    "url": "https://example.com/samsung-q3"
                }
            },
            "daily_news": {
                "text": "ì˜¤ëŠ˜ ì½”ìŠ¤í”¼ ì§€ìˆ˜ëŠ” ì†Œí­ ìƒìŠ¹í•˜ë©° 2650ì„ ì„ íšŒë³µí–ˆìŠµë‹ˆë‹¤",
                "metadata": {
                    "stock_code": "KOSPI",
                    "stock_name": "ì½”ìŠ¤í”¼",
                    "title": "ì½”ìŠ¤í”¼ 2650ì„  íšŒë³µ",
                    "content": "ì˜¤ëŠ˜ ì½”ìŠ¤í”¼ ì§€ìˆ˜ëŠ” ì†Œí­ ìƒìŠ¹í•˜ë©° 2650ì„ ì„ íšŒë³µí–ˆìŠµë‹ˆë‹¤",
                    "url": "https://example.com/kospi-recovery",
                    "publication_date": datetime.now()
                }
            },
            "keywords": {
                "text": "AI ì¸ê³µì§€ëŠ¥ ë°˜ë„ì²´ ë©”ëª¨ë¦¬",
                "metadata": {
                    "stock_code": "000660",
                    "keywords_text": "AI ì¸ê³µì§€ëŠ¥ ë°˜ë„ì²´ ë©”ëª¨ë¦¬",
                    "week_start": datetime.now().strftime("%Y-%m-%d"),
                    "created_at": datetime.now().isoformat()
                }
            },
            "past_events": {
                "text": "2023ë…„ 3ì›” ì‹¤ë¦¬ì½˜ë°¸ë¦¬ì€í–‰ íŒŒì‚°ìœ¼ë¡œ ê¸€ë¡œë²Œ ê¸ˆìœµì‹œì¥ ì¶©ê²©",
                "metadata": {
                    "event_title": "ì‹¤ë¦¬ì½˜ë°¸ë¦¬ì€í–‰ íŒŒì‚° ì‚¬ê±´",
                    "event_description": "2023ë…„ 3ì›” ì‹¤ë¦¬ì½˜ë°¸ë¦¬ì€í–‰ íŒŒì‚°ìœ¼ë¡œ ê¸€ë¡œë²Œ ê¸ˆìœµì‹œì¥ ì¶©ê²©",
                    "event_date": (datetime.now() - timedelta(days=30)).isoformat(),
                    "impact_level": "ë†’ìŒ"
                }
            }
        }
        
        # ê° ì»¬ë ‰ì…˜ì— 1ê°œì”© ë°ì´í„° ì¶”ê°€ ë° ì„ë² ë”© í™•ì¸
        embeddings_info = {}
        
        for collection_name, data in test_data.items():
            print(f"\n5-{list(test_data.keys()).index(collection_name)+1}. {collection_name} í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ê°€...")
            
            # ì„ë² ë”© ìƒì„± ë° í™•ì¸
            text = data["text"]
            embedding = embedding_model.encode(text)
            
            print(f"  ğŸ“ ì…ë ¥ í…ìŠ¤íŠ¸: '{text}'")
            print(f"  ğŸ”¢ ì„ë² ë”© ì°¨ì›: {len(embedding)}")
            print(f"  ğŸ“Š ì„ë² ë”© ê°’ ë²”ìœ„: {embedding.min():.4f} ~ {embedding.max():.4f}")
            print(f"  ğŸ¯ ì„ë² ë”© ì²« 5ê°œ ê°’: {embedding[:5]}")
            
            # ì„ë² ë”© ì •ë³´ ì €ì¥
            embeddings_info[collection_name] = {
                "text": text,
                "embedding_dim": len(embedding),
                "embedding_range": (float(embedding.min()), float(embedding.max())),
                "embedding_sample": embedding[:5].tolist()
            }
            
            # ChromaDBì— ë¬¸ì„œ ì¶”ê°€
            try:
                if collection_name == "daily_news":
                    # daily_newsëŠ” íŠ¹ë³„í•œ ë©”ì†Œë“œ ì‚¬ìš©
                    doc_id = vector_db.add_daily_news(data["metadata"])
                else:
                    # ë‹¤ë¥¸ ì»¬ë ‰ì…˜ë“¤ì€ ì¼ë°˜ add_document ì‚¬ìš©
                    success = vector_db.add_document(
                        document=text,
                        metadata=data["metadata"],
                        collection_name=collection_name
                    )
                    doc_id = "success" if success else "failed"
                
                if doc_id:
                    print(f"  âœ… {collection_name} ë¬¸ì„œ ì¶”ê°€ ì„±ê³µ: {doc_id}")
                else:
                    print(f"  âŒ {collection_name} ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨")
                    
            except Exception as e:
                print(f"  âŒ {collection_name} ë¬¸ì„œ ì¶”ê°€ ì˜¤ë¥˜: {e}")
        
        # ì„ë² ë”© ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„
        print("\n6. ì„ë² ë”© ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„...")
        print("=" * 60)
        
        for collection_name, info in embeddings_info.items():
            print(f"\nğŸ“‚ {collection_name}:")
            print(f"   ì…ë ¥: {info['text'][:50]}...")
            print(f"   ì°¨ì›: {info['embedding_dim']}ì°¨ì›")
            print(f"   ë²”ìœ„: {info['embedding_range'][0]:.4f} ~ {info['embedding_range'][1]:.4f}")
            print(f"   ìƒ˜í”Œ: {[f'{x:.4f}' for x in info['embedding_sample']]}")
        
        # ì„ë² ë”© ìœ ì‚¬ë„ í…ŒìŠ¤íŠ¸
        print("\n7. ì„ë² ë”© ìœ ì‚¬ë„ í…ŒìŠ¤íŠ¸...")
        print("=" * 60)
        
        # ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”© ìƒì„±
        test_texts = [
            "ì‚¼ì„±ì „ì ì‹¤ì  ë°œí‘œ",          # high_impact_newsì™€ ìœ ì‚¬
            "ì½”ìŠ¤í”¼ ì§€ìˆ˜ ìƒìŠ¹",            # daily_newsì™€ ìœ ì‚¬  
            "AI ë°˜ë„ì²´ ê¸°ìˆ ",             # keywordsì™€ ìœ ì‚¬
            "ê¸ˆìœµì‹œì¥ ì¶©ê²©"               # past_eventsì™€ ìœ ì‚¬
        ]
        
        for i, test_text in enumerate(test_texts):
            test_embedding = embedding_model.encode(test_text)
            print(f"\nğŸ” í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ {i+1}: '{test_text}'")
            print(f"   ì„ë² ë”© ì°¨ì›: {len(test_embedding)}")
            print(f"   ì„ë² ë”© ë²”ìœ„: {test_embedding.min():.4f} ~ {test_embedding.max():.4f}")
            print(f"   ì„ë² ë”© ìƒ˜í”Œ: {[f'{x:.4f}' for x in test_embedding[:3]]}")
            
            # ì›ë³¸ ë°ì´í„°ì™€ ìœ ì‚¬ë„ ê³„ì‚°
            for collection_name, info in embeddings_info.items():
                original_embedding = embedding_model.encode(info["text"])
                
                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                similarity = np.dot(test_embedding, original_embedding) / (
                    np.linalg.norm(test_embedding) * np.linalg.norm(original_embedding)
                )
                
                # ë¹„êµ ë¬¸ì¥ ìŒ ëª…í™•íˆ í‘œì‹œ
                print(f"   ğŸ”— ë¹„êµ: '{test_text}' â†” '{info['text'][:30]}...'")
                print(f"       â””â”€ {collection_name}: ìœ ì‚¬ë„ {similarity:.4f}")
        
        # ì‹¤ì œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        print("\n8. ì‹¤ì œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸...")
        print("=" * 60)
        
        collections = ["high_impact_news", "daily_news", "keywords", "past_events"]
        test_queries = [
            "ì‚¼ì„±ì „ì ì‹¤ì ",
            "ì½”ìŠ¤í”¼ ìƒìŠ¹", 
            "AI ë°˜ë„ì²´",
            "ê¸ˆìœµ ìœ„ê¸°"
        ]
        
        for collection, query in zip(collections, test_queries):
            print(f"\nğŸ” {collection} ê²€ìƒ‰: '{query}'")
            
            try:
                results = vector_db.search_similar_documents(
                    query=query,
                    collection_name=collection,
                    top_k=3
                )
                print(f"   âœ… ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ")
                
                for j, result in enumerate(results):
                    distance = result.get('distance', 0)
                    # ChromaDBëŠ” ê±°ë¦¬ ê°’ì„ ë°˜í™˜í•˜ëŠ”ë°, ì½”ì‚¬ì¸ ê±°ë¦¬ì˜ ê²½ìš° 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìœ ì‚¬
                    # ìœ ì‚¬ë„ = 1 - ê±°ë¦¬ê°’ (ë‹¨, ê±°ë¦¬ê°’ì´ 1ë³´ë‹¤ í´ ìˆ˜ë„ ìˆìŒ)
                    if distance <= 1:
                        similarity = 1 - distance
                    else:
                        similarity = max(0, 2 - distance)  # ê±°ë¦¬ê°€ 1ë³´ë‹¤ í´ ê²½ìš° ë³´ì •
                    
                    doc_text = result.get('document', 'N/A')
                    print(f"   ğŸ”— ê²€ìƒ‰ ë¹„êµ: '{query}' â†” '{doc_text[:40]}...'")
                    print(f"       â””â”€ ê²°ê³¼ {j+1}: ê±°ë¦¬ {distance:.4f} â†’ ìœ ì‚¬ë„ {similarity:.4f}")
                    print(f"          ğŸ“„ ì „ì²´ ë‚´ìš©: {doc_text[:60]}...")
                    
            except Exception as e:
                print(f"   âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        
        # ì „ì²´ ë¬¸ì„œ ì¡°íšŒ í…ŒìŠ¤íŠ¸
        print("\n9. ì „ì²´ ë¬¸ì„œ ì¡°íšŒ í…ŒìŠ¤íŠ¸...")
        print("=" * 60)
        
        for collection in collections:
            try:
                all_docs = vector_db.get_all_documents(collection, limit=10)
                print(f"\nğŸ“‚ {collection}: {len(all_docs)}ê°œ ë¬¸ì„œ")
                
                for i, doc in enumerate(all_docs[:2]):  # ìµœëŒ€ 2ê°œë§Œ ì¶œë ¥
                    print(f"   ë¬¸ì„œ {i+1}: {doc.get('document', 'N/A')[:50]}...")
                    
            except Exception as e:
                print(f"   âŒ {collection} ì¡°íšŒ ì‹¤íŒ¨: {e}")
        
        # ìµœì¢… í†µê³„
        print("\n8. ìµœì¢… ì»¬ë ‰ì…˜ í†µê³„...")
        final_stats = vector_db.get_collection_stats()
        for collection, count in final_stats.items():
            print(f"  - {collection}: {count}ê°œ ë¬¸ì„œ")
        
        # ì •ë¦¬
        print("\n9. ì •ë¦¬...")
        vector_db.close()
        print("âœ… ì •ë¦¬ ì™„ë£Œ")
        
        print("\n=== ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼! ===")
        print("ğŸ“Š 4ê°œ ì»¬ë ‰ì…˜ì— í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("ğŸ” ChromaDB ëŒ€ì‹œë³´ë“œì—ì„œ í™•ì¸í•´ë³´ì„¸ìš”: http://localhost:8080")
        return True
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_chromadb_with_embeddings()
    if success:
        print("\nğŸš€ ChromaDBê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤!")
        print("ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    else:
        print("\nâŒ ChromaDB í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨!")
        print("ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.") 